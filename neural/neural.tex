\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{fullpage}

\usepackage{comment}
\usepackage{color}
\usepackage{enumerate}
\usepackage{bm}
\usepackage{amssymb}
% \usepackage{ esint }
\usepackage{amsthm}
\usepackage[normalem]{ulem}
\usepackage{thmtools}
\usepackage[colorlinks=true,
            linkcolor=hanpurple(colorwheel),
            urlcolor=hanpurple,
            citecolor=hanpurple]{hyperref}
            
 
\usepackage{amsfonts,txfonts,psfrag,color}
\usepackage[dvips]{graphicx}
\usepackage{epstopdf}
\usepackage{caption}
\usepackage[dvipsnames]{xcolor}
\usepackage[mathscr]{euscript}
\usepackage[framemethod=tikz]{mdframed}
\usepackage{upgreek}


\numberwithin{equation}{section}

\makeatletter
%\renewcommand{\@secnumfont}{\bfseries}
\makeatother


\usepackage{bera}

%%% THEOREM MACROS
\newtheorem{thm}{Theorem}[section]    
\newtheorem{lem}[thm]{Lemma}         
\newtheorem{prop}[thm]{Proposition}        
\newtheorem{coro}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}          
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}  
\newtheorem*{defi}{Definition}
\newtheorem{rmk}[thm]{Remark}
\newtheorem{quest}[thm]{Question}
\newtheorem{eg}{Example}

\newmdenv[%
	  backgroundcolor=black!4,
          shadow=false,
          shadowsize=1pt,
          linewidth=.5pt,
          frametitlerule=true,
          %roundcorner=10pt,
          rightline=false,
          leftline=false,
          topline=false,
          bottomline=false
          ]{shadowbox}
          


%%%	blackboard bold 			%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\newcommand{		\C		}	{	\mathbb{C}				}
\newcommand{		\N		}	{	\mathbb{N}				}
\newcommand{		\Q		}	{	\mathbb{Q}				}
\newcommand{		\R		}	{	\mathbb{R}				}
\newcommand{		\bbS		}	{	\mathbb{S}				}
\newcommand{		\Z		}	{	\mathbb{Z}				}
\newcommand{		\rmi		}	{	\textrm{i}					}
\newcommand{		\cX		}	{	\mathcal{X}				}
\newcommand{		\SO		}	{	\textrm{SO}				}
\newcommand{		\GL		}	{	\textrm{GL}				}
\newcommand{		\rd		}	{	\textrm{d}					}

%
%
%
%%%	sf letters	 				%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	lower..
\newcommand{		\sff		}	{	{\text{\rm\small\textsf{f}}}		}
\newcommand{		\sfg		}	{	{\text{\rm\small\textsf{g}}}		}
\newcommand{		\sfr		}	{	{\text{\rm\small\textsf{r}}}		}
\newcommand{		\sfu		}	{	{\text{\rm\small\textsf{u}}}		}
\newcommand{		\sfx		}	{	{\text{\rm\small\textsf{x}}}		}
\newcommand{		\sfy		}	{	{\text{\rm\small\textsf{y}}}		}
%	upper..
\newcommand{		\sfB		}	{	{\text{\rm\small\textsf{B}}}		}
\newcommand{		\sfC		}	{	{\text{\rm\small\textsf{C}}}		}
\newcommand{		\sfE		}	{	{\text{\rm\small\textsf{E}}}		}
\newcommand{		\sfO		}	{	{\text{\rm\small\textsf{O}}}		}
\newcommand{		\sfP		}	{	{\text{\rm\small\textsf{P}}}		}
\newcommand{		\sfS		}	{	{\text{\rm\small\textsf{S}}}		}
\newcommand{		\dom		}	{	{\text{\rm dom\,}}		}
\newcommand{		\cod		}	{	{\text{\rm cod\,}}		}
\newcommand{		\id		}	{	{\text{\rm id}}		}
\newcommand{		\matr		}	{	{\mathbf{ Matr}}		}
\newcommand{		\set		}	{	{\mathbf{Set}}		}
\newcommand{		\ens		}	{	{\mathbf{Ens}}		}
\newcommand{		\grp		}	{	{\mathbf{Grp}}		}
\newcommand{		\aut		}	{	{\mathbf{Aut}}		}


%
%
%
%%%	color						%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\newcommand{		\red		}	{\color{red}					} 
\newcommand{		\green	}	{\color{green}				}
\newcommand{		\yellow	}	{\color{yellow}				}
\newcommand{		\iris		}	{\color{hanpurple}			}
\newcommand{		\grey		}	{\color{ashgrey}			}
\newcommand{		\az		}	{\color{azure}}
%
%
%
%%%	formatting					%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	space
\newcommand{		\four		}	{	\vspace{4mm}				}%			vertical space of 4mm
\newcommand{		\n		}	{	\noindent					} %			don't indent line of text
%	punctuation
\newcommand{		\tc		}	{	\,:\,						}%			a colon : with room 
\newcommand{		\jc		}	{	\,,\,						}%			a comma , with room
\newcommand{		\jdots	}	{	\jc \dots \jc				}%			ellipses with room
\newcommand{		\jd		}	{	\,\cdot\,					}%			cdot with room 
\newcommand{		\jddots	}	{	\jd \dots \jd				}%			for taking products, using ellipses 
\newcommand{		\eq		}[1]	{	\begin{align*}#1\end{align*}	}%			quickly display w/ no label
%	brackets and parentheses
\newcommand{		\leftp		}	{	\left(						}
\newcommand{		\rightp	}	{	\right)					}
\newcommand{		\leftb		}	{	\left[						}
\newcommand{		\rightb	}	{	\right]					}
\newcommand{		\leftcb	}	{	\left\{						}
\newcommand{		\rightcb	}	{	\right\}					} 
\newcommand{		\lbob		}	{	\bm{\leftb}					}
\newcommand{		\rbob		}	{	\bm{\rightb}				}
\newcommand{		\ra		}	{	\rangle					}
\newcommand{		\la		}	{	\langle					}
%	misc
\newcommand{\ands}{\text{ and }}
\newcommand{		\B		}	{\textbf					} %			bold text 
\newcommand{		\non		}	{	\nonumber				} %			mute numbering in align display
%
%
%
%%% 	decoration					%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\newcommand{		\wh		}	{	\hat						}
\newcommand{		\til		}	{	\tilde						} 
\newcommand{		\wt		}	{	\widetilde					}
\newcommand{		\ov		}	{	\overline					}%			\ov = \ol
\newcommand{		\ol		}	{	\overline					}%			
\newcommand{		\un		}	{	\underline					}%			\un = \ul
\newcommand{		\ul		}	{	\underline					}%			
\newcommand{		\tri		}	{	\triangleq					}
% 	ordinal indicators
\newcommand{\oith}{\textsuperscript{\rm \hspace{.4mm}th}}
%
%
%
%%% 	greek 					%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	lower
\newcommand{		\al		}	{	\alpha					}
\newcommand{		\e		}	{	\epsilon					}
\newcommand{		\eps		}	{	\epsilon					}
\newcommand{		\lam		}	{	\lambda					}
\newcommand{		\sig		}	{	\sigma					}
\newcommand{		\tht		}	{	\theta					}
\newcommand{		\om		}	{	\omega					}
%	variant
\newcommand{		\vareps	}	{	\varepsilon				}
\newcommand{		\vp		}	{	\varphi					}
%	uppper
\newcommand{		\Oh		}	{	\Omega					}
%
%
%
%%% 	misc. letters				%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\newcommand{		\calI		}	{	\mathcal{I}				}
\newcommand{		\rmV		}	{	\text{ {\rm V}} 				}
\newcommand{		\scrm	}	{	\text{\rm sc}				}
\newcommand{		\tsfE		}	{	{\text{\rm\tiny\textsf{E}}}		} %for using sfE inside a sub- or superscript
\newcommand{		\rmd		}	{\text{{\rm V}}}
\newcommand{ \bbH} {\mathbb{H}}
\newcommand{ \sfD } { \textsf{D}}
%
%
%
%%%	sets	and measures			%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\newcommand{		\sub		}	{	\subset					}
\newcommand{		\1		}	{	\bm{1}					}%			\1 = \indi
\newcommand{		\indi		}	{	\bm{1}					}%	
\newcommand{		\Leb		}	{	{\rm{Leb}}					}
\newcommand{		\Haus	}	{	{\rm{d}}_H					}
\newcommand{		\supp	}	{	\text{supp}				}
\newcommand{		\dist		}	{	\text{dist}					}
\newcommand{		\len		}	{	\text{\rm length}				}
\newcommand{		\vol		}	{	\text{\rm vol}				}
%
%
%
%%% 	"analysis" 					%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\newcommand{		\ls		}	{	\lesssim					}
\newcommand{		\pa		}	{	\partial					}
\newcommand{		\argmin	}	{	\text{argmin}				}
\newcommand{		\sgn		}	{	\text{\rm sgn}				}
\newcommand{		\osc		}	{	\text{osc}					}
%
%
%
%%% 	linear algebra + RMT 		%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\newcommand{		\spa		}	{	\textrm{span}				}
\newcommand{		\Tr		}	{	\text{\rm Tr}				}
\newcommand{		\diag		}	{	{\rm{diag}}					}
\newcommand{		\ind		}	{	\text{\rm ind}				}	% 		index of a matrix
\newcommand{		\goe		}	{	\text{\rm GOE}				}	%		\goe = \Goe = \GOE
\newcommand{		\Goe		}	{	\goe						}	%	
\newcommand{		\GOE	}	{	\goe						}	%	
\newcommand{		\bp		}	{	\boxplus					}	%		free convolution
\newcommand{		\semi	}	{	\mu_{\text{\rm sc}}			}	% 		semicircle law
%
%
%
%%%	matrix names 				%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 	lowercase
\newcommand{		\mate	}	{	\B{\rm \B{e}}				}
\newcommand{		\ide		}	{	\matI						}
% 	uppercase
\newcommand{		\matA	}	{	\B{\rm \B{A}}				}
\newcommand{		\matB	}	{	\B{\rm \B{B}}				}
\newcommand{		\matC	}	{	\B{\rm \B{C}}				}
\newcommand{		\matG	}	{	\B{\rm \B{G}}				}
\newcommand{		\matI		}	{	\B{\rm \B{I}}				}
\newcommand{		\matM	}	{	\B{\rm \B{M}}				}
\newcommand{		\matT	}	{	\B{\rm \B{T}}				}
\newcommand{		\matW	}	{	\B{\rm \B{W}}				}
\newcommand{		\matX	}	{	\B{\rm \B{X}}				}
\newcommand{		\matR	}	{	\B{\rm \B{R}}				}
\newcommand{		\matQ	}	{	\B{\rm \B{Q}}				}

%
%
%
%%% 	probability 				%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 	expectation, variance etc. 
\newcommand{		\E		}	{	\mathbb{E}				}
\newcommand{		\rmE		}	{	\text{ {\rm E}} 				}
\newcommand{		\prob		}	{	\mathbb{P}				}
\newcommand{		\Cov		}	{	\text{\rm Cov}				}
\newcommand{		\var		}	{	{\rm Var}					}
%	misc. 
\newcommand{		\io		}	{	\text{\hspace{1mm}  i.o.}		}	% 		infinitely often
\newcommand{		\rmc		}	{	\text{ {\rm c}} 				}	% 		set compliment
\newcommand{		\sto		}	{	\prec_{\text{\rm sto}}			} 	% 		stochastic domination
\newcommand{		\PPP		}	{	\textrm{PPP}				}	%		Poisson point process
\newcommand{		\PD		}	{	\textrm{PD}				}	%		Poisson-Dirichlet 
\newcommand{		\iid		}	{\text{\rm i.i.d.}				}
\newcommand{		\sprob		}{ \textsf{m}_1 }
%
%
%
%%% 	complexity project specific		%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 	crits
\newcommand{		\crit		}	{	\mathrm{Crt}				}	%		\crit = \Crit = \Crt
\newcommand{		\Crit		}	{	\crit						}	%	
\newcommand{		\Crt		}	{	\crit						}	%		
\newcommand{		\critNell	}	{	\Crit _ { N , \, \ell }			}	%		\critNell = \CritNell
\newcommand{		\CritNell	}	{	\Crit _ { N , \, \ell }			}	%
%	rate fn.
\newcommand{ 	\scrI 		}	{	\mathscr{I} 				}
\newcommand{ 	\rateell 	}	{ 	\mathscr{I}^{\, (\ell)} 			}
% 	misc.
\newcommand{		\HNp		}	{	H_{N,\,p}					}	%		hamiltonian
%
%
%
%%% 	for editing					%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{		\CITE	}	{	 \text{ \iris \emph{ (cite) } } 	}

\newcommand{\lla}{\left\la}
\newcommand{\rra}{\right\ra} 
\newcommand{\ch}{\mathcal{H}}
\newcommand{\cm}{\mathcal{M}}
\newcommand{\ce}{\mathcal{E}}
\newcommand{\emp}{\emph}
\newcommand{\nab}{\nabla}
\newcommand{\thr}{u_{\textrm{th}}}
\newcommand{\gs}{u_{\textrm{gs}}}
\newcommand{\vten}{\vspace{10mm}}
\newcommand{\Hess}{\bm{\mathcal{H}}}
\newcommand{\np}{ \mathfrak{n} }
\newcommand{\rp}{ \mathfrak{r} }
\newcommand{\dee}{\text{\rm d}}
\newcommand{\calD}{\mathcal{D}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calY}{\mathcal{Y}}

\newcommand{\lra}{\leftrightarrow}
\newcommand{\chem}{d_{\text{\rm chem}}}

\newcommand{\second}{\mathcal{C}_{n}^{\,\circ}}
\newcommand{\first}{\mathcal{C}_n}
\newcommand{\diam}{\text{\rm diam}}
\newcommand{\ten}{\vspace{10mm}}

\newcommand{\thp}{\theta_p}
\newcommand{\thpd}{\theta_p(d)} 
\newcommand{\pc}{p_c}
\newcommand{\pcd}{p_c(d)} 
\newcommand{\Zd}{\Z^d}
\newcommand{\Ed}{\mathbb{E}^d}
\newcommand{\Ld}{\mathbb{L}^d}
\newcommand{\Rd}{\R^d}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\pp}{\prob_p}

\newcommand{\ite}[1]{\begin{itemize}#1\end{itemize}}
\newcommand{\theorem}[1]{\begin{thm}#1\end{thm}}
\newcommand{\cas}[1]{\begin{cases}#1\end{cases}}

%%% ??
\def\dul#1{\underline{\underline{#1}}}
%%% ??

%%% color definitions 
\definecolor{ashgrey}{rgb}{0.7, 0.75, 0.71}
\definecolor{customgreen}{rgb}{.73,1.0,.9}
\definecolor{azure}{rgb}{0.0, 0.5, 1.0}
\definecolor{hanpurple}{rgb}{0.32, 0.09, 0.98}
\definecolor{hanpurple(colorwheel)}{rgb}{0.32, 0.09, 0.98}
\definecolor{iris}{rgb}{0.35, 0.31, 0.81}

%%%%%%%%% "begin" 
\begin{document}
%%%%%%%%%




%\tableofcontents
\author{}


\four

\vspace{5mm}
\begin{centering}
\textbf{Notes on \emph{geometric deep learning}}
\end{centering}
\vspace{5mm}

These are notes on the following paper of Bronstein-Bruna-Cohen-Veli\v{c}ovi\'c \cite{BBCV21}, titled

\vspace{5mm}

\begin{centering}

\href{https://arxiv.org/abs/2104.13478}{Geometric deep learning: grids, groups, graphs, geodesics, gauges}

\end{centering}

\vspace{5mm}

\emph{The text below is not entirely my writing, nor is it taken entirely verbatim.}

\tableofcontents

\n\hrulefill

\newpage

\section{Introduction}



Modern neural network (NN) design is built on two algorithmic principles: hierarchical feature learning ( concerning the architecture of the NN ), and learning by local gradient-descent driven by backpropagation ( concerning the learning dynamics undergone by the NN ). 

We model an instance of training data is an element of some high-dimensional vector space, making a generic learning problem subject to the \href{https://en.wikipedia.org/wiki/Curse_of_dimensionality}{curse of dimensionality}. Fortunately, most tasks of interest are not generic, inheriting regularities from the underlying low-dimensionality and structure of the physical world.

Exploiting known symmetries of a large system is a useful, classical remedy against the curse of dimensionality, and forms the basis of most physical theories. The notes \cite{BBCV21} construct a blueprint for neural network architecture which incorporates these ``physical" priors, termed \B{geometric priors} throughout the notes. Importantly, this blueprint provides a unified perspective of the most successful neural network architectures.

%like CNNs, RNNs, GNNs and Transformers. 




% Below, we introduce the mathematical objects relevant to later discussion. The notes \cite{BBCV21} are eclectic enough, so I thought, why not throw in some category theory? This gives us a unified framework into which we can extract objects defined throughout the notes, the goal being to distill ideas presented from the more fundamental definitions presented. Category theory is suited to tracking all of the different ``data-types" we consider in the geometric learning.  Moreover, category theory seems suited to compositional objects like neural nets, though I have nothing deep to say about this. 

\subsection{Categories and groups } 



\begin{defn} A \emph{\B{graph}} is a pair $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, where $\mathcal{V}$ is a set whose elements are called \emph{\B{vertices}}. The set $\mathcal{E}$ consists of \emph{\B{edges}}, defined to be a multi-set of exactly two vertices in $\mathcal{V}$, not necessarily distinct. 


\end{defn}

\begin{defn} 
A \emph{\B{directed graph}} is a pair of sets  $\mathcal{G} = (\mathcal{V}, \mathcal{A})$ of vertices and \B{\emph{arrows}} (or \B{\emph{directed edges}}). An \emph{\B{arrow}} is an \emph{ordered pair} of vertices.
\end{defn}

\begin{defn}  Consider an arrow $f$ of a directed graph $\mathcal{G} = ( \mathcal{V}, \mathcal{A})$, specifically $f \equiv (a,b) \in \mathcal{A}$, with $a,b \in \mathcal{V}$. The operations $\dom$ and $\cod$ act on the arrows $f \in \mathcal{A}$ via $\dom f = a,\, \cod f = b$, and are called the \emph{\B{domain}} operation and \emph{\B{codomain}} operation,  respectively. 
\end{defn}

Given two arrows, $f$ and $g$ in some directed graph, we say that the ordered pair of arrows $(g,f)$ is a \emph{\B{composable pair}} if $\dom g = \cod f$. Going forward, let us express the relations $a = \dom f$ and $b = \cod f$ more concisely via
$$
f : a \to b \, \quad \text { or equivalently, } \, \quad a \xrightarrow[\,]{ f} b  
$$

The next definition formalizes the behavior of a collection of structure-respecting maps between mathematical objects. 

%The following definition is taken from MacLane's \cite{MacLane} definition of a \emph{metacategory}. \footnote{MacLane distinguishes a metacategory: \emph{``any interpretation which satisfies this definition"} from a category \emph{``any interpretation of the category axioms within set theory."} }

\begin{defn} A \emph{\B{category}} is a directed graph $\mathcal{C} = (\mathcal{O},\mathcal{A})$, whose vertices $\mathcal{O}$ we call \emph{\B{objects}}, such that 
\begin{itemize}
\item[\emph{(i)}] For each object $a \in \mathcal{O}$, there is a unique \emph{\B{identity}} arrow $\id_a \equiv \1_a : a \to a$, defined by the following property: for all arrows $f : b \to a$ and $g : a \to c$, composition with the identity arrow $\id_a $ gives
$$
\id_a \circ f = f \quad \text{ and } \quad g \circ \id_a = g
$$
\item[\emph{(ii)}] For each composable pair $(g, f)$ of arrows, there is a unique arrow $g \circ f$ called their \emph{\B{composite}}, with $g \circ f : \dom f \to \cod g$, such that the composition operation is associative. Namely, for given objects and arrows in the configuration $a \xrightarrow[\,]{ f} b \xrightarrow[\,]{ g} c \xrightarrow[\,]{ k} d$,
one always has the equality $k \circ (g \circ f) = (k \circ g ) \circ f$.

\end{itemize}

\end{defn}

Examples are given in the appendix. Given a category $\mathcal{C} = (\mathcal{O},\mathcal{A})$, let
\eq{
\hom (b,c) := \{ \, f : f \in \mathcal{A},  \, \dom f = b \in \mathcal{O}, \, \cod f = c \in \mathcal{O} \, \}
}
denote the set of arrows from $b$ to $c$. Henceforth, we use the terms \emph{\B{morphism}} and arrow interchangeably. 

    
     Groups are collections of symmetries. A \emph{\B{group}} $G$ is a category $\mathcal{C} = (\mathcal{O}, \mathcal{A})$ with $\mathcal{O} = \{ o \}$ ( so that we may identify $G$ with the collection of arrows $\mathcal{A}$ ) such that each arrow has a unique inverse: for $g \in \mathcal{A}$, there is an arrow $h$ such that $g \circ h = h \circ g = \id_o$. 
          
     Each arrow $g \in \mathcal{A}$ thus has $\dom g = \cod g = o$. As remarked, the arrows $g \in \mathcal{A}$ correspond to group elements $g \in G$. The categorical interpretation suggests that the group \emph{acts} on some abstract object $o \in \mathcal{O}$. In the present context, we care how groups act on data, and how this action is represented to a computer. 

\subsubsection*{Group representations}

Linear representation theory allows us to study groups using linear algebra. ( \href{https://wlou.blog/2018/06/22/a-first-impression-of-group-representations/}{a source} ). We start by considering a function $\varphi : G \times V \to V$, where $G$ is a group, and where $V$ is a vector space over $\R$. This allows us to identify group elements $g$ with functions $\varphi(g, \cdot) : V \to V$ from the vector space to itself. When the map $\varphi$ is understood, or general ( as now ), we write $g.v$ in place of $\varphi(g,v)$, and we write $(g.)$ in place of $\varphi(g, \cdot)$. 

The `representatives' $(g.)$ of these group elements $g$ can be composed, and if this compositional structure is compatible with the original group operation, we say $\varphi$ is a \emph{\B{group action}} on $V$. Specifically, $\varphi$ should satisfy $e.v = v$ for all $v \in V$, where $e$ denotes the identity element of $G$, and in general one has $(gh).v  = g.(h.v)$. 

The map $\varphi$ is \emph{\B{$\R$-linear}} if it is compatible with the $\R$-vector space structure on $V$, i.e. additive and homogeneous. Specifically, if for all $v,w \in V$ and all scalars $\lambda \in \R$, one has $g.(v+w) = g.v + g.w$ and $g.(\lambda v) = \lambda g.v$. 


\begin{defn} 
\label{def:R_lin_rep}
An \emph{\B{$\mathbb{R}$-linear representation}} of group $G$ over $\mathbb{R}$-vector space $V$ is an $\mathbb{R}$-linear group action on $V$.
\end{defn}


The next example illustrates how linear group representations arise naturally when considering group actions on data. As mentioned, we consider input data as members of some vector space $V$, which we may assume to be finite dimensional for any practical discussion. Specifically, we consider some finite, discrete domain $\Oh$, which may also have the structure of an undirected graph. 

A \emph{\B{signal}} over $\Oh$ is a function $x : \Oh \to \R^s$, where $s$ is the number of \emph{\B{channels}}. The vector space $\cX(\Oh,\R^s)$ is defined to be the collection of all such signals, for given $\Oh$ and $s$

\vspace{5mm}

\begin{mdframed}
\begin{eg}[ RGB image ] 
Consider, for some $n \in \N$, a signal domain $\Oh = \mathbb{T}_n^2$, where $\mathbb{T}_n^2$ denotes the two-dimensional discrete torus of side-length $n$, namely $( \Z / n\Z )^2$. This domain has natural graph as well as group structures. 

If we imagine each vertex of $\Oh$ to be a pixel, we can express an $n \times n$-pixel color (RGB) image as a signal $x : \Oh \to \R^3$, with the first, second and third coordinates of $\R^3$ reporting R, G and B values of a given pixel. We make two observations: 
\begin{itemize}
\item[(1)] As a vector space, $\mathcal{X}(\Omega)$ is isomorphic to $\mathbb{R}^d$, with $d$ typically very large. In the above example, $d = 3n^2$, which is thirty-thousand for a $n \times n \equiv 100 \times 100$ pixel image. 
\item[(2)] Any group action on $\Omega$ induces a group action on $\mathcal{X}(\Omega)$. 
\end{itemize}

Expanding on the latter, consider a group action of $G$ on domain $\Omega$. As the torus $\Oh$ already has group structure, it is natural to think of it acting on itself through translations, i.e. we now additionally consider $G = \mathbb{T}_n^2$. 

The action of $G \equiv \mathbb{T}_n^2$ on itself $\Oh \equiv \mathbb{T}_n^2$ induces a $G$-action on $\mathcal{X}(\Omega)$ as follows: for $g \in G$ signal $x \in \mathcal{X}(\Omega)$, the action $(g, x) \mapsto \bm{g}.x \in \mathcal{X}(\Omega)$  is defined pointwise at each $u \in \Omega$:
\eq{
(\bm{g}.x)(u) := x(g.\omega),
}
where the bold $(\bm{g}.)$ is used to distinguish the action on signals from the action on the domain. 

\end{eg}
\end{mdframed}

\vspace{5mm}

To summarize: any $G$-action on the domain $\Omega$ induces an $\mathbb{R}$-linear representation of $G$ over the vector space of signals on $\Omega$.


\vspace{5mm}

\begin{mdframed}
\begin{eg} [ One-hot encoding ] It seems like standard practice to encode the collection of classes associated to some ML classification problem as an orthonormal basis. These are given ( to the computer ) in the usual coordinate basis 
\eq{
e_1 \equiv (1, 0, \dots, 0),\, e_2 \equiv (0,1,\dots, 0),\, \dots, \, e_n \equiv (0,\dots, 0,1) \,,
}
hence the nomenclature \emph{\B{one-hot}}. In the preceding example, if one considers a one-hot encoding of the vertices of $\mathbb{T}_n^2$, we see that each signal is expressed with respect to this coordinate system, in the sense that $x = \sum_{j=1}^n x_j e_j$. 

This kind of encoding is useful for considering general symmetries of the domain. For instance, if permuting node labels is a relevant symmetry, the action of the symmetric group $\frak{S}_n$ is naturally represented by $n \times n$ permutation matrices.

\end{eg}
\end{mdframed}

\vspace{5mm}


\subsubsection*{Signals on graphs as node features } 


The following definition reformulates the notion of a signal over the nodes of some graph as \emph{node features}. 

\begin{defn} We say a graph $\mathcal{G} = ( \mathcal{V}, \mathcal{E} )$ is equipped with \emph{\B{node features}} if for each $v  \in \mathcal{V}$, one has the additional data of an $s$-dimensional vector $x(v) \in \mathbb{R}^s$, called the \emph{\B{features}} of node $v$. 
\end{defn}

\begin{rmk} The term `features' is compatible with the usage in ML, supposing that our input signal has domain some graph $\mathcal{G}$. In this case, we can think of a neural network as a sequence of node-layers built ``on top of" the graph $\mathcal{G}$. An input signal endows the first node layer of a NN with features, and the weights of the neural network propagate these through to node features on the nodes of the rest of the network. The features on the last layer of the network can be read off as the output of the NN function. 
\end{rmk}

\vspace{5mm}

\subsection{Equivariance}

\vspace{5mm}

We henceforth consider groups $G$ acting on some space of signals $\cX(\Oh)$ over a ( fixed ) signal domain $\Oh$. The group action is encoded in a linear representation $\rho$, assumed to be described in a given \emph{\B{input coordinate system}}, just as we would need to specify to a computer. Thus, if $\dim ( \cX(\Oh) ) = n$, for each $g \in G$, we have that $\rho(g)$ is an $n \times n$ matrix with real entries.  

The learning dynamics occur in the \emph{\B{hypothesis space}} $\textsf{H}$, a collection of functions 
$$
\textsf{H} \subset \{\, F : \cX(\Oh) \to \mathcal{Y}\, \} ,
$$
where $\mathcal{Y}$ is some unspecified ( context-specific ) space of output signals. We describe $\textsf{H}$ explicitly in Subsection~\ref{subsec:GDLblue}, up to hyperparameters such as depth and layer widths. A key aspect of this blueprint is that $F \in \textsf{H}$ should ( according to the blueprint ) be expressed as a composition of functions, most of which are $G$-equivariant. The requirement of $G$-equivariance constitutes a `geometric prior' from which one can derive the architecture of a generic CNN when $G$ corresponds to translations, and a family of generalizations for other domains and group actions.  

\begin{defn} Let $\rho$ be a representation of group $G$ over $\mathcal{X}(\Oh)$, and let $\rho'$ be a representation of the same group over the $\R$-vector space $\mathcal{Y}$. A function $F: \mathcal{X}(\Oh) \to \mathcal{Y}$ is \emph{\B{$G$-equivariant}} if for all $g \in G$ and $x \in \cX(\Oh)$, we have $\rho'(g) F(x) = F ( \rho(g) x )$. We say $F$ is \emph{\B{$G$-invariant}} if this holds when $\rho'$ is the trivial representation, which is to say $F ( \rho(g) x) = F(x)$ for all $g \in G$ and $x \in \mathcal{X}(\Omega)$.
\end{defn}

%Signals over grids, related to some vision problem, have the advantage that the group acting on these signals can be identified with their domain. Symmetries associated to signals living on more irregular graphs which are not identifiable with $G$ \footnote{i.e. not the Cayley graph of some group $G$} ( such as a social network, in which node features can encode age, location and so on ) still arise from the need to label the nodes of the graph before feeding it into a computer. 

%The group acting on signals in this more general case is the symmetric group $\mathfrak{S}_n$, where the graph is assumed to have $n$ nodes, i.e. $\# \mathcal{V} = n$. The group acts not by permuting the nodes, but by permuting the labels on the nodes. 

% It is natural to consider this symmetry for mathematical reasons: graphs are topological objects, and in particular the nodes in $\mathcal{V}$ are usually not assumed to be provided in any particular order. Thus a desirable property for functions acting on graphs is that it does not depend on the ordering of nodes. 


\begin{mdframed}
\begin{eg}
\label{eg:node_function_permutation}
Suppose we are given either a set $\mathcal{V}$, or more generally a graph $\mathcal{G} = ( \mathcal{V}, \mathcal{E} )$, with $\# \mathcal{V} = n$ in either case. As discussed, a signal over $\mathcal{V} = \{ v_1, \dots, v_n \}$ can be thought of as a collection of node features $\{ \, x(v_1), \dots, x(v_n) \, \}$, with $x(v_j) \in \mathbb{R}^s$. We stack the node features as rows of the $n \times s$ \emph{\B{design matrix}}
\eq{
\bm{X} = 
\left[ 
\begin{matrix}
x_1\\ 
\vdots\\
x_n
\end{matrix}
\right] ,
}
which is effectively the same object as signal $x$, provided the vertices are labeled as described. The action of $g \in \mathfrak{S}_n$ on this input data is naturally represented as an $n \times n$ permutation matrix, $\bm{P} \equiv \rho(g)$. One standard way to construct a permutation invariant function $F$ in this setting is through the following recipe: a function $\psi$ is independently applied to every node's features, and $\varphi$ is applied on its sum-aggregated outputs.
\eq{
    F( \bm{X}) = \varphi \left( \, \sum_{j \, = \, 1}^n \psi(x_j) \, \right) .
}
Such a function can be thought of as reporting some `global statistic' of signal $x$.

Equivariance manifests even more naturally. Suppose we want to apply a function $F$ to the signal to \emph{update} the node features, producing a set of \emph{\B{latent}} (node) features. \footnote{This is the case in which the NN outputs an image segmentation mask; the underlying domain does not change, but the features at each node are updated to the extent that they may not even agree on the number of channels. } We can stack these latent features into another design matrix, $\bm{H} = F(\bm{X})$. The order of the rows of $\bm{H}$ should clearly be tied to the order of the rows of $\bm{X}$, i.e. permutation equivariant: for any permutation matrix $\bm{P}$, it holds that $F(\bm{P} \bm{X} ) = \bm{P} F(\bm{X})$. 

As a concrete example of a permutation equivariant function, consider a weight matrix $\theta \in \mathbb{R}^{s \, \times \, s'}$. This matrix can be used to map a length-$s$ feature vector at a given node to some new, updated feature vector with $s'$-channels. Applying this matrix to each row of the design matrix is an example of a \emph{\B{shared node-wise linear transform}}, and constitutes a linear, $\mathfrak{S}_n$-equivariant map. Note that, if one wishes to express this map in coordinates, it seems the correct object to consider is a $3$-tensor, $``\Theta,"$ constructed as a stack of $n$ copies of $\theta$.
\end{eg}
\end{mdframed}

\vspace{5mm}

The above example considered signals over the nodes of a graph only, thus label permutation symmetry applies equally well, regardless of the graph structure ( or lack of structure ) underlying such functions. 

In the case that signals $x$ have a domain with graph structure, it feels right to want to work with a hypothesis space recognizing this structure, which is encoded by the graph's adjacency matrix. This is the $n \times n$ matrix $\bm{A} \equiv (a_{uv})$, where $a_{uv} = 1$ iff $u \sim v$, and $a_{uv} = 0$ otherwise. This is to say that we wish to consider functions $F \in \textsf{H}$ with $F \equiv F( \bm{X}, \bm{A} )$. Such a function is (label) \emph{\B{permutation invariant}} if $F( \bm{P} \bm{X},\, \bm{P} \bm{A} \bm{P}^{\textrm{T}} ) = F ( \bm{X}, \bm{A})$, and is  \emph{\B{permutation equivariant}} if
\begin{align}
\label{eq:graph_perm_equi}
F( \bm{P} \bm{X}, \bm{P} \bm{A} \bm{P}^T ) = \bm{P}F( \bm{X}, \bm{A})
\end{align}
for any permutation matrix $\bm{P}$. 

\begin{rmk} $\quad$ \emph{(to return to)} $\quad$ One can characterize linear $\mathfrak{S}_n$-equivariant functions on nodes, as discussed in Example~\ref{eg:node_function_permutation}, namely functions $F$ satisfying $F (\bm{P} \bm{X}) = \bm{P} F ( \bm{X} )$. From \cite{BBCV21},  \\

\subitem \emph{``One can verify any such map can be written as a linear combination of two generators, the identity and the average. As observed by Maron et al. 2018, any linear  $\mathbf{F}$ satisfying \eqref{eq:graph_perm_equi} can be expressed as a linear combination of fifteen linear generators; remarkably, this family is independent of $n \equiv \# \mathcal{V}$."}

\end{rmk}

\begin{rmk}
Amongst the generators just discussed, the geometric learning blueprint ``specifically advocates" \emph{( where?? )} for those that are also local, in the sense that the output on node $u$ directly depends on its neighboring nodes in the graph. 
\end{rmk}

We can formalize this constraint explicitly, by defining the \emph{\B{$1$-hop neighborhood}} of node $u$ as
\eq{
\mathcal{N}(u) := \{ v : \{ u,v \} \in \mathcal{E} \} ,
}
as well as the corresponding \emph{\B{neighborhood features}}, 
\eq{
\bm{X}_{\mathcal{N}(u)} := \{ \{ \, x_v : v \in \mathcal{N}(u) \, \} \} ,
}
which is a multiset ( indicated by double-brackets ) for the simple reason that distinct nodes may be decorated with the same feature vector. 



\vspace{5mm}

\begin{mdframed}
\begin{eg}

The node-wise linear transformation described in Example~\ref{eg:node_function_permutation} can be thought of as operating on `$0$-hop neighborhoods' of nodes. We generalize this with an example of a function operating on $1$-hop neighborhoods. Instead of a fixed map between feature spaces $\theta : \mathbb{R}^s \to \mathbb{R}^{s'}$ \emph{cloned} to a pointwise map $\Theta$, we specify a \emph{local} function 
	$$
	\varphi \equiv \varphi( \, x_u, \, \bm{X}_{\mathcal{N}(u)} \, )
	$$ 
	operating on the features of a node as well as those of its $1$-hop neighborhood. 
	
We may construct a permutation equivariant function $\Phi$ ( the analogue of $\Theta$ here, just as $\varphi$ corresponds to $\theta$ ) by applying $\varphi$ to each $1$-hop neighborhood in isolation, and then collecting these into a new feature matrix. As before, for $\mathcal{V} = \{ v_1, \dots, v_n \}$, we write $x_j$ in place of $x_{v(j)}$, and we similarly write $\mathcal{N}_j$ instead of $\mathcal{N}( v(j) )$.
\eq{
\Phi ( \bm{X}, \bm{A} ) = 
\left[
\begin{matrix}
\varphi( \, x_1 , \, \bm{X}_{\mathcal{N}_1} \, ) \\
\varphi( \, x_2 , \, \bm{X}_{\mathcal{N}_2} \, ) \\
\vdots \\
\varphi( \, x_n , \, \bm{X}_{\mathcal{N}_n} \, )
\end{matrix}
\right]
}

The permutation equivariance of $\Phi$ rests on the output of $\varphi$ being independent of the ordering of the nodes in $\mathcal{N}_u$. Thus, if $\varphi$ is permutation invariant ( a local averaging ) this property is satisfied. 
\end{eg}
\end{mdframed}

\vspace{5mm}

\subitem \cite{BBCV21} : \emph{``The choice of $\varphi$ plays a crucial role in the expressive power of the learning scheme."}


\begin{rmk} The authors \cite{BBCV21} emphasize that, when considering signals $x \equiv \bm{X}$ over graphs, it is natural to consider a hypothesis space whose functions operates on the pair $( \bm{X}, \bm{A})$, where $\bm{A}$ is the adjacency matrix of the signal domain. Thus, for such signals the domain naturally becomes part of the input. 

The GDL blueprint distinguishes between two contexts: one in which the input domain is fixed, and another in which the input domain varies from signal to signal. The preceding example demonstrates that, even in the former context, it can be essential that elements of $\textsf{H}$ treat the ( fixed ) domain as an argument. 

When the signal domain has geometric structure, it can often be leveraged to construct a \emph{coarsening operator}, one of the components of a GDL block in the learning blueprint. Such an operator passes a signal $x \in \cX(\Oh)$ to a signal $y \in \cX( \Oh')$, where $\Oh'$ is a `coarsened version' of $\Oh$. As the blueprint calls for a sequence of such blocks, we often wish to act on the latent signal $y$ by a pointwise non-linearity, followed by the action of an equivariant function on signals in $\cX( \Oh')$. The domain may be fixed for each input, but this domain changes as the signal passes through the layers of the NN, and it is natural that the functions the NN is built out of should pass this data forward. 

\subitem \cite{BBCV21} : \emph{``Due to their additional structure, graphs and grids, unlike sets, can be coarsened in a non-trivial way, giving rise to a variety of pooling operations... more precisely, we cannot define a non-trivial coarsening assuming set structure alone. There exist established approaches that infer topological structure from unordered sets, and those can admit non-trivial coarsening"}


\end{rmk}

\subsection{Tensors}

\subsubsection*{Coordinate transformations of vectors}

Consider a pair of orthonormal bases, the \emph{old basis} $(e_1, \dots, e_n)$ and the \emph{new basis} $(\tilde{e}_1, \dots, \tilde{e}_n )$. A signal $x \in \mathbb{R}^n$ can be expressed in either coordinate system 
\eq{
x & = e_1 x^1 + \dots + e_nx^n\, \equiv \,e_jx^j \\
x & = \tilde{e}_1 \tilde{x}^1 + \dots + \tilde{e}_n \tilde{x}^n\, \equiv\, \tilde{e}_j  \tilde{x}^j ,
}
where we have used the Einstein summation convention. We adopt this notation going forward. We can express the basis $( \tilde{e}_j )$ in terms of the $( e_j)$ via
\eq{
\tilde{e}_j = e_i \bm{S}^i_j \, ,
}
where $\bm{S}$ is called the \emph{direct transformation matrix} from the old basis to the new. We let $\bm{T}$ denote $\bm{S}^{-1}$, and note that the coordinate coefficients transform \emph{contravariantly}:
\eq{
\tilde{x}^j =  \bm{T}_i^j x^i\,.
}
We assume that the underlying vector space is $\mathbb{R}^n$, and moreover, we equip it with the usual inner product ( in order to make sense of orthogonality ), and note that the Riesz representation theorem allows us to assign a given orthonormal basis $(e_j)$ of vector space $V$ to a canonical dual basis $(e^j)$ on the dual vector space $V^*$. Elements of $V^*$ are called dual vectors, or covectors. This canonical dual basis is given by
\eq{
e^j &= \la e_j, \cdot \ra , \\
&\equiv e_i  \, \delta^{ij} ,
}
and thus,
\eq{
e_j = \delta_{ij} \, e^i. 
}


\begin{rmk} The dual basis can be thought of as a `coordinate selector,' acting on a vector $x \in V$ by 
\eq{
e^j(x) &= e^j \, e_k \, x^k \\
&=x^k \, e^j \,e_k  \\
&= \delta_k^j  \, x^k \\
&= x^j.
}   
\end{rmk}

\begin{rmk} Let us consider how the objects 
\eq{
\delta_{ij} \, \equiv \, e_i \, e_j ,\quad  \delta_i^j  \, \equiv\, e_i \, e^k ,\quad  \delta^{ij} \,\equiv \, e^i \, e^j
} 
transform with the bases, i.e. how to express the following new basis objects
\eq{
\tilde{\delta}_{ij}, \quad \, \tilde{\delta}_i^j,  \quad \, \tilde{\delta}^{ij}
}
in terms of the corresponding old, via the matrices $\bm{S}$ and $\bm{T}$. One has
\eq{
\tilde{\delta}_{ij} &= \tilde{e}_i\, \tilde{e}_j = e_{\ell}\, \bm{S}^\ell_i  \, e_k  \,\bm{S}^k_j \\ 
&=  \bm{S}^\ell_i \, \delta_{\ell k} \, \bm{S}^k_j 
} 
Likewise, one has
\eq{
\tilde{\delta}_i^j \, = \, \bm{S}_i^\ell \, \delta_\ell^k \, \bm{T}_k^j \quad \text{ and } \quad \tilde{\delta}^{ij} \, = \, \bm{T}_\ell^i \, \delta^{\ell k } \, \bm{T}_k^j
}
\end{rmk}



Given a change of basis $(e_j) \mapsto (\tilde{e}_j)$, the induced transformation $(e^j) \mapsto (\tilde{e}^j)$ is contravariant, which is consistent with the transformations described above:
\eq{
\tilde{e}^j  &\equiv \tilde{e}_i \, \delta^{ij}  \\
&= e_\ell \, \bm{S}_i^\ell \, \bm{T}_\ell^i \, \delta^{\ell k} \, \bm{T}_k^j \\
&= e_\ell \,  \delta^{\ell k } \, \bm{T}_k^{j} \\
&=  \bm{T}_k^j e^k
}

\begin{defn} A \emph{\B{tensor of type}}, or \emph{\B{valency}} $(r,s)$ over vector space $V$ is a multilinear map 
\eq{
\bm{A} : \underbrace{V^* \times \, \dots \, \times V^*}_{ k \text{ copies }}  \,\times \, \underbrace{ V \times \, \dots \, \times V }_{ \ell \text{ copies }} \to \R
}
\end{defn}

When a basis $(e_j)$ is given, we can express $\bm{A}$ with respect to this coordinate system through a total of $n^{r+s}$ scalars, denoted generically by $\bm{A}_{ \quad j_1, \, \dots\, , \, j_s} ^ { i_1, \, \dots \, , \, i_r} $. The coordinate indices $i_1, \dots, i_r$ are the \emph{\B{contravariant indices}}, while the $j_1, \dots, j_s$ are the \emph{\B{covariant indices}}. They are so-named because of the transformation law of the entries under a change of basis $(e_j ) \mapsto (\tilde{e}_j)$ induces the linear transformation
\eq{
\tilde{\bm{A}} _{ \quad j_1, \, \dots\, , \, j_s} ^ { i_1, \, \dots \, , \, i_r} 
=
\bm{T}_{k_1}^{i_1} \, \dots \, \bm{T}_{k_r}^{i_r} \, \bm{A} _{ \quad \ell_1, \, \dots\, , \, \ell_s} ^ { k_1, \, \dots \, , \, k_r} \, \bm{S}_{j_1}^{\ell_1}  \, \dots \, \bm{S}_{j_s}^{\ell_s} 
}
Let $\mathcal{T}(r,s)$ denote the vector space of type-$(r,s)$ tensors over $V$. The dimension of this vector space is $n^{r+s}$, i.e. the number of scalar entries in the coordinate representation of such a tensor. 

There are two operations on tensors of interest
\begin{itemize}
\item contraction
\item outer product
\end{itemize}

\newpage
\subsection{GDL blueprint ( with input domain fixed ) } \label{subsec:GDLblue}

\subsubsection*{Setup} 

Our formal treatment of a classification problem starts with the following objects: 

\begin{itemize}
\item[$\Oh$] : A graph with $n$ vertices. 
\item[$(e_j)_{j\,=\,1}^{\,n}$] : An `input coordinate system.' Each vector in the orthonormal basis $e_j$ corresponds to a vertex of $\Oh$. This is the basis in which signals are expressed to a computer.
\item[$\cX(\Oh,\R^s)$ ] :  The space of $s$-channel signals $x : \Oh \to \R^s$. In terms of $(e_j)$, one can write $x$ as
\eq{
x = \bm{x}^{cj} e_j ,
}
where the index $j$ varies over the nodes of $\Oh$, and where index $c$ indicates the channel. Note that $\bm{x} \equiv \bm{X}$, the so-called \emph{design matrix} previously introduced. 
\item[$G$] : A group $G$ acting on $\Oh$, via $\Oh \ni u \mapsto g.u \in \Oh$. This induces the `pointwise' action on signals, $x \mapsto \bm{g}.x$. 
\item[$\rho$] : The representation of the induced action of $G$, $x \mapsto \bm{g}.x$. Let $g \in G$. Supposing signals are expressed with respect to $(e_j)$, we can then identify $\rho(g)$ with a TENSOR...

\end{itemize}


\subsubsection*{GDL block} 


Let $\Omega$ and $\Omega'$ be domains, $G$ a symmetry group over $\Omega$, and write $\Omega' \prec \Omega$ if $\Omega'$ arises from $\Omega$ through some coarsening operator (presumably this coarsening operator needs to commute with the group action).\\

\begin{enumerate}

\item[(1)] \emph{linear $G$-equivariant layer} 
	$$
	B : \mathcal{X}(\Omega, \mathcal{C}) \to \mathcal{X}(\Omega' , \mathcal{C}')
	$$ 
	such that $B(g.x) = g.B(x)$ for all $g \in G$ and $x \in \mathcal{X}(\Omega, \mathcal{C})$.

\item[(2)] \emph{nonlinearity}, or \emph{activation function} $\quad$ 
	$$
	s : \mathcal{C} \to \mathcal{C}'
	$$ 
	applied domain-pointwise as $(\mathbf{s}(x))(u) = s(x(u))$.

\item[(3)] \emph{local pooling operator} or \emph{coarsening operator} $\quad$ 
	$$
	P : \mathcal{X}(\Omega, \mathcal{C}) \to \mathcal{X}(\Omega', \mathcal{C})
	$$ 
	which gives us our notion $\Omega' \prec \Omega$.

\end{enumerate}

The last ingredient is a global pooling layer applied last, compositionally. 

\begin{enumerate}
\item[(4)] \emph{$G$-invariant layer}, or \emph{global pooling layer} $\quad$ 
	$$
	A : \mathcal{X} (\Omega, \mathcal{C}) \to \mathcal{Y}
	$$ 
	satisfying $A(g.x) = A(x)$ for all $g \in G$ and $x \in \mathcal{X}(\Omega, \mathcal{C})$. 
\end{enumerate}


\subsubsection*{Hypothesis space} 

These objects can be used to define a class of $G$-invariant functions $f: \mathcal{X}(\Omega, \mathcal{C}) \to \mathcal{Y}$ of the form
    $$
    f = A \circ \mathbf{s}_J \circ B_J \circ P_{J-1} \circ \dots \circ P_1 \circ \mathbf{s}_1 \circ B_1 ,
    $$
where the blocks are selected so that the output space of each block matches the input space of the next one. Different blocks may exploit different choices of symmetry groups $G$.

\subsection{Discussion} 


\begin{rmk}
Shift-invariance arises naturally in vision and pattern recognition. In this case, the desired function $f \in \textsf{H}$, typically implemented as a CNN, inputs an image and outputs the probability of the image to contain an object from a certain class. It is often reasonably assumed that the classification result should not be affected by the position of the object in the image, i.e., the function $f$ must be shift-invariant.

Multi-layer perceptrons lack this property, a reason why early ( 1970s ) attempts to apply these architectures to pattern recognition problems failed. The development of NN architectures with local weight sharing, as epitomized by CNNs, was among other reasons motivated by the need for shift-invariant object classification. 

A prototypical application requiring shift-equivariance is image segmentation, where the output of $f$ is a pixel-wise image mask. This segmentation mask must follow shifts in the input image. In this example, the domains of the input and output are the same, but since the input has three color channels while the output has \emph{one channel per class}, the representations $(\rho, \mathcal{X}(\Omega, \mathcal{C}) )$ and $(\rho', \mathcal{Y} \equiv \mathcal{X}(\Omega, \mathcal{C}'))$ are somewhat different. 
\end{rmk}

\begin{rmk} When $f$ is implemented as a CNN, it may be written as a composition of $L$ functions, where $L$ is determined by the depth and other hyperparameters:
\eq{
f = f_L \circ f_{L-1} \circ \dots \circ f_2 \circ f_1 .
}

Examining the individual layer functions making up CNN, one finds they are not shift-invariant in general but rather shift-equivariant. The last function applied, namely $f_L$, is typically a ``global-pooling" function that is genuinely shift-invariant, causing $f$ to be shift-invariant, but to focus on this ignores the structure we will leverage for purposes of expressivity and regularity. 
\end{rmk}


\newpage

\section{Geometric deep learning models} 

\subsection{Learning with scalar-signals on a cyclic group}


In the simplest setting, the underlying domain is a one-dimensional grid, and the signals only have a single channel. We can identify this grid $\Oh$ with the Cayley graph of cyclic group
\eq{
C_n = \la \, a : a^n = 1 \, \ra \equiv \{ \, 1, a, a^2, \dots, a^{n-1} \, \}.
}
It is convenient to parametrize the group, and hence the grid, through the exponent of the generator 
 \eq{
 C_n \equiv \{ 0, 1, \dots, n -1 \}
 } 
as this indexing is consistent with the way most programming languages index vectors, and clearly we can reinterpret the group operation as addition modulo $n$. The vector space of single-channeled ( i.e. real-valued ) signals
\eq{
\cX(C_n,\R) = \{ x : C_n \to \R \} ,
}
is finite dimensional, and each $x \in \cX(C_n, \R)$ may be expressed as 
\eq{
x = 
\left[ 
\begin{matrix}
x_0\\ 
\vdots\\
\,x_{n-1}\,
\end{matrix}
\right] 
}
with respect to some implicit coordinate system used by the computer, the ``\B{input coordinate system}." This is the same coordinate system used by the representation $\rho$ of translation group $G \equiv C_n$, which we now describe. 

Given a vector $\theta = (\theta_0 , \dots, \theta_{n-1})$, recall the associated \emph{circulant matrix} is the $n \times n$ matrix with entries 
\eq{
\bm{C}(\theta) := \left( \, \theta_{ (u - v) \mod n} \right)_{ 0 \, \leq \,u,\,v \, \leq n-1 } 
}

Consider the case of $\theta_S := (0,1,0,\dots, 0)^T$, the associated circulant matrix, $\bm{S} := \bm{C}(\theta_S)$ acts on vectors by shifting the entries of vectors to the right by one position, modulo $n$. This is a shift or translation operator, which we denote $\bm{S}$. 

\begin{lem} A matrix is circulant if and only if it commutes with $\bm{S}$. Moreover, given any two vectors $\theta, \eta \in \mathbb{R}^n$, one has $\bm{C}(\theta) \bm{C}(\eta) = \bm{C}(\eta) \bm{C}(\theta)$. 
\end{lem}

The importance of $\bm{S}$ to the present discussion is that it generates a group isomorphic to the `one-dimensional translation group' $C_n$. This is to say, a natural representation of $C_n = \la \, a : a^n = 1 \, \ra$ to consider is the group isomorphism induced by mapping the generator $a$ of $C_n$ to $\bm{S}$. Specifically, the representation $\rho$ of $G$ over $\cX( C_n, \R)$ is given by
\eq{
\rho ( a^j ) := \bm{S}^j 
}


\begin{coro} Any $f : \cX(C_n, \R) \to \cX(C_n,\R)$ which is linear and $C_n$-equivariant can be expressed ( in the input coordinate system ) as an $n \times n$ circulant matrix $\bm{C}(\theta)$ for some vector $\theta$.  

\end{coro}



\vspace{5mm}
\begin{mdframed}
\begin{eg}
Our previous recipe for designing an equivariant function $F= \Phi( \bm{X}, \bm{A})$ using a local aggregation function $\varphi$. In this case, we can express
$$
\varphi ( \mathbf{x}_u, \mathbf{X}_{\mathcal{N}(u)} ) = \varphi( \mathbf{x}_{u-1}, \, \mathbf{x}_u, \, \mathbf{x}_{u+1} ),
$$
where the addition and subtraction in the indices above is understood to be modulo $n$. 

 If in addition, we insist that $\varphi$ is linear, then it has the form 
$$
 \varphi( \mathbf{x}_{u-1}, \, \mathbf{x}_u, \, \mathbf{x}_{u+1} ) = \theta_{-1} \mathbf{x}_{u-1} + \theta_0 \mathbf{x}_u + \theta_1 \mathbf{x}_{u+1},
$$
and in this case we can express $\mathbf{F} = \Phi (\mathbf{X}, \mathbf{A} )$ through the following matrix multiplication:
$$
\left[
\begin{matrix}
\theta_0 & \theta_1 & \text{ } & \text{ } & \theta_{-1} \\
\theta_{-1} & \theta_0 & \theta_1 & \text{ } &   \text{ } \\
\text{} & \ddots & \ddots & \ddots & \text{ } \\
\text{ } & \text{ } & \theta_{-1} & \theta_0 & \theta_1 \\
\theta_1 & \text{ } & \text{ } & \theta_{-1} & \theta_0 
\end{matrix} 
\right]
\left[
\begin{matrix}
\mathbf{x}_0 \\
\mathbf{x}_1 \\
\vdots \\
\,\mathbf{x}_{n-2} \, \\
\mathbf{x}_{n-1}  
\end{matrix}
\right]
$$
This special multi-diagonal structure is sometimes referred to as ``weight sharing" in the machine learning literature. 
\end{eg}
\end{mdframed}

\vspace{5mm}

Circulant matrices are synonymous with discrete convolutions; for $x \in \cX(\Oh,\R)$ and $\theta \in \R^n$, their \B{convolution} $x \star \theta$ is defined by 
\eq{
( x \star \theta )_u &:= \sum_{v = 0}^{n-1} x_{v \mod n}\, \theta_{ (u-v) \mod n}  \, ,\\
&\equiv \bm{C}(\theta) x 
}

\begin{rmk}
This leads to a possible alternate but equivalent \emph{definition} of convolution as a translation equivariant linear operation, moreover by replacing `translation' by a more general group $G$, one can generalize convolution to settings whose domain has symmetry other than translational. 
\end{rmk} 


\subsection{A simple CNN}

Thus, we come to possibly the simplest neural network that we can construct through the above blueprint. Suppose we have a binary classification problem, with the following hypothesis space. Let $\textsf{H}_1$ denote the hypothesis space of functions $f : \mathcal{X}( C_n, \R) \to \{0,1\}$ of the form 
\eq{
f = A \circ P_1 \circ \bm{r}_1 \circ B_1 \,,
}
where the components of $f$ are 
\begin{itemize}

\item [ $B_1$ ] : $\quad$ A $C_n$-equivariant function, to be learned. It is represented as a circulant matrix $\bm{C}(\theta)$, where $\theta$ is a vector $\theta \equiv (\theta_0, \dots, \theta_{n-1})$ whose entries $\theta_j$ are parameters to be learned. 

\item[ $ \bm{r}_1 $ ] : $\quad$ We consider the ReLU activation function, $r_1 : \mathbb{R} \to \mathbb{R}_{\geq\, 0}$ defined by $r_1(w) = \max(0,w)$, for $w \in \mathbb{R}$. The bold-face denotes the entry-wise action of this function on a given vector, thus for $y \equiv (\,y_1, \,\dots, \, y_n \, ) \in \cX(C_n, \R)$, which we imagine as the output of $B_1(x)$ for some input signal $x$, we have $\bm{r}_1 (y ) = ( \,  \max(0,y_1), \,  \dots, \, \max(0,y_n) )$. There are no parameters to be learned, in this layer. 

\item [ $P_1$ ] : $\quad$ A coarsening operator. In this case, let us say it is a group homomorphism $P_1 : C_n \to C_{n / d }$ for some divisor $d \mid n$ \footnote{zero-padding} , and let us say that it operates through max-pooling on the signal, over the pre-images of each element of $C_{n / d}$. 

\item[ $A$ ] : $\quad$ A global-pooling layer. We assume this has the form of a fully-connected layer, followed by a softmax. Specifically,

\subitem 

\subitem  

\end{itemize}

The categorical notation 
\eq{
\Oh_1 \equiv C_n  \xrightarrow[\,]{ f} \Oh  \xrightarrow[\,]{ g}  \xrightarrow[\,]{ k} d
}
is nice, as it makes explicit the domain at each stage in the composition. 

\newpage
\subsection{Implementation in TensorFlow keras} 

Let us make clear which libraries we are importing:

\vspace{5mm}

\begin{mdframed}
\begin{verbatim}
import tensorflow as tf 

from tensorflow.keras import datasets, layers, models
\end{verbatim}
\end{mdframed}

\vspace{5mm}

\begin{rmk}
A note on `\href{https://www.w3schools.com/python/python_inheritance.asp}{python inheritance}:' Inheritance allows us to define a class that inherits all the methods and properties from another class. The parent class is the class being inherited from, also called base class. The child class is the class that inherits from another class, also called derived class.

In the keras API, the \B{layers} class is the child of the \B{models} class, and all concrete layer methods inherit from the \B{layers} class, such as \B{Conv2D}. 
\end{rmk}

Before considering a simple GDL block, let us examine how to initialize the specific layer type, \B{Conv2D}, which plays the role of the $G$-equivariant function in a generic GDL block, where $G$ is the translation group. 

\vspace{5mm}

\begin{mdframed}
\begin{verbatim}
tf.keras.layers.Conv2D(
    filters, 
    kernel_size, 
    strides=(1, 1), 
    padding='valid',
    data_format=None, 
    dilation_rate=(1, 1), 
    groups=1, 
    activation=None,
    use_bias=True, 
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros', 
    kernel_regularizer=None,
    bias_regularizer=None, 
    activity_regularizer=None, 
    kernel_constraint=None,
    bias_constraint=None, **kwargs
)
\end{verbatim}
\end{mdframed}

Let us discuss each argument of the \B{Conv2D} method:

\begin{itemize}
\item \emph{filters}: This is the ``dimension of the output space," namely the number of output channels. Given that this layer is plays the role of a $G$-equivariant layer, $E : \cX(\Oh, \R^s) \to \cX(\Oh, \R^{s'})$, the filters argument is the value of $s'$.  
\item \emph{kernel size}: the height and width of the two-dimensional convolution window. 
\item \emph{strides and dilation rate}: placing a non-trivial argument ( $\neq 1$ ) in both of these slots will raise an error. 
\end{itemize}

Let us consider the following GDL block: 

\vspace{5mm}

\begin{mdframed}
\begin{verbatim}
model.add( 
	layers.Conv2D( 
		32, 
		(3,3), 
		activation = relu, 
		input_shape = (32,32,3) 
		) 
	)
model.add(layers.MaxPooling2D( (2,2) ) 
\end{verbatim}
\end{mdframed}

\subsection{Convolutional neural networks} 

In section (4.2), we have fully characterized the class of linear and local translation equivariant operators, given by convolutions $C(\theta) x = x \star \theta$ with a localized filter $\theta$. Let us first focus on scalar-valued (`single-channel' or `grayscale') discretized images, where the domain is the grid $\Oh = [H] \times [W ]$, and $\mathbf{x} \in \cX(\Oh,\R)$, with generic $u \in \Oh$ having coordinate description $u = (u_1, u_2)$. 

Any convolution with a compactly supported filter of size $H^f \times W^f$ can be written as a linear combination of generators $\theta_{1,1}, \dots, \theta_{H^f, W^f}$, given for example by the unit peaks
\eq{
\theta_{vw} (u_1, u_2) = \delta(u_1 - v, u_2 - w) .
}
Any local linear equivariant map is thus expressible as
\eq{
F(\mathbf{x}) = \sum_{v=1}^{H^f} \sum_{w = 1}^{W^f} \alpha_{vw} C(\theta_{vw} ) \mathbf{x} \,, 
}

\begin{rmk} Note that we usually imagine $\mathbf{x}$ and $\theta_{vw}$ as matrices, but in the above display, both $\mathbf{x}$ and $\theta_{vw}$ have their two coordinate dimensions flattened into one, making $\mathbf{x}$ a vector and $C(\theta_{uv})$ a matrix. 

\end{rmk}

In coordinates, this corresponds to the familiar two-dimensional convolution:
\eq{
F(\mathbf{x}) = \sum_{a=1}^{H^f} \sum_{b = 1}^{W^f} \alpha_{ab} x_{u + a,\, v+b} 
}

Other choices of the basis $\theta_{vw}$ are also possible, and will yield equivalent operations (for potentially different choices of $\alpha_{uv}$). A popular example are \emph{``directional derivatives"}:
\eq{
\theta_{vw}(u_1, u_2) = \delta(u_1, u_2) - \delta(u_1- v, u_2 - w), \quad (v,w) \neq (0,0) 
}
taken together with the local average, $\theta_0 (u_1, u_2) = \tfrac{1}{ H_f W_f }$ . These are related to diffusion processes on the grid -- \cite{BBCV21}.

When the scalar input channel is replaced by multiple channels (e.g., RGB colors, or more generally an arbitrary number of feature maps), the convolutional filter becomes a \emph{convolutional tensor} expressing arbitrary linear combinations of input features into output maps. In coordinates, this can be expressed as
\begin{align}
\label{eq:conv_layer}
F(\mathbf{x})_{uvj} 
	= 
		\sum_{a =1}^{H^f} 
			\sum_{b=1}^{W^f} 
				\sum_{c = 1}^M 
					\alpha_{jabc} 
					x_{u+a, v+ b, c}
					\quad
					j \in [ N ] 
\end{align}
where $M$ and $N$ are respectively the number of input and output channels. 

\subsubsection*{Efficient multiscale computation}

As discussed in the GDL template for general symmetries, extracting translation invariant features out of the convolutional operator $F$ requires a non-linear step. Convolutional features are processed through a non-linear \emph{activation function} $\sigma$, acting element-wise on the input, i.e. $\sigma : \cX(\Oh) \to \cX(\Oh)$, as $\sigma(\mathbf{x}) (u ) = \sigma( \mathbf{x}(u) )$. 

The most popular example at the time of writing is the \B{rectified linear unit (ReLU)},
\eq{
\sigma(x) = \max(x,0) 
}

\subitem \cite{BBCV21} : \emph{``This non-linearity effectively} rectifies \emph{the signals, pushing their energy towards lower frequencies, and enabling the computation of high-order interactions across scales by iterating the construction"}\\

In the early works of Fukushima and Miyake (1982) and LeCun et al. 1998, CNNs and similar architectures had a multiscale structure, where after each convolutional layer \eqref{eq:conv_layer} one performs a grid coarsening $P : \cX( \Oh) \to \cX (\Oh')$, where the grid $\Oh'$ has coarser resolution than $\Oh$. 

This allows for multiscale filters with effectively increasing receptive field, yet retaining a constant number of parameters per scale. Several coarsening strategies( called  \emph{pooling} ) may be used, the most common are applying a low-pass anti-aliasing filter ( e.g. local average ) followed by grid downsampling, or non-linear max pooling (?) 

In summary, a `vanilla' CNN layer can be expressed as a composition of basic objects from the learning blueprint, 
\eq{
h = P( \sigma (F (x))
}

\subsubsection*{Deep and residual networks} 

A CNN architecture in its simplest form is specified by the hyperparameters
\eq{
(H_k^f, W_k^f, N_k, p_k )_{ k \leq K},
}
with $M_{k+1} = N_k$ and $p_k = 0,1$ indicating whether grid coarsening is performed or not. While all these hyperparameters are important in practice, a particularly important question is to understand the role of depth $K$ in CNN architectures, and what are the fundamental tradeoffs involved in choosing such a key hyperparameter, especially in relation to the filter sizes $(H_k^f, W_k^f)$. 

While a rigorous answer to this question is still elusive, mounting empirical evidence collected throughout recent years suggests a favorable tradeoff towards deeper (large $K$) yet thinner (small $(H_k^f, W_k^f)$).

A crucial insight by He et al. (2016) was to reparametrize each convolutional layer to model a perturbation of the previous features, rather than a generic non-linear transformation:
\eq{
h = P( x + \sigma (F(x)) )
}

The resulting residual networks provide several key advantages over the previous formulation. In essence, the residual parametrization is consistent with the view that the deep network is a discretization of an underlying continuous dynamical system, modelled as an ODE. Crucially, learning a dynamical system by modeling its velocity turns out to be much easier than learning its position directly. In our learning setup, this translates into an optimization landscape with more favorable geometry, leading to the ability to train much deeper architectures than before.

The key advantage of the ResNet parametrization has been rigorously analyzed in simple scenarios (Hardt and Ma, 2016), and remains an active area of theoretical investigation. 

Finally, neural ODEs ( Chen et al. 2018 ) are a recent popular architecture that pushes the analogy with ODEs even further, by learning the parameters of the ODE $\dot{\mathbf{x}} = \sigma ( F( \mathbf{x} ))$

\subsubsection*{Normalization} 

Another important algorithmic innovation that boosted empirical CNN performance significantly is the notion of \B{normalization}. In early models of neural activity, it was hypothesized that neurons perform some form of local `gain control', where the layer coefficients $x_k$ are replaced by 
\eq{
\tilde{x}_k = \sigma_k^{-1} \odot ( x_k - \mu_k) ,
}
where $\mu_k$ and $\sigma_k$ encode the first- and second-order moment information of $x_k$ respectively. \emph{``Further, they can be either computed globally or locally"}

In the context of Deep Learning, this principle was widely adopted through the \B{batch normalization layer} ( Ioffe-Szegedy, 2015 ), followed by several variants. Despite some attempts to rigorously explain benefits of normalization in terms of better conditioned optimization landscapes ( Santurkar et al., 2018 ), a general theory that can provide guiding principles is still missing at the time of writing.

\subsubsection*{Data augmentation} 

While CNNs encode the geometric priors associated with translation invariance and scale separation, they do not explicitly account for other known transformations that preserve semantic information, e.g. lighting or color changes, or small rotations and dilations.

A pragmatic approach to incorporate these priors with minimal architectural changes is to perform \B{data augmentation}, where one manually performs said transformations to the input images and adds them to the training set.

Data augmentation has been empirically successful and is widely used -- not only to train statee-of-the-art vision architectures, but also to prop up several developments in self-supervised and causal representation learning ( Chen et al., 2020; Grill et al., 2020; Mitrovic et al., 2020 ). However, it is provably sub-optimal in terms of sample complexity ( Mei et al., 2021 ); a more efficient strategy considers instead architectures with richer invariance groups, as we discuss next.

\subsection{Group-equivariant CNNs}  

As previously discussed, we can generalize the convolution operations from signals on Euclidean space to signals on any \emph{homogeneous space $\Oh$} acted upon by a group $G$. By analogy to the Euclidean convolution where a \emph{translated} filter is matched with the signal, the idea of group convolution is to move the filter around the domain using the group action. By virtue of the transitivity of the group action, we can move the filter to any position on $\Oh$. \footnote{ So vertex-transitive graphs are the discrete analogue of a homogeneous space } We now discuss several concrete examples of the general idea of group convolution, including implementation aspects and architectural choices. 

\subsubsection*{Discrete group convolution} 

We begin by considering the case in which $\Oh$ and $G$ are both discrete. 

\begin{itemize}
\item[(1)] As our first example, we consider medical volumetric images represented as signals on a three-dimensional grid. We think of input signals, a priori supported on a finite discrete box, as signals on $\Oh \equiv \Z^3$ through \B{zero-padding}. As symmetry group $G$, we consider ``$\mathbb{Z}^3 \rtimes O_h$" \footnote{ Does $\rtimes$ just denote the cartesian product for them? } Here $O_h$ is the octahedral group \footnote{( do we need a prefix `special' ? ) } generated by rotations about the three coordinate axes, so these are all orientation-preserving graph automorphisms of the lattice.  
\item[(2)] As our second example, consider DNA sequences made up of four letters: C, G, A, T. The sequences are represented as signals on the one-dimensional grid $\Oh = \Z$, which take values in $\mathbb{R}^4$ naturally, through the \B{one-hot} encoding of each letter, i.e. each letter is identified with a distinct standard basis element of $\mathbb{R}^4$. Naturally, we can consider the discrete one-dimensional translation symmetry group $\mathbb{Z}$ as a factor of $G$ acting on such signals. 

DNA sequences have an additional interesting symmetry. This symmetry arises from the way DNA is physically embodied as a double helix, and the way it is read by the molecular machinery of the cell. Each strand of the double helix begins with what is called the $5'$-end and ends with a $3'$-end, with the $5'$ on one strand complemented by a $3'$ on the other strand. In other words, the two strands have opposite orientation.

Since the DNA molecule is always read starting at the $5'$-end, but we do not know which one, a sequence such as ACCCTGG is equivalent to the reversed sequence with each letter replaced by its complement, CCAGGGT. This is called \B{reverse-complement symmetry} of the sequence. The two element group $\mathbb{Z}_2 = \{0,1\}$ thus acts on strings in the following way: $0$ is represented as the identity, while $1$ is represented as the reverse-complement. The acting symmetry group is thus $\mathbb{Z} \times \mathbb{Z}_2$. 

In this case, group convolution defined earlier is given as
\eq{
(x \star \theta) (g)  = \sum_{u \in \Oh} x_u \rho(g) \theta_u,
}
the inner product between the (single-channel) input signal $x$ and a $g$-transformed filter $\theta$ by some $g \in G$, via $\rho(g) \theta_u = \theta_{g^{-1} u }$, and the output $x \star \theta$ is a function on $G$. 
\end{itemize}

\subsubsection*{Transform+convolve approach} 

We show that group convolution can be implemented in two steps: a filter transformation step, and a translational convolution step. 

In the context of the two examples mentioned, the filter transformation step consists of creating rotated (or reverse-complement transformed) copies of a basic filter, while the translational convolution is the same as in standard CNNs, and thus efficiently computable on hardware such as GPUs. 

To see this, note that in both our examples (.. have product structure $G = T \times R$ ..) , we can write a general transformation $g \in G$ as a composition $g = tr$ of a ``rotation" $r \in G$ and a translation $t \in G$. That $\rho$ is a group representation, we have $\rho(g) = \rho(t) \rho(r)$, so
\eq{
( x \star \theta ) (tr ) &= \sum_{ u \, \in \, \Oh } x_u \rho(t) \rho(r) \theta_u \\
&=  \sum_{u \, \in \, \Oh } x_u ( \rho(r) \theta) _{u-t} 
}
The word rotation is in quotes because we can think of $r$ as either a genuine rotation, as in the first example, where $r \in O_h$, or as an action of the reverse complement symmetry, $r \in \mathbb{Z}_2$, in the second example. The last equation above is the standard ( planar Euclidean ) convolution of the signal $x$ and the transformed filter $\rho(h) \theta$. 

\vspace{5mm}

\begin{mdframed}
Thus, to implement group convolution for these groups, we take the canonical filter $\theta$, create transformed copies $\theta_r = \rho(r) \theta$ for each $r \in R$, and then convolve $x$ with each of these filters: $(x \star \theta)(tr) = (x \star \theta_r) (t)$.

For both of our examples 
\end{mdframed}

\vspace{5mm}

\newpage 

\section{Fourier analysis on groups}

\subsection{Fourier analysis on a cyclic group} 

\n\hrulefill 
\emph{ Derivation of the discrete Fourier transform }
\hrulefill

\begin{itemize}
\item Recall that (diagonalizable) matrices are \emph{jointly diagonalizable} if and only if they mutually commute. 
\item Thus, there exists a common eigenbasis for all the circulant matrices, and we can compute this basis by finding the eigenvectors of \emph{any} circulant matrix.
\item The basis of the shift operator happens to be the discrete Fourier basis. For $k = 0,1, \dots, n-1$, let 
$$
\bm{\varphi}_k = \frac{1}{\sqrt{n}} \left( 1, \,e^{2\pi \rmi k / n}, \, e^{4\pi \rmi k / n},\, \dots, \, e^{2\pi \rmi (n-1)k / n} \right)
$$
\item We can arrange this basis into an $n \times n$ ``Fourier matrix" $\Phi = ( \bm{\varphi}_0, \, \dots, \, \bm{\varphi}_{n-1} )$. Multiplication by $\Phi^*$ gives the \emph{discrete Fourier transform (DFT)}, and multiplication by $\Phi$ gives the inverse DFT, 
$$
\hat{x}_k = \frac{1}{\sqrt{n}} \sum_{u = 0}^{n-1} x_u e^{ - 2\pi \rmi ku /n }\,, \quad x_u = \frac{1}{\sqrt{n}} \sum_{u = 0}^{n-1} \hat{x}_k e^{+ 2\pi \rmi ku / n} 
$$
\item The eigenvalues of $\mathbf{C}(\bm{\theta})$ are the Fourier transform of the filter, which is a restatement of the convolution theorem:
$$
\mathbf{C}(\bm{\theta}) \bm{x} 
= \Phi \left[ 
\begin{matrix}\,
\hat{\theta}_0 & \text{ } & \text{ } \\
\text{ } &\ddots &  \text{ } \\
\text{ } & \text{ } & \hat{\theta}_{n-1} 
\,\end{matrix}
\right]
\Phi ^* \bf{x} = \Phi ( \hat{\bm{\theta}} \odot \hat{\bm{x}} )
$$
\item Because the Fourier matrix $\Phi$ has special algebraic structure, the matrix products $\Phi^* \bm{x}$ and $\Phi \bm{x}$ can be computed with $O( n \log n)$ complexity using a \emph{Fast Fourier Transform (FFT)} algorithm. This is a reason for the popularity of frequency domain filtering. Moreover, the filter is typically designed directly in the frequency domain, so $\hat{\bm{\theta}}$ is never explicitly computed. 
\end{itemize}

\n\hrulefill 
\emph{ On generalizing Fourier transform, and discussion of continuous version }
\hrulefill

\begin{itemize}
\item Realizing that the adjacency matrix of the ring graph is exactly the shift operator, one can develop the graph Fourier transform and an analogy of the convolution operator by computing the eigenvectors of the adjacency matrix. 
\item Attempts to develop graph neural networks by analogy to CNNs, sometimes termed \emph{spectral GNNs}, exploited this exact blueprint. We will see later that this analogy has some important limitations: the first comes from the fact that a grid is fixed, and hence all signals on it can be represented in the same Fourier basis. On general graphs, the Fourier basis depends on the structure of the graph. ``Hence, we cannot directly compare Fourier transforms on two different graphs, a problem that translated into a lack of generalization in machine learning problems. "
\item Moreover without any notions of direction, we can only organize the Fourier basis functions by their frequencies (energies), and graph filters built in this way are oblivious of direction\footnote{but why is this important?}
\item We now consider the continuum analogue of the above discussion. Consider functions defined on $\Omega = \mathbb{R}$, and the translation operator $(S_v f ) (u) = f( u - v)$. 
\item As previously discussed, if we apply $S_v$ to the Fourier basis functions $\varphi_\xi(u) = e^{ \rmi \xi u } $ yields, by associativity of the exponent, 
$$
S_v e^{\rmi \xi u } = e^{- \rmi \xi v } e^{\rmi \xi u} ,
$$
which implies $\varphi_\xi $ is a complex eigenvector of $S_v$ with complex eigenvalue $e^{-\rmi \xi v}$. 
\item Moreover, the spectrum of the translation operator is simple, which means that any two functions with the same eigenvalue must be co-linear: suppose that $S_v f = e^{-\rmi \xi_0 v } f$ for some frequency $\xi_0$. Taking the Fourier transform of both sides, we have for all $\xi$,
$$
e^{\rmi \xi v } \hat{f}(\xi) = e^{- \rmi \xi_0 v} \hat{f}(\xi),
$$
implying $\hat{f}(\xi) = 0$ for $\xi \neq \xi_0$, thus $f = \alpha \varphi _{\xi_0}$. 
\item For a general linear operator $C$ that is translation equivariant, which is to say $S_v C = C S_v$, we have
$$
S_v C e^{\rmi \xi u} = CS_v e^{\rmi \xi u} = e^{-\rmi \xi v } C e^{\rmi \xi u} ,
$$
and thus $Ce^{\rmi \xi u}$ is also an eigenfunction of $S_v$ with eigenvalue $e^{-\rmi \xi v}$, from which we once again use simplicity of the Fourier spectrum to conclude that $Ce^{\rmi \xi u } = \beta \varphi_\xi (u)$.
\item In other words, the Fourier basis is the eigenbasis of all translation equivariant operators. 
\item As a result, $C$ is diagonal in the Fourier domain, and can be expressed as $Ce^{\rmi \xi u} = \hat{p}_C (\xi) e^{\rmi \xi u}$, where $\hat{p}_C(\xi)$ is a \emph{transfer function} acting on different frequencies \footnote{so this just deforms one diagonal matrix into another} 
\item For an arbitrary function $x(u)$, by linearity,
\begin{align*}
(Cx) (u) &= C \int_{-\infty}^\infty \hat{x} (\xi) e^{\rmi \xi u} \textrm{d} \xi \\
&= \int_{-\infty}^\infty \hat{x}(\xi) \hat{p}_C(\xi) e^{\rmi \xi u} \textrm{d} \xi \\
&= \int_{-\infty}^\infty p_C(v) x(u - v) \textrm{d} v \\
&= ( x \star p_C) (u) ,
\end{align*}
where $p_C(u)$ is the inverse Fourier transform of $\hat{p}_C(\xi)$. It thus follows that every linear translation equivariant operator is a convolution. \footnote{(from text) The spectral characterization of the translation group is a particular case of a more general result in functional analysis, \emph{Stone's theorem}}
\end{itemize}





\subsubsection*{Fourier analysis on $\mathbb{R}$} 


Before describing multiscale representations we review some concepts in harmonic analysis. Arguably the most famous signal decomposition is the \emph{Fourier transform}. In one dimension, 

    $$
    \hat{x} (\xi ) \triangleq \int_{-\infty}^\infty x(u) \exp ( - \textrm{i} \xi u ) \textrm{d} u 
    $$
    
    expresses the function $x \in L^2(\Omega)$ on the domain $\Omega \equiv \mathbb{R}$ as a linear combination of orthogonal oscillating \emph{basis functions} $\varphi_\xi(u) = \exp ( \textrm{i} \xi u ) $ indexed by their \emph{frequency} $\xi$, i.e. their rate of oscillation. 

\begin{itemize}


    
\item Recall that the \textbf{convolution} of signal $x$ with \textbf{filter} $\theta$ is
    $$
    (x \star \theta) (u ) \triangleq \int_{\mathbb{R}} x(v) \theta(u -v ) \textrm{d} v ,
    $$
    
    which is a standard model of linear signal filtering. The \textbf{cross-correlation} of $x$ with $\theta$ is identical to the convolution up to a sign change in the argument of the filter:
    $$
    (x \star \theta) (u) \triangleq \int_{\mathbb{R}} x(v) \theta (u + v),
    $$
\end{itemize}

\n\hrulefill
\begin{rmk}
Following what seems to be a common ML convention, we use the \emph{same notation} for both transforms: ``since the filter is typically learnable, the distinction is purely notational. "
\end{rmk}
\n\hrulefill

\begin{itemize}
    
\item The Fourier transform intertwines convolution and multiplication, which is to say that for all frequences $\xi \in \mathbb{R}$,
    $$
    \widehat{x \star \theta} (\xi) = \hat{x} (\xi) \cdot \hat{\theta}(\xi)
    $$
    
\item Many fundamental differential operators such as the Laplacian are described as convolutions on Euclidean domains. Such operators can be defined intrinstically over very general geometries, providing a formal procedure of extending objects to the menagerie of domain-types we consider.

\item An essential aspect of Fourier transforms is that they reveal \emph{global} properties of the signal and the domain, such as smoothness or conductance. Such global behavior is convenient for studying domains with many global symmetries, such as translation, but less so for the more general symmetries we consider. 

\item We require a signal decomposition able to trade between spatial and frequential localisation, as we see next.

\end{itemize}


\begin{itemize}
\item Our discussion of grids highlighted how shifts and convolutions are intimately connected: convolutions are linear shift-equivariant operations, and any shift-equivariant linear operator is a convolution. These operators are diagonalized by the Fourier transform.
\item Both convolution and the Fourier transform can be defined for any group of symmetries that we can sum or integrate over. 
\item Consider once again $\Omega =\mathbb{R}$. We can understand convolution as a pattern-matching operation: we match shifted copies of filter $\theta(u)$ with an input signal $x(u)$. The value of the ``convolution" (but really, it is cross-correlation defined below) \footnote{ I think I've asked this before, but is there any utility to the hypothesis space to allow $\pm1$ in argument of filter to be learned, to allow both convolution and cross-corellation to coexist, going from layer to layer? This sounds like it messes with equivariance, and is not well-posed.  }  $(x \star \theta) (u )$ at a point $u$ is the inner product of the signal $x$ with the filter shifted by $u$:
$$
( x \star \theta ) (u ) = \int_{\mathbb{R}} x(v) \theta(u + v) \textrm{d} v 
$$
\item Note that in this case, $u$ is both a point on the domain $\Omega = \mathbb{R}$, and also an element of the translation group, which we can identify with the domain itself. 
\item Thus we can generalize the above construction by replacing the translation group by another acting on $\Omega$. 
\end{itemize}

\n\hrulefill 
\emph{ Group convolution }
\hrulefill

\begin{itemize}
\item As discussed previously in abstract, the action of the group $G$ on the domain $\Omega$ induces a \emph{representation} $\rho$ of $G$ on the space of signals $\mathcal{X}(\Omega)$ via $\rho(g) x(u) = x(g^{-1} u)$.
\item In the previous example, $G$ is the translation group, whose elements act by shifting the coordinates $u+v$, whereas $\rho(g)$ is the shift operator acting on signals as $(S_v x)(u) = x(u - v)$. 
\item In the abstract setting, in order to apply a filter to the signal, we invoke our assumption of $\mathcal{X}(\Omega)$ being a Hilbert space, with an inner product\footnote{so (why) is it wrong to view the filter as a signal? And do we need completeness or just an inner product?} 
$$
\langle x , \theta \rangle = \int_{\Omega} x(u) \theta(u) \textrm{d}u 
$$
\item Having defined how to transform signals (with the group) and match them with filters, we can define the \emph{group convolution} for signals on $\Omega$:
$$
( x \star \theta ) (g) = \langle x, \rho(g) \theta \rangle ,
$$
and we note that $x \star \theta$ takes as argument elements $g$ of the group $G$, rather than points in the domain $\Omega$.\\
\item Hence the next layer, which takes $x \star \theta$ as input, should act on signals defined on the group $G$, a point we return to shortly.
\item Just as the traditional Euclidean convolution is shift-equivariant, the generalized convolution is $G$-equivariant. 
\item Here is a statement whose importance I don't yet understand: ``the key observation is that matching the signal $x$ with a $g$-transformed filter $\rho(g) \theta$ is the same as matching the inverse transformed signal $\rho(g^{-1}) ) x$ with the untransformed filter. We can express this as
$$
\langle x , \rho(g) \theta \rangle = \langle \rho(g^{-1})x, \theta \rangle ,
$$
a statement relating the adjoint of $\rho(g)$ to its inverse. The $G$-equivariance of the group convolution follows from
\begin{align*}
( \rho(h) x \star \theta ) (g) &= \langle \rho(h) x , \rho(g) \theta \rangle \\
&= \langle x , \rho(h^{-1} g) \theta \rangle \\
&= \rho(h) (x \star \theta) (g) 
\end{align*} 
\end{itemize}

\n\hrulefill 
\emph{ Examples using specific groups }
\hrulefill

\begin{itemize}
\item We have studied above the case of a one-dimensional grid, the choice of domain here is $\Omega = \mathbb{Z}_n = \{ 0 , \dots, n-1\}$ and the cyclic shift group $G = \mathbb{Z}_n$. 
\item In this case, an element $g \in G$ is identified with some $u = 0, \dots, n-1$ such that $g.v = v -  u \mod n$, whereas the inverse element is $g^{-1} . v = v + u \mod n$. 
\item Importantly, in this example the elements of the group (shifts) are elements of the domain (``indices"). We can thus, with some abuse of notation, identify the two structures (i.e. , $\Omega = G$); our expression for group convolution in this case
$$
( x \star \theta ) (g) = \sum_{ v = 0}^{n-1} x_v \theta_{g^{-1} v } 
$$
leads to the familiar convolution $( x \star \theta) = \sum_{ v = 0}^{n-1} x_v \theta_{v + u \mod n}$. 

\end{itemize}


\n\hrulefill 
\emph{ Spherical convolution }
\hrulefill

\begin{itemize}
\item Now consider the two-dimensional sphere $\Omega = \mathbb{S}^2$ with the group of rotations, the \emph{special orthogonal group} $G = SO(3)$. 

\item This is a pedagogical choice, but has practical applications. In astrophysics, for example, observational data often naturally has spherical symmetry. Furthermore, spherical symmetries are very important in applications in chemistry when modeling molecules and trying to predict their properties.

\item  Representing a point on the sphere as a three-dimensional unit vector $u$, the action of the group can be represented as a $3 \times 3$ orthogonal matrix $\matR$ with $\det(\matR) = 1$. The spherical convolution can thus be written as the inner product between the signal and the rotated filter,
$$
(x \star \theta ) (\matR)  = \int_{S^2} x(u) \theta(\matR^{-1} u) du
$$

\item Note that the group is now not identical to the domain; $\text{SO}(3)$ is a Lie group, a ($3$-)manifold with group structure, while $S^2$ is two-dimensional.  

\item Consequently, convolution is a function on $\text{SO}(3)$, rather than on $\Omega$. 

\item This has important practical consequences: in our geometric deep learning blueprint, we concatenate multiple equivariant maps ( ``layers" in deep learning jargon ) by applying a subsequent operator to the output of the previous one. 

\item In the case of translations, we can apply multple convolutions in sequence, since their outputs are all defined on the same domain $\Omega$. In the general setting, since $x \star \theta$ is a function on $G$ rather than $\Omega$, we cannot use exactly the same operation subsequently -- and this means the next operation must consider or ``deal with" signals son $G$, i.e. $x \in \mathcal{X}(G)$. 

\item Our definition of group convolution allows for this: we can always consider $G$ as a domain $\Omega$, which is acted upon by group $G$, namely itself. 

\item This yields the representation $\rho(g)$ acting on $x \in \mathcal{X}(G)$ by $(\rho(g) x ) (h) = x(g^{-1}h )$. 

\item Just as before, the inner product is defined by integrating the point-wise product of the signal and the filter over the domain, which now equals $\Omega = G$. 

\item In our example of spherical convolution, a second layer of convolution would thus have thee form
$$
((x \star \theta) \star \eta) (\matR) = \int_{\text{SO}(3)} ( x \star \theta) ( \matQ ) \eta ( \matR^{-1} \matQ ) \textrm{d} \matQ 
$$

\item As convolution involves an inner product defined through integration over the domain $\Omega$, we can only use\footnote{in practice? They seem to imply that six dimensions is not small enough, which is surprising: ``likewise, integrating over the affine group in three dimensions, which has a six-dimensional linear representation} it on domains $\Omega$ that are small (in the discrete case) or low-dimensional (in the continuous case)
\item For instance, we cannot in practice perform convolutions with respect to the group of permutations.
\item Nevertheless, we can still build equivariant convolutions for large groups $G$ by working with signals defined on low-dimensional spaces $\Omega$ on which $G$ acts. Indeed, it is possible to show that any equivariant linear map $f: \mathcal{X}(\Omega) \to \cX(\Oh')$ between two domains $\Omega, \Omega'$ can be written as a generalized convolution, similar to the group convolution discussed here.
\item The Fourier transform we derived in the previous section from the shift-equivariance property of the convolution can also be extended to a more general case by projecting the signal onto the matrix elements of irreducible representations of the symmetry group. In the case of $\SO(3)$, this gives rise to \emph{spherical harmonics}. 
\item Finally, we point to the assumption that has so far underpinned our discussion in the section: whether $\Omega$ was a grid, plane or sphere, we could transform every point into any other point (vertex transitivity), intuitively meaning that all the points on the domain ``look the same." A domain with such a property is a \emph{homogeneous space}, and this simply generalizes the definition of vertex transitivity: for any $u,v \in \Omega$, there is $g \in G$ such that $g.u = v$. 
\end{itemize}

\newpage

\section{Geometric objects}


\begin{itemize}
\item In a previous example, we considered $\bbS^2$ as a manifold, and one with global symmetry structure ( it is a homogeneous space ). 
\item The majority of manifolds do not have such global symmetries, and in this case, we can't straightforwardly define an action of $G$ on the space of signals over $\Omega$ and use it to ``slide" filters around the manifold to construct a convolution. 
\item Nonetheless, manifolds have two types of invariance that we will explore in this section: transformations preserving metric structure and local reference frame change. 
\item In computer graphics and vision, manifolds are a common mathematical model of surfaces (embedded in three dimensions), used to represent boundaries of objects in some three dimensional scene. 
\item These objects allow for ignoring the internal structure of the object, if it is not relevant to the graphics, and in structural biology, internal folding of a protein molecule is often irrelevant for interactions that happen on the molecular surface. 
\item Manifolds are deformable, and many slight deformations of objects ( as in the surface of a human figure gradually changing its pose ) essentially preserve intrinsic metric structure, the embedding is changing. This furnishes an example of one of the more general symmetries mentioned: metric preserving maps, namely isometries.
\item By the previous point, manifolds fall under the setting of \emph{varying domains} in the geometric deep learning blueprint.
\end{itemize}

\subsection{Riemannian manifolds}


\begin{itemize}
\item A \emph{manifold} is a paracompact Hausdorff space locally homeomorphic to $\mathbb{R}^n$. \footnote{ From wikipedia: \emph{``In mathematics, a paracompact space is a topological space in which every open cover has an open refinement that is locally finite. These spaces were introduced by Dieudonn\'{e} (1944). Every compact space is paracompact. Every paracompact Hausdorff space is normal, and a Hausdorff space is paracompact if and only if it admits partitions of unity subordinate to any open cover."} Thus paracompactness is a definition tailored for useful topological tools of bump functions and partitions of unity.}
\item Saying locally homeomorphic to $\mathbb{R}^n$ is somewhat vague: we don't want the dimension to vary from neighborhood to neighborhood, or across connected components. 
\item To be specific, a manifold $M$ must come equipped with an \emph{atlas} $\mathcal{A}$ of charts $\alpha : U_\alpha \to \widetilde{U}_\alpha \subset \mathbb{R}^d$, each of which is a homeomorphism between $U_\alpha \subset M$ open and a bounded, open, connected domain in $\mathbb{R}^d$. The dimension $d$ is the same for all charts, and we require the $U_\alpha$ to form an open cover of $M$.  
\item A \emph{smooth manifold} is a manifold $(M,\mathcal{A})$ with the following ``smooth compatibility" constraint on the atlas $\mathcal{A}$. Given two charts $\alpha$ and $\beta$ with $U_\alpha \cap U_\beta$ non-empty, let 
\subitem $V_{\alpha, \, \beta} := \alpha ( U_\alpha \cap U_\beta)$ be this region of overlap in ``$\alpha$-coordinates," and
\subitem let $V_{\beta,\,\alpha} := \beta (U _\alpha \cap U_\beta)$ be this region of overlap in ``$\beta$-coordinates." \\
One can consider the map $T_{\alpha, \, \beta} : V_{\alpha,\,\beta}  \to V_{\beta,\,\alpha} $ given by the composition $\beta \circ \alpha^{-1}$ restricted to $V_{\alpha,\,\beta}$, and while in general this is a homeomorphism, for a smooth manifold we require this to be a diffeomorphism.\footnote{maybe I want my neighborhoods to be contractible, in addition to connected? }
\item With smooth structure, one can now define a \emph{tangent space} $T_xM$ at each $x \in M$ in the following way:
\subitem We consider the family of curves of the form $\gamma : (-\epsilon, \epsilon) \to M$, for some small $\epsilon$, 
$$
\mathcal{C}_x = \{ \gamma :  \text{ $\gamma$ is smooth, $\gamma(0) =x$ } \}
$$
where smoothness means ``smooth when viewed from a(ny) coordinate chart." We can now from equivalence classes out of the above family, with the relation $\gamma \sim \eta$ if, under any chart whose domain is a neighborhood of $x$, one has $\gamma'(0) = \eta'(0)$. We then define
$$
T_xM := \mathcal{C}_x / \sim 
$$
\item Thus, tangent vectors are (or can be viewed as) equivalence classes of curves passing through a given $x \in M$, and curves are equivalent if their velocities agree at $x$. 
\item One can endow the collection of all tangent spaces
$$
TM := \bigsqcup_{x \in M} T_x M
$$
with the structure of a smooth $2d$-manifold, this space is called the \emph{tangent bundle}.
\item A Riemannian manifold $(M,g)$ is a smooth manifold $(M, \mathcal{A})$ equipped with a \emph{Riemannian metric} $g$, where $g_x : T_xM \times T_xM \to \mathbb{R}$ in a way depending smoothly on $u$, such that this map defines an inner product within each tangent space,
$$
\langle X, Y \rangle_u = g_u(X,Y) ,
$$
which gives us a notion of length and angle within each tangent space. 
\item Note that tangent vectors are abstract geometric objects, and which exist without imposing any coordinate system on the manifold. If we express a tangent vector $X$ as an array of numbers, we can only represent it as a list of coordinates $(x_1, \dots, x_d)$ relative to some local basis $\{ X_1, \dots, X_d\} \subset T_uM$. Similarly, the metric can be expressed as a $d \times d$ matrix $G$ with elements $g_{ij} = g_u(X_i, X_j)$ in that basis. 
\end{itemize}

\n\hrulefill 
\emph{ Riemannian manifolds and symmetry }
\hrulefill

\begin{itemize}
\item Properties which can be expressed entirely in terms of the metric $g$ are called \emph{intrinsic}. This is a crucial notion for the discussion, as according to the template, we seek to construct functions acting on signals defined over $\Omega$ that are invariant to isometries. 
\item We can generate such functions by looking at functions of intrinsic quantities, and we recover a notion of geometric stability when extending the discussion to approximate isometries. 
\end{itemize}

\subsection{Scalar and vector fields}

\begin{itemize}
\item A (smooth) \emph{scalar field} is a function of the form $x : \Omega \to \mathbb{R}$. Scalar fields form a vector space $\cX(\Oh, \R)$ with inner product
$$
\la x, y \ra = \int_\Oh x(u) y(u) \, \textrm{d} u,
$$
where $\textrm{d}u$ is the volume form induced by the metric $g$. 
\item A (smooth) \emph{tangent vector field} is a function of the form $X : \Omega \to T\Omega$ assigning to each point a tangent vector in the respective tangent space, i.e. $u \mapsto X(u) \in T_u\Oh$.
\item Vector fields also form a vector space $\cX(\Omega, T\Omega)$ with the inner product defined through the Riemannian metric,
$$
\la X, Y \ra := \int _ \Oh g_u(X(u), Y(u)) \textrm{d} u
$$
\end{itemize}


\n\hrulefill 
\emph{ Intrinsic gradient }
\hrulefill

\begin{itemize}

\item Another way to think of ( and define ) vector fields is as a generalized notion of derivative. In classical calculus, one can locally linearize a smooth function through the \emph{differential}
$$
\rd x(u) = x( u + \rd u ) - x(u) ,
$$
reporting the change of the value of the function $x$ at point $u$ after an infinitesimal displacement $\rd u$. 
\item This is often impossible to generalize naively, as $u + \rd u$ is meaningless without a group operation on the manifold. 
\item Tangent vectors can be used to model local infinitesimal displacement. In particular, we can operate on scalar fields through vector fields in a natural way.
\item Given a smooth scalar field $x \in \cX(\Oh,\R)$ to be acted on by a vector field, we can actually characterize vector fields as linear maps
$$
Y : \cX(\Oh,\R) \to \cX (\Oh,\R)
$$ 
satisfying the properties of \emph{derivation}:
\subitem[ vanish on constants ] $\quad$ $Y(c) = 0$ for any constant(-valued scalar field) $c \in \cX(\Oh,\R)$
\subitem[ linearity ] $\quad$ $Y(x + z) = Y(x) + Y(z)$ for all $x, z \in \cX(\Oh,\R)$
\subitem [ product rule ] $\quad$ $ Y(xz) = Y(x) z + xY(z) $
\item The relation between vector fields and the differential $\rd$ is given by duality, one can think of $\rd$ as a map taking some signal $u$ to a \emph{covector field}, namely a linear functional of a vector field. This map is defined by
$$
\rd x (Y) = Y(x) 
$$ 
\item At each point $u$, thee differential can be regarded as a linear functional 
$$
\rd x_u : T_u\Oh \to \R
$$
acting on tangent vectors $X \in T_u \Oh$. 
\item If we are given such an object $\textrm{d}x_u$ on an inner product space, the Riesz representation theorem gives us the existence of some tangent vector ``$\nabla x(u)$," with the notation here purely formal, which encodes the action of $\textrm{d} x_u$.
\item In the context of a Riemannian manifold equipped with metric tensor $g$, the inner product $g_u$ gives rise to a canonical way of representing the cotangent vector $\textrm{d} x_u$ through a tangent vector, namely $\nabla  x(u)$ is the unique tangent vector satisfying
$$
\rd x_u(X) = g_u(\nabla x(u), X)
$$
for each $X \in T_u \Oh$. 
\item The notation here is suggestive of the definition to be made: we refer to the tangent vector $\nabla x(u) \in T_u\Oh$ as the \emph{intrinsic gradient} of $x$. 
\item At the level of the tangent bundle, the gradient can thus be considered as an operator $\nabla : \cX(\Oh,\R) \to \cX(\Oh, T\Oh)$ with mapping given by $x(u) \mapsto \nabla x(u) \in T_u\Oh$. 
\item Thus, the gradient of scalar field $x$ is a vector field $\nabla x$.
\end{itemize}

\n\hrulefill 
\emph{ Geodesics }
\hrulefill

\begin{itemize}

\item Consider a smooth curve $\gamma : [0,T] \to \Omega$ on the manifold with endpoints $u = \gamma(0)$ and $v = \gamma(T)$. Among all curves joining $u$ and $v$, we are interested in those of minimum length, i.e. we seek $\gamma$ minimizing the following length functional:
\begin{align*}
\ell(\gamma) &= \int_0^T \| \gamma'(t) \|_{\gamma(t)} \rd t \\
&= \int_0^T g_{\gamma(t)}^{1/2} ( \gamma'(t), \gamma'(t)) \rd t 
\end{align*}
\item Such curves are called \emph{geodesics}. Such curves do not necessarily need a Riemannian metric in order to be defined, but rather a more general notion of a \emph{connection}, or \emph{covariant derivative}. 
\item Given a Riemannian metric, there is a unique ``compatible" connection called the \emph{Levi-Civita connection}, where the connection notion of geodesic for the Levi-Civita connection is identical to the metric notion first introduced.
\item The significance of geodesics to the learning blueprint is that we can use these to define a way to transport tangent vectors on the manifold, which we will need to construct a convolution-like operation. 
\end{itemize}

\subsection{Parallel transport}
\hrulefill

\begin{itemize}

\item One issue we have already encountered with manifolds is that we cannot directly add or subtract points in the domain. The same problem arises when trying to compare tangent vectors at \emph{different} tangent spaces.

\item Geodesics provide a mechanism to move vectors between distinct tangent spaces: given a tangent vector $X \in T_u\Oh$ and a geodesic $\gamma$ joining $u$ to $v$, one can define a new set of tangent vectors $X(t) \in T_{\gamma(t)}\Oh$, namely along the geodesic $\gamma$, by holding the length of $X(t)$ constant, as well as the angle between $X(t)$ and the velocity vector $\gamma'(t)$. These are summarized as follows
\subitem $g_{\gamma(t)} (X(t), \gamma'(t) ) = g_{\gamma(0)} (X, \gamma'(0)) = \textrm{const.}$
\subitem $\| X(t) \|_{\gamma(t)} = \| X \|_u = \textrm{const.}$

\item The map we are discussing, $\Gamma_{u \to v} (X) : T_u \Oh \to T_v\Oh$, which maps $X \in T_u \Oh$ to $X(T) \in T_v \Oh$ in the above notation, is called \emph{parallel transport} or \emph{connection}. Due to angle and length preservation conditions, parallel transport amounts to only rotation of the vector, so it can be associated with an element $g_{u \to v}$ of the special orthogonal group $\SO(s)$ ( the latter called the \emph{structure group} )

\item The result of the transport, and hence the group element $g_{u \to v} \in \SO(s)$ depends on the path taken. 

\end{itemize}

\n\hrulefill 
\emph{ Exponential map, geodesic distances, isometries }
\hrulefill

\begin{itemize}

\item Locally around a point $u$, it is always possible to define a unique geodesic in a given direction, $X \in T_u \Omega$, i.e. so that $\gamma(0) = u$ and $\gamma'(0) = X$. 

\item When $\gamma_X(t)$ is defined for all $t \geq 0$, the manifold is said to be \emph{geodesically complete}, and this map ( to be defined, and called the \emph{exponential map} ) is defined over the whole tangent space. 

\item Note that geodesic completenesss does not necessarily guarantee that $\exp$ is a global diffeeomorphism; the largest radius $r$ about $u$ for which $\exp_u (B_r(0)) \subset T_u (\Oh)$ is mapped diffeomorphically iss called the \emph{injectivity radius}.

\item The Hopf-Rinow theorem establishes the equivalence of geodesic and metric completeness. It guarantees that for any $u,v \in M$, there is $\gamma$ joining $u$ and $v$ such that 
$$
d_g (u,v) = \ell(\gamma).
$$

\item Consider now a deformation of manifold $\Oh$ into another manifold $\tilde{\Oh}$ with Riemannian metric $h$, we assume that this map
$$
\eta : (\Oh,g) \to (\tilde{\Oh}, h) 
$$
is a diffeomorphism, so that its differential $\rd \eta : T \Oh \to T \tilde{\Oh}$ defines what's called the \emph{pushforward} map between tangent bundles.

\item The pushforward enables us to compare the two metrics $g$ and $h$ via the pullback $\eta^*$ defined pointwise for $u \in M$ as
$$
(\eta^* h)_u (X,Y) = h_{\eta(u)}( \rd \eta_u(X), \rd \eta_u(Y) ) .
$$
If the pullback metric $\eta^*h$ coincides with $g$ at every point, the map $\eta$ is called a Riemannian isometry.

\item By the Hopf-Rinow theorem mentioned above, this corresponds to an isometry at the level of metric spaces. Let $d_g$ and $d_h$ be the (abstract) metrics induced by Riemannian metrics $g$ and $h$. A Riemannian isometry between $(\Oh,g)$ and $( \tilde{\Oh}, h)$ induces an isometry between metric spaces $(\Oh, d_g)$ and $(\Oh, d_h)$, in that
$$
d_g(u,v) = d_h(\eta(u), \eta(v)) \quad \text{ for all $u,v \in \Oh$ } ,
$$
or more concisely, $d_g = d_h \circ (\eta \times \eta)$. 

\item On connected manifolds, the converse of Hopf-Rinow is also true: every metric isometry is also a Riemannian isometry. 

\item In our geometric deep learning blueprint, $\eta$ is a model of domain deformations. When $\eta$ is an isometry, intrinsic quantities are unaffected by such deformations. One can generalize exact (metric) isometries through notions of \emph{metric dilation}
$$
\textrm{dil}(\eta) = \sup_{u\, \neq\, v \, \in \,\Oh } \frac{ d_h ( \, \eta(u), \,\eta(v) \, ) }{d_g(\,u,\,v\,)} 
$$
or \emph{metric distortion}
$$
\textrm{dis}(\eta) = 
	\sup_{u\, v \, \in \, \Oh } 
		\left| \, d_h(\, \eta(u), \eta(v) \,) - d_g (u,v) \, \right|,
$$
which respectively capture the relative and absolute change of geodesic distance under $\eta$. 

\item Note that the latter notion of metric distortion has already been used implicitly: viewing $\eta$ as a domain deformation, recall that a function $f \in \mathcal{F}(\cX(\Oh))$ is stable under domain deformations if 
$$
\| f(x, \Oh) - (x \circ \eta^{-1}, \tilde{\Oh} ) \| \leq C \| x \| \textrm{dis}(\eta)
$$

\end{itemize} 

\subsection{Spectral methods}
\hrulefill

\begin{itemize}

\item Given a Riemannian manifold $\Oh$, the collection of Riemannian self isometries is denoted $\textrm{Iso}(\Oh)$ \emph{(one assumes they mention this because it will be the group lurking in the background)}

\item Our aim is to construct intrinsic convolution-like operations on manifolds, which by construction will be invariant to isometric deformations. 

\item There are two ways of approaching this in the developed frameworks: 

\subitem One is to use analogy of the Fourier transform and to define convolution as a product in the Fourier domain. 

\subitem The other is to define convolution spatially, by ``correlating" a filter locally with the signal. 

\item We discuss the spectral approach first. 

\item Recall that in the (discrete) Euclidean domains, the Fourier transform can be obtained through eigenvectors of any circulant matrix, which are jointly diagonalizable due to their commutativity. The Fourier transform is then the original object in the coordinates of this orthonormal eigenbasis.

\item One example of a circulant matrix in the above setting is the discrete derivative, and in the continuum, we can use a differential operator to define an analogy of the Fourier transform on general domains. 

\item In Riemannian geometry, it is common to use the orthonormal basis of the Laplacian operator, which we now define on manifolds.

\item Recall our definition of the intrinsic gradient operator,
$$
\nabla : \mathcal{X}(\Omega, \mathbb{R}) \to \mathcal{X}(\Omega, T\Omega),
$$
producing a tangent vector field indicating the local direction of steepest increase of some input scalar field on the manifold. 

\item In a similar manner, we can define the \emph{divergence operator} $\nabla^* : \mathcal{X}( \Omega, T\Omega) \to \mathcal{X}(\Omega, \mathbb{R})$: if we think of a tangent vector field as a flow on the manifold, the divergence measures the net flow of a field at a point. We use the notation $\nabla^*$ ( as opposed to \emph{div} ) to emphasize that the two operators are adjoint: 
$$
\la  X, \nabla x \ra = \la \nabla^* X, x \ra
$$

\item the \B{Laplacian} (or Laplace-Beltrami operator) is an operator defined on $\cX(\Oh)$, through $\Delta = \nabla^* \nabla$, which can be interpreted as the difference between the average of a function on an infinitesimal sphere around a point and the value of the function at the point itself. 

\item The Laplacian is self-adjoint, or ``symmetric," through the following integration-by-parts identity
$$
\langle \nabla x , \nabla x \rangle = \langle x, \Delta x \rangle = \langle \Delta x, x \rangle 
$$
\item The quadratic form on the left is called the \B{Dirichlet energy} of the signal $x$, defined more concretely as
$$
\mathfrak{c}^2(x) = \| \nabla x\|^2 = \int_\Oh \| \nabla x(u) \|_u^2 \textrm{d} u = \int_{\Oh} g_u ( \nabla x(u), \nabla x(u)) \textrm{d}u ,
$$
which measures the smoothness or elasticity of $x$.
\item The Laplacian operator admits an eigendecomposition
$$
\Delta \varphi_k = \lambda_k \varphi_k , \quad k = 0,1,2,\dots
$$
with countable spectrum if the manifold is compact (which we always assume), and with orthogonal eigenfunctions $\la \varphi_k, \varphi_\ell \ra = \delta_{k,\ell}$, due to the self-adjointness of $\Delta$. 
\item The Laplacian eigenbasis can be constructed through a Gram-Schmidt procedure, as a set of orthogonal minimizers of the Dirichlet energy,
$$
\varphi_{k+1} = \text{argmin}_\varphi \| \nabla \varphi \|^2 \quad \text{ s.t. } \quad \| \varphi \| = 1 \text{ and } \la \varphi, \varphi_j \ra = 0
$$
for $j = 0,1, \dots, k$, allowing for the interpretation of $(\varphi_k)_{k \geq 0}$ as the ``smoothest orthogonal basis on $\Oh$." These eigenfunctions, as well as the eigenvalues $(\lambda_k)_{k \geq 0}$, together can be interpreted as analogous to the atoms and frequencies in the classical Fourier transform. 
\item This orthogonal basis allows us to expand square-integrable functions on $\Omega$ into Fourier series:
$$
x(u) = \sum_{ k \geq 0 } \la x , \varphi_k \ra \varphi_k(u) 
$$
where $\hat{x}_k := \la x, \vp_k \ra$.
\item Truncating the Fourier series results in an approximation error which may be bounded ( \cite{BBCV21} cite Aflalo-Kimmel 2013 ) as follows: 
$$
\left\| 
x - \sum_{k = 0}^N 
\la x, \vp_k \ra\vp_k 
\right\|^2
\leq
\frac{
\| \nabla x \|^2
}
{
\lambda_{N+1}
}
$$
\item Aflalo et al. (2015) further showed that no other basis attains a better error, making the Laplacian eigenbasis \emph{optimal} for representing smooth signals on manifolds. 
\end{itemize}

\n \hrulefill
\emph{ Spectral convolution on manifolds }
\hrulefill

\begin{itemize}
\item \B{Spectral convolution} can be defined as the product of the Fourier transforms of the signal $x$ and the filter $\theta$,
$$
( x \star \theta ) (u ) = \sum_{ k \geq 0} ( \hat{x}_k \cdot \hat{\theta}_k ) \varphi_k(u),
$$
this uses the intertwining of convolution and multiplication as a method for defining a non-Euclidean convolution. 
\item In practice, a direct computation of the display directly above appears to be prohibitively expensive due to the need to diagonalize the Laplacian. 
\item Even worse, it turns out geometrically unstable: the higher-frequency eigenfunctions of the Laplacian can change dramatically with even small near-isometric perturbations of the domain $\Oh$. 
\item A more stable solution is provided by realizing the filter as a \emph{spectral transfer function} of the form $\hat{p} (\Delta)$,
\begin{align*}
( \hat{p}(\Delta) x )(u) &= \sum_{ k\geq 0} \hat{p} (\lambda_k) \la x, \varphi_k \ra \varphi_k(u) \\
&= \int_\Oh x(v) \sum_{k \geq 0 } \hat{p} (\lambda_k) \varphi_k(v) \varphi_k(u) \textrm{d}v 
\end{align*}
\item We can interpret the above display in two ways: either as a \emph{spectral filter}, where we identify $\hat{\theta}_k = \hat{p} (\lambda_k)$, or as a spatial filter with a position dependent kernel, whose role is played by the sum in the second line
$$
\theta(u,v) = \sum_{k \geq 0 } \hat{p} ( \lambda_k ) \varphi_k(v) \varphi_k(u)
$$
\item The advantage of this formulation is that $\hat{p}(\lambda)$ can be characterized by a small number of coefficients, and choosing a good parametrization of the spectral transfer functions allows for an efficient computation of the filter, avoiding the spectral decomposition.
\end{itemize}

\n \hrulefill
\emph{ Spatial convolution on manifolds }
\hrulefill

\begin{itemize}
\item An alternative attempt at defining convolution on manifolds is to ``match a filter at different points," like we did in the definition of group convolution
$$
(x \star \theta) (g) = \la x , \rho(g) \theta \ra = \int_\Oh x(u) \theta(g^{-1} u) \textrm{d} u ,
$$
noting that $x \star \theta$ takes group elements $g$ as arguments. Here we could analogously write
$$
(x \star \theta ) (u) = \int_{T_u \Oh} x (\exp_u Y ) \theta_u (Y) \textrm{d} Y,
$$
where we have used the (yet to be defined) \B{exponential map} $\exp_u$ to pull signal $x$ back to a function on $T_u\Oh$. The filter $\theta_u$ is defined directly in the tangent space at each point, and is thus position-dependent.
\item If one defines the filter $\theta$ intrinsically, such a convolution would be isometry invariant, a property mentioned as crucial in graphics in vision.
\item There are substantial differences from the previous construction: firstly, because a manifold is generally not a homogeneous space, we don't have a group structure that allows us to ``share" the filter $\theta$ across the manifold in a straightforward way. 
\item An analogy of this operation on the manifold (in our current attempt to define convolution on manifold) would require a parallel transport to move the filter $\theta$ from one tangent space to another. 
\item In general, the result of a parallel transport depends on the path taken between two points $u$ and $v$, so \emph{the way we move the filter around matters.} 
\item A second difference is that we can use the exponential map only locally, so the filter must be local, with support bounded by the injectivity radius. 
\item Thirdly and crucially, in order for a computer to work with the abstract geometric object $X \in T_uM$, with the goal being able to compute $\theta(X)$ for some tangent space filter $\theta$, we must express $X$ relative to a \emph{local basis} $\omega_u : \mathbb{R}^s \to T_u \Omega$. Thus the coordinates used by the computer are the $s$-dimensional array $\mathbf{x} = \omega_u^{-1}(X)$. We can thus re-express convolution locally \footnote{ And also over the same domain; is this important? Beforehand the tangent space serving as the domain of integration was $u$-dependent. Though these are isomorphic, should not be regarded as ``the same"}
$$
(x \star \theta) (u) = \int_{ [0,1]^s } x (\exp_u(\omega_u \mathbf{y} )) \theta (\mathbf{y}) \textrm{d} \mathbf{y},
$$
and importantly, we can construct this convolution using the same filter for each point, as all data is pulled back to the unit cube, on which the filter is defined.
\item As the exponential map is intrinsic (through the definition of geodesic)
\end{itemize}

\n \hrulefill
\emph{ discussion }
\hrulefill

\begin{itemize}
\item We seem to assume we can carry the frame $\omega_u$ along to another manifold, i.e. $\omega_u' = \textrm{d} \eta_u \circ \omega_u$.  Obtaining such a frame ( or \emph{gauge} in physics terminology ) given only the manifold $\Oh$ in a consistent manner is difficult.
\item Firstly, a smooth global gauge may not exist: thiss is the ssituation on manifolds that are not \emph{parallelizable}, meaning there does not exist a smooth non-vanishing tangent vector field.
\item Secondly, we do not have a ``canonical gauge on manifolds", so this choice is arbitrary, and our convolution seemss to depend on $\omega$. 
\item This is a case where practice diverges from theory: in practice, it is possible to build frames that are mostly smooth, with a limited number of singularitiess, e.g. by taking the intrinsic gradient of some intrinsically defined scalar field on the manifold. 
\item Moreover, such constructions are ``stable", i.e. the framess constructeed this way will be identical on isometric manifolds, and similarly on approximately isosmetric ones.
\item The solution is not entirely satissfactory because near singularities, the filter orientation (being defined in a fixed manner relative to the gauge) will vary wildly, leading to a non-smooth feature map even if the input signal and filter are smooth. 
\item Moreover, there is no clear reason why a given direction at some point $u$ should be considered ``equivalent" to another direction at an altogether different point $v$. 
\item Thus despite \emph{practical} alternativess, we will look for a more theoretically well0funded approach that would be altogether independent of the choice of gauge. 
\end{itemize}

\subsection{Gauges and bundles}

\subsubsection*{Fiber bundles}

\begin{defn} A \B{fiber bundle} is a quadruple $(E, F, B, \pi: E \to B)$, where $E$, $F$ and $B$ are topological spaces respectively called the \B{total space}, \B{fiber} and \B{base space}. The function $\pi$ is a continuous map from the total space to the base space, such that for each $b \in B$, 
$$
\pi^{-1} (b) \cong F,
$$
where $\cong $ means that the two topological spaces are homeomorphic. We will assume the fiber bundles we consider are ``locally trivial," in that the above quadruple must satisfy the following: for each $b \in B$, there is an open neighborhood $U \in b$ in $B$ such that 
$$
\pi ^{-1}(U) \cong U \times  F.
$$
A ``trivial" fiber bundle is one such that $E \cong B \times F$.
\end{defn}

\subsubsection*{Examples}

\subitem annulus
\subitem Mobius band
\subitem Tangent bundle

\begin{defn}[ vector bundle ]
\end{defn}

\begin{itemize}
\item In the context of geometric deep learning, fiberss are used to model the feature spaces at each point in the manifold $\Oh$, the dimension of the fiber being the number of channels. 
\item In this setting, an important symmetry called \B{gauge symmetry} may present itself.
\item Consider $d$-dimensional manifold $\Oh$ with its tangent bundle $T\Oh$, so that the number of channels is $s = d$ in this setting. Let $X: \Oh \to T\Oh$ be a vector field. Relative to a gauge $\omega$ for the tangent bundle, $X$ is represented as a function $\mathbf{x} : \Oh \to \mathbb{R}^s$.
\item Importantly, we are interested in the underlying geometric object $X$, whose representation as a function $\mathbf{x} \in \mathcal{X}(\Oh, \R^s)$ \emph{depends on the choice of gauge $\omega$})
\end{itemize}

\subsubsection*{Tangent bundles and the structure group} 

\begin{itemize}
\item When we change the gauge, we need to apply at each point an invertible matrix that maps the old gauge to the new one. 
\item This matrix is unique for every pair of gauges at each point, but possibly different at different points. In other words, a \emph{gauge transformation} is a mapping $g : \Oh \to \GL(s)$, where $\GL(s)$ is the general linear group of invertible $s \times s$ matrices. It acts on gauge $\omega_u : \R^s \to T_u\Oh$, to produce a new gauge
$$
\omega_u ' = \omega_u \circ g_u : \mathbb{R}^s \to T_u \Oh
$$
\item The gauge transformation acts on a coordinate vector field at each point via 
$$
\mathbf{x}'(u) = g_u^{-1} \mathbf{x} (u) 
$$ 
\item Let us note the underlying vector field remains unchanged:
\begin{align*}
X(u) &= \omega_u' ( \mathbf{x}'(u)) \\
&= \omega_u( g_u g_u^{-1} \mathbf{x}(u)) \\
&= \omega_u( \mathbf{x}_u ) \\
&= X(u) 
\end{align*}

\item \emph{I did not understand the next sentence starting "more generally, we may have a field of geometric quantities...} because I'm unsure of how $\rho_2, \rho_1$ relate to the representation $\rho$ of $\GL(s)$ mentioned..

\item Sometimes we may wish to restrict attention to frames with a certain property, such as orthogonal frames, right-handed frames, etc.. Unsurprisingly, we are interested in a group of transformations on frames which preserve these properties, respectively $O(s)$ and $SO(s)$.

\item In general, we have a group $G$ called the \B{structure group} of the bundle, and a \B{gauge transformation} is a map $g : \Oh \to G$. 

\item ``A key observation is that in all cases with the given property, for any two frames at a given point there exists exactly one gauge transformation relating them."
\end{itemize}

\begin{defn}[ Sections over a fiber bundle]

\end{defn}

\subsubsection*{Examples} 

\begin{itemize}
\item vector field
\item calc of variation like perturbations of middle circle of annulus
\end{itemize}

\begin{mdframed}
\begin{itemize}
\item Gauge theory is not specific to tangent bundles, it applies to vector bundles; one can model an ``infinitely high resolution" RGB image as a section over the trivial vector bundle
$$
[0,1]^2 \times \mathbb{R}^3 
$$
\item It is customary to express an RGB image relative to a gauge that has basis vectors R, G and B, in that order. But we may equally well permute the basis vectors ( color channels ) independently at each position, as long as we remember the frame ( order of channels).
\item As a computational option this is ``rather pointless", but we will see shortly it is conceptually useful to think about gauge transformations for the space of RGB colors, because it allows us to express a gauge symmetry - in this case, an equivalence between the colors - and make functions defined on images respect this symmetry (treating each color equivalently). 
\item A gauge transformation is somehow ``internal" to the neural network, it does not change the underlying RGB image.\footnote{ So maybe this is why acting on RGB basis by $\mathfrak{S}_3$ to augment is not useful, maybe even harmful. } 
\item In ML applications, we are interested in constructing functions $f \in \mathcal{F}(\mathcal{X}(\Oh))$ on such images, implemented as layers of a neural network. 
\item It follows that if, for whatever reason, we were to apply a gauge transformation to our image, we would need also to change the function $f$ (network layers) so as to preserve their meaning. 
\item Consider, for simplicity, a $1 \times 1$ convolution, i.e. a map that takes an RGB pixel $\mathbf{x}(u) \in \mathbb{R}^3$ to a feature vector $\mathbf{y}(u) \in \mathbb{R}^C$. 
\item According to our geometric learning blueprint, the output is associated with a group representation $\rho_{out}$, in this case, a $C$-dimensional representation of the structure group $G  =\mathfrak{S}_3$. Similarly, the input iss associated with a representation $\rho_{in}(g) = g$.
\item If we apply a gauge transformation to the input, we would need to change the linear map $1 \times 1$ convolution $f: \mathbb{R}^3 \to \mathbb{R}^C$ into
$$
f' = \rho_{out}^{-1} (g) \circ f \circ \rho_{in} (g)
$$
so that the output feature vector $\mathbf{y}(u) = f(\mathbf{x}(u)$ transforms like $\mathbf{y}'(u) = \rho_{out} (g_u) \mathbf{y}_u$
\item Indeed,
\eq{
\mathbf{y}' &= f'(\mathbf{x}') = \rho_{out}^{-1} (g) f ( \rho_{in} (g) \rho_{in}^{-1}(g) \mathbf{x} \\
&= \rho_{out}^{-1}(g) f(\mathbf{x})
}
\end{itemize} 
\end{mdframed}


\subsubsection*{ Gauge symmetries } 

\begin{itemize}
\item To say we consider gauge transformations as symmetries is to say that any two gauges related by a gauge transformation are to be considered equivalent. \footnote{How is this related to a Grassmannian?}
\item In general we can consider a group $G$ and a collection of frames at every point $u$ such that for any two of them there is a unique $g(u) \in G$ that maps one frame onto the other. 
\item When we regard gauge transformations as symmetries in our GDL blueprint, we are interested in making functions $f$ in the hypothesis space equivariant under gauge transformations.
\item Concretely, this means that if we apply a gauge transformation to the input, the output should undergo the same transformation (perhaps acting via a different representation of $G$ \footnote{so these distinct representations are the $\rho_1$ and $\rho_2$ I didn't understand above} ). 
\item We noted before that when we change the gauge, the function $f$ should be changed as well, but for a gauge equivariant map, this is not the case:  consider again the RGB color space example. The map $f : \mathbb{R}^3 \to \mathbb{R}^C$ is equivariant if $f \circ \rho_{in}(g) = \rho_{out}(g) \circ f$.
\item ``Thus, the coordinate expression of a gauge equivariant map is independent of the gauge, in the same way that in the case of graphs, we applied the same function regardless of how the input nodes were permuted.

\item However, unlike the case of graphs and other examples covered so far, gauge transformations act not on $\Oh$, but separately on each of the feature vectors $x(u)$ by a transformation $g(u) \in G$ for each $u \in \Oh$. 

\item Further considerations enter the picture when we look at filters on manifolds with larger spatial support. Let us first consider the example of a mapping $f : \cX(\Oh, \R) \to \cX(\Oh, \R)$ from scalar fields to scalar fields on a $d$-dimensional manifold. 

\item Unlike vectors and other geometric quantities, scalars have no orientation, so a scalar field $x \in \cX(\Oh,\R)$ is invariant to gauge transformations. 

\item Having characterized such $f$ in terms of convolutions, we can write $f$ as before under the ``spatial filter" interpretation of convolution, as a convolution-like operation with a position dependent filter $\theta : \Oh \times \Oh \to \R$,
$$
(x \star \theta) (u) = \int_\Oh \theta(u,v) x(v),
$$
which implies we have a potentially different filter $\theta_u = \theta(u , \circ)$ at each point, and is synonymous with losing spatial weight sharing. I think their point is that gauge symmetry does not alone lead to weight sharing. 

\item Consider now a more interesting case of a mapping $f : \cX(\Oh,T\Oh) \to \cX ( \Oh, T\Oh)$ from vector fields to vector fields. Relative to a gauge, the input and output vector fields $X,Y \in \cX ( \Oh, T\Oh)$ are vector-valued functions $\mathbf{x}, \mathbf{y} \in \cX(\Oh,\R^s)$, with $s \equiv d$ in this case. 

\item A general linear map between such functions can be written using the same equation used for scalars, only replacing the scalar kernel by a matrix-valued one, $\mathbf{\Theta} :\Oh \times \Oh \to \R^{s \times s}$. 

\item The matrix $\mathbf{\Theta} (u,v) $ should map tangent vectors in $T_v \Oh$ to tangent vectors in $T_u \Oh$, but these points have different gauges that we may change arbitrarily and independently. That is, the filter would have to satisfy $\mathbf{\Theta}(u,v) = \rho^{-1} ( g(u)) \mathbf{\Theta}(u,v) \rho( g(v))$ for all $u,v \in \Oh$. Here $\rho$ denotes the action of $G$ on vectors, given by an $s \times s$ rotation matrix. 


\item Since $g(u)$ and $g(v)$ can be chosen freely, this is an overly strong constraint on the filter, in fact it implies the filter is zero. 

\item A better approach is to first transport the vectors to a common tangent space by means of the connection, and then impose gauge equivariance with respect to a single gauge transformation at one point only. \footnote{This really encourages thinking locally} 

\item A natural point to chose is the argument of the convolution, $u$. One can then define the following map between vector fields:
$$
( \mathbf{x} \star \mathbf{\Theta} ) (u ) = \int_{\Oh} \mathbf{\Theta} (u,v) \rho( g_{v \to u} ) \mathbf{x}(v) \textrm{d} v,
$$
where $g_{u \to v } \in G$ denotes the parallel transport along the geodesic joining $v$ to $u$.\footnote{This also assumes locality, i.e. filter with local support, in order to have a unique geodesic} 

\item Under a gauge transformation $g_u$, this element transforms as 
$$
g_{u \to v} \mapsto g_u^{-1} g_{u \to v} g_v,
$$
and the field itself transforms as $\mathbf{x}(v) \mapsto \rho(g_v) \mathbf{x}(v)$. 

\item If the filter commutes with the structure group representation, $\mathbf{\Theta} (u,v) \rho(g_u) = \rho(g_u) \mathbf{\Theta}(u,v)$, the previous equation defines a \emph{gauge-equivariant convolution}, which transfsorms as
$$
( \mathbf{x}' \star \mathbf{\Theta} ) (u) = \rho^{-1} (g_u) ( \mathbf{x} \star \mathbf{\Theta} ) (u)
$$
Umm... where is the group action on the LHS? Is this a typo they fix?
\end{itemize}

\subsection{Geometric graphs and meshes} 

\begin{defn}[ $2$-complex ] 

\end{defn}

\begin{defn}[ Triangulation ]  face orientation
\end{defn}

\begin{itemize}
\item The condition ``Each edge shared by exactly two triangles" (either baked into above definition, or asserted as a specialization after) ensures the $1$-hop neighborhoods around each node are disk-like, and the mesh thus constitutes a discrete manifold. These are called \emph{manifold meshes} in practice?
\item We can equip these discrete objects with metrics analogous to the Riemannian setting. 
\item In the simplest instance, the mesh is considered embedded in $\mathbb{R}^3$ ( particularly in graphics and vision )  and one can define the metric structure on the mesh through the pullback $\ell$ of the Euclidean metric under the embedding. 
\item Any property which can be expressed solely in terms of $\ell$ is \emph{intrinsic}.
\item By analogy to graphs, let us assume a mesh with $n$-nodes, each associated with an $s$-dimenssional feature vector, which we can arrange (assuming some arbitrary ordering) into an $n \times d$ matrix $\mathbf{X}$.
\item The features can represent the geometric coordinates of the nodes ass well as additional properties such as colors, normals, or when such objects are used to model molecules, we can store chemical information like atomic number at each node.
\end{itemize}

\subsubsection*{ Mesh Laplacian } 

\begin{itemize}
\item We can discretize the Laplacian on a manifold mesh in the following way:
$$
( \Delta \mathbf{X}) _ u = \sum_{v \, \in \, \mathcal{N}_u } w_{uv} ( \mathbf{x}_u - \mathbf{x}_v) ,
$$
or in matrix notation, as an $n \times n$ symmetric matrix $\Delta = \mathbf{D} -\mathbf{W}$, where $\mathbf{D} = \textrm{diag} ( d_1, \dots, d_n)$ is called the \emph{degree matrix}, and $d_u = \sum_v w_{uv}$ is the \emph{degree} of node $u$. 
\item The display directly above performs local permutation-invariant aggregation of neighbor features
$$
\varphi ( \mathbf{x}_u, \mathbf{X}_{\mathcal{N}(u)} ) = d_u \mathbf{x}_u - \sum_{v \, \in \, \mathcal{N}_u} \omega_{uv} \mathbf{x}_v,
$$
so that $\mathbf{F}(\mathbf{X}) = \Delta \mathbf{X}$ is an instance of the general blueprint for constructing permutation-equivariant functions on graphs. 
\item Let us remark that there is nothing about the graph Laplacian defined above which is specific to meshes. Even on an unweighted graph, one can take $\mathbf{W} =\mathbf{A}$, the latter the adjacency matrix.
\item Laplacian's constructed in this way are called combinatorial.
\item For geometric graphs ( which do not necessarily have the additional structure of meshes, but whose nodes do have spatial coordinates that induce a metric in the form of edge-lengths ), it is common to use weights related to the metric, for instance $w_{uv} \propto e^{- \ell_{uv}}$.
\item Meshes have an advantage in that we can exploit the additional structure afforded by the weights, and we can define the edge weights according to the \emph{cotangent formula}
$$
w_{uv}  =\frac{\cot \angle_{uqv} +\cot\angle_{upv}}{2a_u}
$$
where $\angle_{uqv}$ and $\angle_{upv}$ are the two angles opposite the shared edge $(u,v)$ within the trianglulation ( making use of manifold mesh property ). The factor $a_u$ is the local area element, 
$$
a_u = \frac{1}{3} \sum _{ v, \, q \,: \,(u,v,q) \, \in \, \mathcal{F} } a_{uvq},
$$
which is parsed as follows: the faces of the triangulation are denoted $\mathcal{F}$. The sum is taking place over all triangles which use the vertex $u$, thus we are effectively looking at the $1$-hop neighborhood of $u$. Each triangle in this neighborhood has a barycenter, and the collection of these barycenters form the vertices of a polygon contained in the $1$-hop neighborhood triangles. The factor $a_u$ is the area of this polygon. 
\item The cotangent Laplacian can be shown to have multiple convenient properties: it is positive-semidefinite, thus it can be used for spectral analysis with its eigenvalues considered the analogue of frequencies. 
\item It is also local, as discussed, depending only on the $1$-hop neighbors. 
\item Most importantly, it has the property that the cotangent Laplacian converges in an appropriate sense to the continuous Laplacian, when the mesh size tends to zero. 
\item The cotangent Laplacian is thus a discrete analogue of the Laplacian operator on a Riemannian manifold. 
\item While one expects the Laplacian to be intrinssic, this is not obvious, and it takes some effort to express the cotangent weights entirely in terms of the discrete metric $\ell$ as
$$
w_{uv} = \frac{ - \ell_{uv}^2 + \ell_{vq}^2 + \ell_{uq}^2}{ 8 a_{uvq}} + \frac{ - \ell_{uv}^2 + \ell_{vp}^2 + \ell_{up}^2 }{ 8 a_{uvp}} ,
$$
where the area of the triangles $a_{ijk}$ is given as
$$
a_{uvq} = \sqrt{ s_{uvq}(s_{uvq} - \ell_{uv} ) (s_{uvq} - \ell_{vq} ) (s_{uvq} - \ell_{uq} ) },
$$
``using \emph{Heron's semiperimeter formula} with $s_{uvq} = \frac{1}{2} (\ell{uv} + \ell_{uq} + \ell_{vq} ). $"
\item This endows the Laplacian with \emph{isometry invariance}. Moreover, as observed, it is invariant to the permutation of nodes in $\mathcal{N}_u$, as it involves aggregation in the form of summation. 
\item While on general graphs, this is a necessary evil due to the lack of a canonical ordering of neighbors, on meshes we can order the $1$-hop neighbors according to some orientation, and the only ambiguity is the selection of the first node.
\item This, iinstead of any possible permutation we need to account for \emph{cyclic shifts}, or rotations, which intuitively corresponds to the ambiguity arising from $\SO(2)$ gauge transformations. 


\end{itemize}

\subsubsection*{ Spectral analysis on meshes } 

\begin{itemize}
\item The orthogonal eigenvectors $\Phi = (\varphi_1, \dots, \varphi_n)$ diagonalizing the Laplacian matrix, i.e. 
$$
\Delta = \Phi \Lambda \Phi^T 
$$ 
where $\Lambda = \textrm{diag}(\lambda_1, \dots, \lambda_n)$, are used as the non-Euclidean analogy of the Fourier basis.
\item We can design a filter $\theta$ directly in the Fourier domain, denoted $\hat{\theta}$, and one can perform spectral convolution on the mesh on the Fourier side via multiplication
$$
X \star \theta = \Phi \diag( \Phi^T \theta) (\Phi^T X) = \Phi \diag(\hat{\theta}) \hat{X} 
$$
\item It is tempting to exploit this spectral definition of convolution to generalize CNNs to graphs, which was done by Bruna et al in 2013. 
\item However, it appears that the non-Euclidean Fourier transform is extremely sensitive to even minor perturbations of the underlying mesh or graph.
\item Thus it is best suited to settings in which the domain is fixed, and not changing from signal to signal. Unluckily, this makes the Fourier transform based approach inappropriate in graphics and vision settings, in which one trains a neural network on one set of $3D$ shapes and tests on a different set.
\item As noted previously, it is preferable to use spectral filters dfined through some transfer function $\hat{p}$ acting on the Laplacian. 
$$
\hat{p} (\Delta) X = \Phi \hat{p}(\Lambda) \Phi^T X = \Phi \diag(\hat{p}(\lambda_1), \dots, \hat{p}(\lambda_n) ) \hat{X} 
$$
\item When $\hat{p}$ can be expressed in terms of matrix-vector products, the eigendecomposition of the $n \times n$ matrix $\Delta$ can be avoided altogether. For example, one can use polynomials of degree $r$ as filter functions ( Defferrard et al. 2016)
$$
\hat{p}(\Delta) X = \sum_{k=0}^r \alpha_k \Delta^k X
$$
\item This amounts to multiplication of the $n \times s$ feature matrix $X$ by the $n \times n$ Laplacian matrix $r$ times. Since the Laplacian is typically sparse, with $O(\# \mathcal{E})$ non-zero elements, this operation hass low complexity of $O (\# \mathcal{E})$.
\item \emph{Furthermore, since the Laplacian is local, a polynomial filter of degree $r$ is localized in $r$-hop neighborhood. }
\item Note that this means that the support of the filter depends on the resolution of the mesh, so that an equivalent support on a finer mesh requires using larger $r$. 
\item For this reason, in computer graphs, it is more common to use \emph{ rational filters}, since they are resulution-independent. There are many ways to define such filters (see Patane 2020), the most common being as a polynomial of some rational function, e.g. $\frac{\lambda-1}{\lambda+1}$. More generally, one can use a complex function, such as the \emph{Cayley transform} $\frac{\lambda - \textrm{i}}{\lambda + \textrm{i}}$ that maps the real line to the unit circle in the complex plane. Both of these functions are Mobius transformationss.
\item Levie at al (2018) used spectral fiilters expressed as Cayley polynomials, real rational functions with complex coefficients $\alpha_\ell \in \mathbb{C}$. 
$$
\hat{p} ( \lambda) 
	= \textrm{Re} 
	\left(  
		\sum_{\ell = 0}^r 
			\alpha_\ell 
			\left(
			\frac{ \lambda - \textrm{i} }{ \lambda + \textrm{i}  }
			\right)^\ell 
	\right)
$$
\item When applied to matrices, computation of the Cayley polynomial requires matrix inversion 
$$
\hat{p} (\Delta) = \textrm{Re} \left( \sum_{ \ell = 0}^r \alpha_\ell (\Delta - \textrm{i} I ) ^\ell (\Delta - \textrm{i} I )^{-\ell} \right),
$$
which can be carried out approximately with linear complexity. 
\item Unlike polynomial filters, rational filters do not have local support, but do have exponential decay. 
\item A crucial difference compared to the direct computation of the Fourier transform is that polynomial and rational filters are stable under approximate isometric deformations of the underlying graph or mesh. \footnote{In signal processing, polynomial filters are termed \emph{finite impulse response} (FIR) whereas rational filters are \emph{infinite impulse response} (IIR)}
\end{itemize}

\subsubsection*{ Meshes as operators and functional maps } 

\begin{itemize}
\item The paradigm of functional maps suggests thinking of meshes as operators. As we will show, this allows us to obtain more interesting types of invariance by exploiting the additional structure of meshes.
\item For the purpose of the discussion, assume the mesh $\mathcal{T}$ is constructed upon embedded nodes with coordinates $X$. 
\item If we construct an intrinsic operator like the Laplacian, it can be shown it encodes completely the structure of the mesh, and one can recover the mesh (up to its isometric embedding) \footnote{ Is this more elegantly expressed functorally?} 
\item Thus we will assume a general operator, or $n \times n$ matrix $Q(\mathcal{T}, X)$ is a representation of our mesh. 
\item Learning functions of the form $f(X, \mathcal{T})$ can be rephrased as learning functions of the form $f(Q)$. 
\item Similar to graphs and sets, the nodes of meshes also have no canonical ordering, i.e. functions on meshes must satisfy the permutation invariance or equivariance conditions,
\eq{
f(Q) &= f(PQP^T) \\
PF(Q) &= F(PQP^T)
}
or any permutation matrix $P$.
\item However, compared to general graphs we now have more structure: we may assume our mesh arises from the discretization of some underlying continuous surface $\Oh$. 
\item It is thus possible to have a different mesh $\mathcal{T} '= (\mathcal{V}', \mathcal{E}', \mathcal{F}')$ with $n'$ nodes and coordinates $X'$, but representing the same object $\Oh$ as $\mathcal{T}$.
\item Importantly, the meshes $\mathcal{T}$ and $\mathcal{T}'$ can have a different connectivity structure and even different number of nodes, $(n' \neq n)$, in particular they are in general not isomorphic as $2$-complexes.  
\item Functional maps were introduced by Ovsjanikov et al. (2012) as a generalization of the notion of correspondence to such settings, replacing the correspondence between points on the two domains ( i.e. a map $\eta : \Oh \to \Oh'$ ) with correspondence between functions, i.e. a map $C : \cX(\Oh) \to \cX(\Oh')$. 
\item A \B{functional map} is a linear operator $C$, represented as an $n' \times n$ matrix, establishing a correspondence between signals $x'$ and $x$ on the respective domains as
$$
x' = Cx
$$ 
\item Rustamov et al. (2013) showed that in order to guarantee \emph{area-preserving} mapping, the functional map must be orthogonal, $C^T C = I$, in particular $C \in O(n)$ and $C$ is invertible.

\item The functional map establishes a relation between the operator representation of meshes,
$$
Q' = CQC^T, \quad Q = C^T Q' C 
$$
which we can reinterpret as follows: given an operator representation $Q$ of $\mathcal{T}$ and a functional map $C$, we can construct its representaiton $Q'$ of $\mathcal{T}'$ by first mapping the signal from $\mathcal{T}'$ to $\mathcal{T}$, using $C^T$, then applying the operator $Q$, and then mapping the signal back to $\mathcal{T}'$, using $C$. 
\item This leads to a more general class of remeshing invariant (or equivariant) functions on meshes, satissfying
\eq{
f(Q) &= f(CQC^T) = f(Q')
CF(Q) &= F(CQC^T) = F(Q') 
}
for any $C \in O(n)$. 
\item The previous setting of permutation invariance an equivariance is a particular case, which can be thought of as a trivial remeshing in which only the order of the nodes is changed. 
\item Wang wet al (2019a) showed that given an eigendecomposition of the operator $Q = V \Lambda V^T$, any remeshing invariant or equivariant function can be expressed as 
$f(Q) = f(\Lambda)$ and $F(Q) = VF(\Lambda)$.
\item In other words, remeshing-invariant functions involve only the spectrum of $Q$. 
\item Indeed, functions of Laplacian eigenvalues have been proven in practice to be robust to surface discretization and perturbation. 

\end{itemize}

\newpage
\section{Learning in high dimensions}

\begin{itemize}
\item The simplest formalization of supervised learning considers a set of $N$ observations 
    $$
    \mathcal{D} = \{ \, (x_i, y_i) \, \}_{i=1}^N \,,
    $$
    assumed to be drawn in an i.i.d. fashion from an underlying data distribution $\mathbb{P}_{\textrm{data}}$, which is defined over $\mathcal{X} \times \mathcal{Y}$.  Here $\mathcal{X}$ and $\mathcal{Y}$ are respectively the data and the label domains. 
    
\item Importantly, $\mathcal{X}$ is a high-dimensional space; one typically assumes $\mathcal{X} = \mathbb{R}^d$ for some large dimension $d$. 

\item We further assume the labels $y$ are generated by an \emph{unknown} function $f$, so that  $y_i = f(x_i)$ for each $i =1, \dots, N$. 

\item The learning problem is then estimation of the function $f$. This is typically done through a parametrized class of functions, or \emph{hypothesis space},
	$$
	\mathcal{F} = \{ \, f_{\theta} : \theta \in \Theta\, \} 
	$$
	Neural networks (NNs) are examples of such parametric function classes, in which case $\theta \in \Theta$ is a vector network weights.

\item In this idealized setup, labels have no noise. Modern deep learning systems typically operate in this so-called \emph{interpolating regime}, where the estimated $f \in \mathcal{F}$ satisfy $\tilde{f}(x_i) = f(x_i)$ for all $i = 1, \dots, N$ (so ``interpolating regime" seems to mean that all training data must be correctly classified). 

\item In an ideal world, the performance of a given $f_\theta \in \mathcal{F}$ would be measured in terms of the expected performance on new samples drawn from $\mathbb{P}_{\textrm{true}}$, given a good choice of \emph{loss} $L(\,\cdot,\,\cdot\,)$
    $$
    \mathcal{L}(f_\theta) \triangleq \mathbb{E}_{\textrm{true}} \, L ( \, f_\theta(x), f(x) \,) ,
    $$
    in practice, this expectation (governing the random variable $x$) is approximated empirically. 
    
\item A successful learning scheme thus needs to encode the appropriate notion of regularity, or \emph{inductive bias} for $f$, imposed through the construction of the function class $\mathcal{F}$ and the use of \emph{regularisation}. 
\end{itemize}


\subsection{Inductive bias via function regularity}

\begin{itemize}
\item Even very simple choices of NN architecture yield dense hypothesis spaces, the subject of various \emph{universal approximation theorems}.
 
\item Universal approximation does not imply an \emph{absence} of inductive bias. Given a hypothesis space $\mathcal{F}$ with universal approximation, consider a functional $\mathfrak{c} : \mathcal{F} \to \mathbb{R}_+$ quantifying an abstract notion of \emph{function complexity} (Here, complexity is not used in the sense of spin glasses). Regularizing the given learning problem with respect to the functional $\mathfrak{c}$ amounts to seeking functions $f_* \in \mathcal{F}$ such that 
    $$
    f_* \in \textrm{argmin}_{\, g \,\in\, \mathcal{F}\, } \mathfrak{c}(g),
    $$
    subject to the ``interpolating regime" constraint that $f_*(x_i) = f(x_i)$ for all $i = 1, \dots, N$. 
    
    
\item For most function spaces, $\mathfrak{c}$ can be defined as a norm.

\item In the context of neural networks, $\mathfrak{c}$ factors through the parametrization, in that 
	$$
	\mathfrak{c}(f_\theta) \equiv \mathfrak{c}(\theta)
	$$ 
	The $\ell_2$-norm of the network weights, called \emph{weight decay}, as well as the so called \emph{path-norm} are popular choices in deep learning literature. 

\item From a Bayesian perspective, such $\mathfrak{c}$ may be interpreted as the negative log of the prior for the function of interest. \footnote{expand}

\item The regularizing functional $\mathfrak{c}$ can be enforced explicitly, by incorporating $\mathfrak{c}$ into the loss, or implicitly, as the result of an optimization scheme compatible with $\mathfrak{c}$. For instance, it is considered well-known that gradient-descent on an under-determined least-squares objective will choose interpolating solutions with minimal $\ell_2$-norm. 

\item All in all, a natural question arises: how are we to define effective priors that capture the expected regularities and complexities of real-world prediction tasks?
\end{itemize}

\subsection{The curse of dimensionality} 

\begin{itemize}
\item Consider a classical notion of regularity: $1$-Lipschitz functions $f : \mathcal{X} \to \mathbb{R}$, i.e. those satisfying $|f(x) - f(x') | \leq \| x - x' \|$ for all $x, x' \in \mathcal{X}$. 

\item If our only knowledge of the target function $f$ is that it is $1$-Lipschtiz, how many observations do we expect to require to ensure our estimate $\tilde{f}$ will be close to $f$? The general answer is necessarily exponential in the dimension $d$ of $\mathcal{X}$.

\item The situation is not better if one replaces the Lipschitz prior by, for instance, the Sobolev class $\mathcal{H}^s(\mathcal{X})$. There is an established minimax rate of approximation and learning for the Sobolev class of the order $\epsilon^{-d/s}$. This shows additional smoothness assumptions on $f$ can only improve the statistical picture when $s \propto d$, an unrealistic assumption.

\item The point of these examples is that we can't escape the curse of dimensionality if our only priors on the hypothesis space come from these classical notions of regularity. 

\item Fully-connected NNs define function spaces with more flexible notions of rigidity, due to the structure of the underlying computation graph. For instance, one can choose a regularization promoting sparsity in the weight vector $\theta$. This regularization allows these architectures to break this curse of dimensionality, but at the expense of making strong assumptions on the nature of the target function $f$. For instance, that $f$ depends on a collection of low-dimensional projections of the input.

\item In most real-world applications, functions of interest tend to exhibit complex long-range correlations unable to be expressed with low-dimensional projections. 

\item It is thus necessary to look for alternative source of regularity.

\end{itemize}

\subsection{Geometric priors}

\begin{itemize}

\item There is hope against the curse of dimensionality for physically-structured data, where one can employ fundamental principles of \emph{symmetry} and \emph{scale separation}.

\item The symmetry considered respects the structure of the \emph{domain} $\Omega$ of input signals. 

\item To be clear, we assume our machine learning system operates on \textbf{signals} (functions) on some domain $\Omega$. The space of $\mathcal{C}$-valued signals on domain $\Omega$ is
    $$
    \mathcal{X} (\Omega, \mathcal{C} ) = \{ x : \Omega \to \mathcal{C} \} 
    $$
    Here $\mathcal{C}$ a vector space, whose dimensions are called \textbf{channels}. An inner-product on $\mathcal{X}(\Omega, \mathcal{C} )$ is defined (presumably, as mentioned, to leverage some functional analysis?)  via
    $$
    \langle x, y \rangle_{\mathcal{X}} \triangleq \int_\Omega \langle\, x(u), y(u) \, \rangle_{\mathcal{C}} \, \mu( \textrm{d}u )\,,
    $$
    When $\Omega$ is discrete, $\mu$ can be taken as counting measure; going forward we omit $\mu$ from the notation, writing $\textrm{d}u$ for brevity. 
    
\item Our learning system exhibits ``scale separation" when important characteristics of the signal are preserved when flattening (or informally, projecting) the signal into a representative defined over a coarser domain. 

\item A prior on the hypothesis space respecting the symmetry of the domain and exhibiting scale-separation is called a \textbf{geometric prior}. 

\item Geometric priors are built into convolutional neural networks (CNNs) in the form of convolutional, weight-sharing filters (exploiting translational symmetry) and pooling (connected to scale separation). 

\item The main goal of what the authors call \emph{geometric deep learning} is to systematically extend ideas already present in CNN architecture to learning settings with other domains, such as graphs and manifolds. 

\end{itemize}

\subsection{Deformation stability}

\begin{itemize}

\item The symmetry formalism introduced above captures an idealised world where we know exactly which transformations are to be considered as symmetries, and we want to respect these symmetries \emph{exactly}. However, the real world is noisy and this model falls short in two ways.

\item While simple groups provide a way to understand global symmetries of the domain $\Omega$ (and by extension, of signals on it, $\mathcal{X}(\Omega)$ ), they do not capture \emph{local} symmetries well.

\item Consider a video scene with several objects, each moving along its own different direction. At subsequent frames, the resulting scene will contain approximately the same semantic information, yet no global translation explains the transformation from one frame to another. 

\item The discussion below distinguishes between two scenarios: the first in which the domain $\Omega$ is fixed, and where signals $x \in \mathcal{X}(\Omega)$ are undergoing deformations, and the setting where the domain $\Omega$ itself may be deformed. 

\end{itemize}

\n\hrulefill 
\emph{ Stability to signal deformations }
\hrulefill

\begin{itemize}

\item In many applications, we know a priori that a small deformation of the signal $x$ should not change the output of $f(x)$. It is tempting to consider such deformations as symmetries. For instance, we could view small \emph{(with respect to Dirichlet energy, for instance)} diffeomorphisms $\tau \in \textrm{Diff}(\Omega)$, or even small \emph{(localized?)} bijections as symmetries. 

\item ``Small" deformations can however be composed to form ``large" deformations, so small deformations do not form a group. 

\item Large deformations can materially change the semantic content of the input, and so it is not a good idea to use the full group $\textrm{Diff}(\Omega)$ as the symmetry group used to build a geometric prior. 

\item A better approach is to quantify how far a given $\tau \in \textrm{Diff}(\Omega)$, by using a ``complexity measure" $\mathfrak{c}(\tau)$ such that $\mathfrak{c}(\tau) = 0$ whenever $\tau \in G$. 

\item Examining our previous definition of exact invariance and equivariance under group actions, observe that we may relax (or generalize) these notions using the notion of \emph{deformation stability} (or \emph{approximate invariance}):
    \begin{align}
    \label{eq:def_stability}
    \| f ( \rho(\tau) x ) - f(x) || \leq C \mathfrak{c}(\tau) \| x\| , \quad \text{ for all } x \in \mathcal{X}(\Omega)
    \end{align}
    where $\rho(\tau) x(u) = x(\tau^{-1}u )$ as before, and where $C$ is a universal constant. A function $f \in \mathcal{F}(\mathcal{X}(\Omega) )$ satisfying \eqref{eq:def_stability} is said to be ($\mathfrak{c}$-)\textbf{geometrically stable}. 
    
\item Since $\mathfrak{c}(\tau) = 0$ for $\tau \in G$, this definition generalises the $G$-invariance property defined above. Its utility in applications depends on introducing an appropriate deformation cost. 

\item In the case of images defined over a continuous Euclidean plane, a popular choice is the \emph{Dirichlet energy} $\mathfrak{c}_{\textrm{Dirichlet}}$, 
    $$
    \mathfrak{c}_{\textrm{Dirichlet}}(\tau) \triangleq \left( \int_\Omega \| \nabla \tau(u) \| ^2 \textrm{d} u \right)^{1/2}
    $$
    which measures the ``elasticity" of $\tau$. 
\end{itemize}

\n\hrulefill 
\emph{ Stability to domain deformations }
\hrulefill

\begin{itemize}
\item Often the object being deformed is not the signal, but the underlying domain $\Omega$ itself. 

\item Letting $\mathcal{D}$ denote the space of all possible variable domains \footnote{(such as the space of all graphs, or the space of Riemannian manifolds)}. We suppose one can define for $\Omega, \tilde{\Omega} \in \mathcal{D}$ an appropriate metric $\mathfrak{d}( \Omega, \tilde{\Omega})$ satisfying $\mathfrak{d}(\Omega, \tilde{\Omega}) = 0$ if $\Omega$ and $\tilde{\Omega}$ are equivalent in some sense.

\item For example, the \emph{graph edit distance} vanishes when graphs are isomorphic, and the Gromov-Hausdorff distance between Riemannian manifolds \footnote{viewed as metric spaces equipped with geodesic distances} vanishes when two manifolds are isometric. 

\item A common construction of such distances between domains relies on some group $G = ( \eta )_{\eta \in G}$ of invertible mappings $\eta : \Omega \to \tilde{\Omega}$ which can be used to to ``align" the metric structure of two domains. 

\item One can then compare the distance or adjacency structures of $\Omega$ and $\tilde{\Omega}$ (which are denoted $d$ and $\tilde{d}$ respectively), and in particular one can define a distance between the metric spaces $(\Omega,d)$ and $(\tilde{\Omega}, \tilde{d})$ by 
\begin{align}
\label{eq:frak_d_def}
    \mathfrak{d}( \Omega, \tilde{\Omega} ) = \inf_{ \eta \, \in\, G } \left\|\, d - \tilde{d} \circ ( \eta \times \eta ) \, \right\|_{\Omega \, \times \, \Omega}  
\end{align}
    
    To be clear, for fixed $\eta$, we ``clone" this to a map $\eta \times \eta$ on pairs of points in $\Omega$. The function $\tilde{d} \circ (\eta \times \eta)$ is a real-valued function on $\Omega \times \Omega$ which reports the $\tilde{d}$-distance between the $\eta$-images of any pair in $\Omega \times \Omega$. Likewise, $d$ is a function on $\Omega \times \Omega$. The norm in the display above reports the (sup?) norm of the difference between these two functions. 
    
\item Slightly misusing notation, we write $\mathcal{X}(\mathcal{D}) = \{ (\mathcal{X}(\Omega), \Omega) : \Omega \in \mathcal{D} \}$ for the ensemble of possible input signals defined over a varying domain. A function $f : \mathcal{X}(\mathcal{D}) \to \mathcal{Y}$ is \textbf{stable to domain deformations} if

    $$
    \| f(x, \Omega) - f(\tilde{x}, \tilde{\Omega}) \| \leq C \| x \| \mathfrak{d} (\Omega, \tilde{\Omega}) 
    $$
    
    for all $\Omega, \tilde{\Omega} \in \mathcal{D}$ and $x \in \mathcal{X} (\Omega)$. \footnote{\emph{( Q: what about the signal $\tilde{x}$? )}}
    
    
\item It can be shown that stability to domain deformations is a natural generalisation of stability of signal deformations, by viewing the latter in terms of deformations of the volume form. \footnote{Expand, reference is Gama et al. 2019} 
    
    
\end{itemize}

\n\hrulefill

\begin{rmk}[J] Is the geometric learning framework related to / enhanced by the notion of ``concentration compactness," in which quotienting by a group compactifies a space of functions?
\end{rmk}

\n\hrulefill

\subsection{Scale separation}

\begin{itemize}
\item Deformation stability substantially strengthens the global symmetry priors, but is not sufficient in itself to overcome the curse of dimensionality. Informally speaking, there are still ``too many" functions that respect
    $$
    \| f( \rho(\tau) x ) - f(x) \| \leq C\mathfrak{c}(\tau) \|x \| \quad \text{ for all } x \in \mathcal{X} (\Omega) 
    $$
    without further structure on the prior. 
\item A key insight to strengthen geometric priors is to exploit the multiscale structure of physical tasks. 

\end{itemize}


\n\hrulefill 
\emph{ Multiscale representations }
\hrulefill

\begin{itemize}
\item The essential insight of multi-scale methods is to decompose functions defined over the domain $\Omega$ into elementary functions which are localised \emph{both in space and frequency}. 

\item In the case of wavelets, this is achieved by ``correlating" a translated and dilated filter ( \emph{mother wavelet} ) $\psi$, producing a representation called a \emph{continuous wavelet transform}
    $$
    (W_\psi x)(u, \xi) = \xi^{-1/2} \int_{-\infty}^\infty \psi \left( \frac{v-u }{\xi } \right) x(v) \textrm{d} v
    $$ 
    The translated and dilated filters are called \emph{wavelet atoms}; their spatial position and dilation correspond to the coordinates $u$ and $\xi$ of the wavelet transform, usually sampled dyadically, e.g. $\xi = 2^{-j}$ and $u = {2^{-j} k }$, where $j$ is called the \emph{scale}. 
    
\item Multi-scale signal representations capture regularity properties beyond global smoothness, and the multiscale, localised wavelet decompositions considered here are \emph{stable} in ways that Fourier decompositions are not.

\item Consider $\tau \in \textrm{Aut}(\Omega)$ and its associated linear representation $\rho(\tau)$. When $\tau(u) = u -v$ is a shift, the operator $\rho(\tau)$ is a \emph{shift operator}, say $S_v$ that commutes with convolution. 

\item Since\footnote{I want to try to say this in my own words} convolution operators are diagonalised by the Fourier transform, such a (spatial) shift operator becomes (on the frequency side) a multiplicative phase:
    $$
    \widehat{ S_v x } (\xi) = \exp ( -  \textrm{i} \xi v ) \hat{x} (\xi)
    $$
    The \emph{Fourier modulus} $f(x) = | \hat{x} |$ of the signal $x$ is then a shift-invariant function, $f(S_v x) = f(x)$, which we use as a case-study in stability (of Fourier versus multiscale representations).
    
\item Suppose however we have only an \emph{approximate} translation, $\tau(u) = u - \tilde{\tau}(u)$, with $ \| \nabla \tau \|_\infty \leq \epsilon$, one can show\footnote{It would be nice to write this computation out}
    $$
    \frac{ \| \, f( \rho(\tau) x ) - f(x) \, \| }{ \| x \| } = O(1) ,
    $$
    regardless of how small $\epsilon$ is, the ``error" here, by which I mean $\| f( \rho(\tau)  ) - f(x) \||$, is proportional to the magnitude of the signal $x$. 
    
\item In contrast, one can show the wavelet decomposition $W_\psi x$ is \emph{approximately equivariant} to deformations, in that
    $$
    \frac{ \| \rho(\tau) W_{\psi}(x) \| }{ \| x\| } = O(\epsilon) 
    $$
    
\item In other words, decomposing the signal information into scales using localized filters rather than frequencies turns a global unstable representation of the signal into a family of locally stable features. \footnote{It seems that what they are also doing in some sense is decomposing the ``global" translation symmetries themselves into a family of local symmetries. }

\item Importantly, such measurements at different scales are not yet invariant, and need to be progressively processed towards the low frequencies, hinting at the ``deep" compositional nature of modern neural networks.

\end{itemize}

\n\hrulefill 
\emph{ Scale separation prior }
\hrulefill

\begin{itemize}

\item Consider a multiscale coarsening of the data domain $\Omega$ into a heirarchy $\Omega_1, \dots, \Omega_J$. This can be done in a fairly natural way when working with a metric space.

\item The ``renormalization structure" $( \Omega_j )_{j =1}^J$ induces a sequence of signal-spaces
    $$
    \mathcal{X}_j ( \Omega_j, \mathcal{C}_j ) \triangleq \{ x_j : \Omega_j \to \mathcal{C}_j \}
    $$
    
\end{itemize}
    
\n\hrulefill

\begin{rmk}[ J ] $\quad$ I find it important to point out that the channels are allowed to \emph{change}. This is something I was not expecting when looking at a real-life convolutional net. 
\end{rmk}

\n\hrulefill

\begin{itemize}

\item A function $f : \mathcal{X} (\Omega) \to \mathcal{Y}$ is \emph{locally stable at scale $j$} if it admits a factorization of the form $f \approx f_j \circ P_j$, where 
    $$
    P_j : \mathcal{X}(\Omega) \to \mathcal{X}_j (\Omega_j) 
    $$
    is a non-linear \emph{coarse-graining} and $f_j : \mathcal{X}(\Omega_j) \to \mathcal{Y}$ is a function ``adapted" to the scale $j$. 
    
\item Effectively, this framework is based in the ansatz that while the target function $f$ may depend on complex long-range interactions between features over the whole domain, in locally-stable functions it is possible to \emph{separate} the interactions across scales, by first focusing on localized interactions that are then propagated towards the coarse scales, by first focusing on localized interactions that are then propagated towards the coarse scales.

\item Such principles are of fundamental importance in many areas of physics and mathematics, through the renormalisation group, or leveraged in important numerical algorithms like the \emph{Fast Multipole Method}, the latter developed to speed calculation of long-ranged forces in $n$-body problems. 

\item In ML, multi-scale representations and local invariance are the fundamental mathematical principles underpinning the efficiency of CNNs and Graph NNs, typically implemented in the form of \emph{local pooling}

\end{itemize}

\subsection{The blueprint of geometric deep learning}

\begin{itemize}
\item One can combine principles of symmetry, geometric stability and scale separation into a blueprint for learning stable ``representations" (in a non-algebraic sense) of high-dimensional data. 

\item These ``representations" are functions $f$ operating on signals $\mathcal{X}(\Omega, \mathcal{C})$ over the domain $\Omega$, the latter acted on by a symmetry group $G$.

\item Observe that expressivity, in this context, requires use of a non-linear object: if $f$ is linear and $G$-invariant, then for all $x \in \mathcal{X}(\Omega)$,
    $$
    f(x) = \frac{1}{\mu(G)} \int_G f (g.x) \mu( \textrm{d} g )  \\
     \equiv_{\,(\textrm{linearity})\,} f \left( \frac{1} { \mu(G) }\int (g.x) \mu (\textrm{d}g) \right)
    $$
    where $\mu$ denotes Haar measure on $G$.
    
\item In the context of images acted upon by a translation group, our expressivity would be limited to functions on the average RGB vector for each input, were we to restrict ourselves to linear components of our compositional model. 

\item The above reasoning shows the family of \emph{linear invariants} is not very rich. The family of \emph{linear equivariants} provides a much more powerful tool, as it enables the construction of ``rich" and stable features, \emph{by composition with appropriate non-linear maps}. 

\item Let $B : \mathcal{X}(\Omega, \mathcal{C}) \to \mathcal{X}(\Omega, \mathcal{C}') $ is $G$-equivariant satisfying $B(g.x) = g.B(x)$ for all $x \in \mathcal{X}$ and $g \in G$. Consider also $s: \mathcal{C}' \to \mathcal{C}''$ an arbitrary non-linear map. The composition 
    $$
    U := \mathbf{s} \circ B : \mathcal{X}(\Omega, \mathcal{C}) \to \mathcal{X} (\Omega, \mathcal{C}'')
    $$
    is also $G$-equivariant.

\item Above, $\mathbf{s} : \mathcal{X}(\Omega, \mathcal{C}') \to \mathcal{X}(\Omega, \mathcal{C}'')$ is the \emph{element-wise instantiation of $s$}, defined by 
    $$
    ( \mathbf{s} (x) )(u) \triangleq s( x (u ))
    $$
    

\item A natural question is whether any $G$-invariant function can be approximated at arbitrary precision by such a model, for appropriate choices of $B$ and $\sigma$. 

\item One can adapt standard universal approx. theorem arguments to show shallow ``geometric" networks are also universal approximators, ``by properly generalizing the group average to a general non-linear invariant." 

\item A single layer $U$ cannot approximate functions with long-range interactions, but a composition of several such layers, $U_J \circ \dots \circ U_1$ increases the \emph{receptive field} while preserving the "stability properties" of local equivariants. 

\end{itemize}

\n\hrulefill
\begin{rmk}
$\quad$ Is stability a mechanism for generalisation?
\end{rmk}
\n\hrulefill



The receptive field is further increased by interleaving downsampling operators that coarsen the domain.



\newpage

\appendix

\section{Appendix}

\subsection{On sets and categories}

We do not distinguish between a \B{set} \footnote{sets will not be defined rigorously, we think of them as ``a collection of things."} and the more general notion of a \B{class}. Categories are general enough that one should consider ``classes," not sets, of objects and morphisms. This is implicitly (through the definition of a graph) ignored below. 

There are adjectives like \emph{small} and \emph{locally-small} that one can prepend, seemingly to avoid set-theoretic considerations. The former means that the objects in the category are genuinely a set, the latter means that the collection of morphisms (or arrows) between any two objects are genuinely a set. These subtleties are lost on me, for now, so I will proceed naively and will avoid these terms if possible.  


The first example given in \cite{MacLane} is the \emph{metacategory of sets}: the objects are the class of all sets, and the arrows are the class of all functions. He later distinguishes it from the following object: 






\vspace{5mm}

\begin{mdframed}
\begin{eg}

\begin{itemize}
\item[ $\mathbf{0}$ ] is the empty category ( no objects, no arrows ). 
\item[ $\mathbf{1}$ ] is the category with one object and one ( identity ) arrow.
\item[ $\mathbf{2}$ ] is the category with two objects $a, b$ and just one arrow $a \to b$ not the identity. 
\item [ \emph{Sets} ] A category is \B{discrete} when every arrow is an identity, and is thus determined by the underling set of objects; we can thus view sets and discrete categories as the same notion. 

\item [ \emph{Monoids} ] A \B{monoid} is a category with one object. It is thus determined by the collection of arrows from the object to itself, which obey the composition axiom, and one of which is the identity. A monoid is the same notion as a \emph{semigroup} with identity. 

\item [ \emph{Groups} ] A \B{group} is a category with one object, in which every arrow has a (two-sided) inverse under composition; an arrow $e : a \to b$ is \B{invertible} if there is an arrow $e' : b \to a$ such that $e' \circ e = \id_a$ and $e \circ e' = \id_b$, in which case we write $e^{-1}$ for $e$. 


\item [ \emph{Matrices} ] The set of all rectangular matrices $\matr_{\mathbb{R}}$ with real-valued entries forms a category. The objects $\mathcal{O}$ are the positive integers, $\mathcal{O} \equiv \N = 1,2, \dots $, and each $m \times n$ matrix $A$ is regarded as an arrow $A: n \to m$. The composition law is matrix multiplication, and so invertible arrows are precisely invertible matrices. We can replace the real numbers $\R$ in this example with the complex numbers $\C$, or with any commutative ring $K$. 
\end{itemize}

\end{eg} 
\end{mdframed}

\vspace{5mm}

\begin{defn}
The category of sets is denoted $\set$, and it is defined as follows. We assume there is a big enough set $U$, whose elements are sets, and which we call the ``universe," and we describe a set $x$ as small if it is a member of the universe. We then take $\set$ to be the category whose set $U$ of objects is the set of small sets, and whose arrows are all possible functions between small sets. 
 \end{defn}

 
 The above definition is a special case of the following.
 
 \begin{defn} Given a set of sets, $V$, we take $\ens_V$ to be the category with objects all sets $X \in V$, and arrows all functions $f: X \to Y$, with the usual notion of composition. We may write $\ens$ to denote a general category of this form, for some unspecified $V$. 
 \end{defn}
 
Thus, $\set = \ens_U$ for some implicit \emph{universe} $U$. Before considering the category $\grp$ of groups, we give a category-free definition of these objects. 
 
 \begin{defn}
A set $G$, equipped with binary operation $\diamond : G \times G \to G$  called \emph{composition}, is a \textbf{group} if $(G,\diamond)$ satisfies the following ( Below, we write $g \diamond h$ more concisely as $gh$ ):
\begin{itemize}
   \item[\emph{( Associativity )}]: $(g_1 g_2) g_3 = g_1 (g_2 g_3)$ for all $g_1, g_2, g_3 \in G$.
    
    \item[\emph{( Identity )}] : there is a unique element $e \in G$ such that $eg = ge = g$ for all $g \in G$.
    
    \item[\emph{( Inverse )}] : for each $g \in G$, there is a unique inverse $g^{-1} \in G$ such that $gg^{-1} = g^{-1} g = e$
    
    \item[\emph{( Closure )}] : the group is closed under the composition operation $\diamond$. 
\end{itemize}
\end{defn}


\vspace{5mm}
\begin{mdframed}
\begin{eg} [ \emph{Translations} ] The four groups below encode some kind of one-dimensional translational symmetry, in the first two cases, the binary operation is the usual notion of addition. In the latter two, it is addition modulo $n$ and $N$ respectively. 

\begin{itemize}
\item[ \emph{(i)}] The real numbers $\mathbb{R}$.
\item[ \emph{(ii)} ] The subgroup of integers $\mathbb{Z}$.
\item[ \emph{(iii)} ] The cyclic group 
$$
C_n := \mathbb{Z} / n \mathbb{Z} = \{ 0,1, 2, \dots ,n-1\}
$$
with binary operation given by addition modulo $n$. 
\item[ \emph{(iv)} ] The circle of circumference $N$,
$$
N \cdot S_1 := \mathbb{R} / N \cdot \mathbb{R} = [0,N) .
$$
\end{itemize}
\end{eg}
\end{mdframed}

\vspace{5mm}

\begin{mdframed}
\begin{eg}[ \emph{Permutations} ]  Let $[n] := \{1, \dots, n\}$, and define the \B{symmetric group} on $n$ symbols $\frak{S}_n$ to be the collection of bijections $[n] \to [n]$, with group operation $\circ$ the usual function composition. 
\end{eg}
\end{mdframed}
\vspace{5mm}

\begin{mdframed}
\begin{eg}[ \emph{Products of groups} ] 
The cartesian product $G \times H$ of two groups has a natural group structure; the composition law is defined coordinate-wise through those of $G$ and $H$. This is the \B{direct product} $G\times H$. 

\begin{itemize}
\item[ \emph{(i)} ] The group $\mathbb{R}^d$ ( $d$-dimensional Euclidean space ) is the $d$-fold direct product of $\mathbb{R}$ with itself. Likewise, the group $\mathbb{Z}^d$ ( the $d$-dimensional integer lattice ) is the $d$-fold product of $\mathbb{Z}$ with itself. 
\item[ \emph{(ii)} ] $N \cdot (S_1)^{\times d}$ and $\mathbb{T}_n^d := C_n^{\times d}$ are respectively continuous and discrete tori. 
\end{itemize}


\end{eg}
\end{mdframed}

\vspace{5mm}


\begin{mdframed}
\begin{eg} [ \emph{Linear groups} ] We write $\GL(n)$ to denote the \B{general linear group}, which is the collection of $n \times n$ matrices with real entries. We write $\GL_{\mathbb{C}}(n)$ for the analogous object over $\mathbb{C}$, and if $V$ is a vector space, we write $\GL(V)$ for the group of (vector space) automorphisms from $V$ to itself. Important subgroups of $\GL(n)$ are the \B{orthogonal group} $O(n)$ and \B{special orthogonal group} $SO(n)$. An important subgroup of $\GL_{\mathbb{C}}(n)$ is the \B{unitary group}. 
\end{eg}
\end{mdframed}



\vspace{5mm}

The category $\grp$ has objects $\mathcal{O}$ consisting of all groups, while morphisms or arrows $\mathcal{A}$ between two groups $G$ and $H$ consist of the set of group homomorphisms between $G$ and $H$. 

\begin{defn}[ Group homomorphism ] A group homomorphism between two groups $(G,\diamond_G)$ and $(H,\diamond_H)$ is a function $\varphi: G \to H$ such that for all $g, g' \in G$, one has
$$
\varphi( g \diamond_G g') = \varphi(g) \diamond_H \varphi(g')
$$
\end{defn}



\vspace{5mm}

\subsubsection*{Functors}

\vspace{5mm}

Functors are a categorical generalization of group representations. Below, we will use the term \B{morphism} interchangeably with ``arrow." The collection of categories $\B{Cat}$ itself constitutes a category, whose morphisms are functors. 

\begin{defn}[ \cite{MacLane} ] Let $\mathcal{C}$ and $\mathcal{B}$ be categories. A \B{functor} $T: \mathcal{C} \to \mathcal{B}$ is a morphism between categories, and consists of two suitably related functions: the \emph{object function} $T$, which assigns to each object $c$ an object $Tc$ of $\mathcal{B}$, and the \emph{arrow function} (also written $T$) which assigns to each arrow $f : c \to c'$ of $\mathcal{C}$ an arrow $Tf : Tc \to Tc'$ of $\mathcal{B}$, such that
\begin{itemize}
\item[ \, ] $T(\id_c) = \id_{Tc}$, and
\item[ \, ] $T( g\circ f) = Tg \circ Tf$, whenever $(g,f)$ is a composable pair. 
\end{itemize}
\end{defn}

We now state three equivalent formulations of The following holds with $\mathbb{R}$ replaced by any field.

\begin{lem} Consider a group $G$ and an $\mathbb{R}$-vector space $V$. The following data are equivalent:
\begin{itemize}
\item A representation of $G$ on $V$, as in Definition~\ref{def:R_lin_rep}. 
\item A group homomorphism $G \to \GL(V)$.
\item A covariant functor from $G$ (considered as a one-object category) to the category of $\mathbb{R}$-vector spaces, $\mathbb{R}$-{\rm \B{Mod}}. 
\end{itemize}
\end{lem}

\subsubsection*{Functors and equivariance}


\begin{rmk} 
\label{rmk:curry}
Given an $\mathbb{R}$-linear representation of $G$ on vector space $V$, the map from $V \to V$ given by $v \mapsto g.v$ is invertible for any $g \in G$; this follows from the definition. 

In particular, \emph{(ii)} of Definition~\ref{def:R_lin_rep} implies that the map from $G \to \GL(V)$ defined by
$$
g \mapsto \left(\, v \mapsto g.v \, \right)
$$
is a group homomorphism. \emph{``So by using a currying\footnote{From Wikipedia: In mathematics and computer science, currying is the technique of converting a function that takes multiple arguments into a sequence of functions that each takes a single argument. Expand} argument, we have seen every group representation gives rise to a group homomorphism $G \to \GL(V)$. Conversely, given a group homomorphism $\rho : G \to \GL(V)$, we can uncurry to get a representation on $V$ by setting $\varphi(g,v) : = \rho(g) v$."}
\end{rmk}

The above remark demonstrates that the notion of an $\mathbb{R}$-linear representation of $G$ on $V$ is the same as the notion of a group homomorphism $G \to \GL(V)$. 




%
%
%
%
%
%
\newpage








\begin{comment}


\chapter{Neural nets}




\begin{itemize}
\item Tuca's spin glass course\dots\dots\,\emph{June 3 2019 lecture}
\item Gabrie 2020\dots\dots\, \emph{Mean-field inference methods for neural networks}
\item Andrew Ng \dots\dots\,\emph{Stanford video lectures on Youtube}
\item Goodfellow et al.\dots\dots\,\emph{Deep Learning}
\end{itemize} 
%%%%

%%%%
\newpage
\n\hrulefill
\vspace{2mm}
\section{learning}

\begin{shadowbox}
\emph{Goodfellow Ch1:}
\begin{itemize}
\item ``Inventors have long dreamed of creating machines that think." 
\item ``The true challenge to artificial intelligence proved to be solving the tasks that are easy for people to perform but hard for people to describe formally."
\item ``This book is about a solution to these more intuitive problems. The solution is to allow computers to learn from experience and understand the world in terms of a hierarchy of concepts." 
\item  ``This dependence on representations .. appears throughout CS and even daily life. In computer science, operations such as searching a collection of data can proceed exponentially faster if the collection is structured and indexed intelligently. " (more generally a comment on features, their relation to the notion of a coordinate system, and the fact that they don't come pre-specificed for all tasks, e.g. recognition tasks) 
\item ``One solution to this problem is to use machine learning to discover not only the mapping from representation to output but also the representation itself. This approach is known as {\az\B{representation learning}}. Learned representations often result in much better performance..."
\item ``When designing features or algorithms for learning features, our goal is usually to separate the {\az\B{factors of variation}} that explain the observed data... A major source of difficulty in many real-world artificial intelligence applications is that many of the factors of variation influence every single piece of data we are able to observe. The individual pixels in an image of a red car might be very close to black at night. The shape of the car's silhouette depends on the viewing angle. Most applications require us to disentangle the factors of variation and discard the ones we do not care about."
\item ``The idea of learning the right representation for the data provides one perspective on deep learning. Another perspective on deep learning is that depth allows the computer to learn a multi-step computer program. Each layer of the representation can be thought of as the state of the computer's memory after executing another set of instructions in parallel. 
\item ``There are two main ways of measuring the depth of a model. The first view is based on [ essentially the number of layers in the computation graph ]... Another approach, used by deep probablistic models, regards the depth of a model as being not the depth of the computational graph but the depth of the graph describing how concepts are related to each other."
\end{itemize}
\end{shadowbox} 

\vspace{5mm}

List of ML algorithms:

\begin{itemize}
\item linear regression
\item logistic regression
\item naive Bayes
\item autoencoder (example of representation learning)
\item multilayer perceptron 
\item long short-term memory (LSTM)
\end{itemize}

Let $\mathcal{D}$ denote a {\az\B{training set}}:
 \eq{
 \mathcal{D} & = (\, \mathcal{X}, \mathcal{Y} \,) \\
 & = \{ x_k, \, y_k \}_{ k = 1}^P ,
 } 
 where $\mathcal{X} \subset \R^N$, and where $\mathcal{Y} \subset \R^M$. $\mathcal{D}$ is assumed to be sampled in an iid manner from a joint distribution $\mu_{\text{pop}}(x,y)$. 

A training algorithm chooses a predictor $h$ from a {\az\B{hypothesis class}}, a collection of functions $\mathcal{X} \to \mathcal{Y}$ (often arising from such a collection of functions beetween the ambient spaces of input and output data). The choice is made to minimize a {\az\B{cost function}} $\mathcal{J}$, which is a metric of error. 

\begin{eg}[ cost function: empirical risk with respect to a loss function ]
The {\az\B{empirical risk}} 
\eq{
\mathcal{R}_{\ell}^{(\mathcal{D})} (h) \tri \frac{1}{P} \sum_{k=1}^P \ell ( y_k, \, h(x_k) ) ,
}
with respect to a {\az\B{loss function}} $\ell : \mathcal{Y} \times \mathcal{Y} \to \R$ is one example of a cost function. The loss function itself enables comparison of vectors in the output space.
\end{eg}


\begin{shadowbox}
\emph{Gabrie \S2.1: ``this learning objective nevertheless does not guarantee {\az\B{generalization}}, i.e. the ability of the predictor $h$ to be accurate on inputs ... not in the training set. It is a surrogate for the ideal, but unavailable {\az\B{population risk}}"}
\end{shadowbox} 


where the population risk is
\eq{
\mathcal{R}_\ell(h) \tri \E_{\text{pop}} \left( \ell ( Y, h(X) ) \right),
}
and where the pair $(X,Y)$ is sampled from $\mu_{\text{pop}}$ the expectation $\E_{\text{pop}}$ is taken with respect to this randomness. Notice that all this has been stated without introducing a network. 

Deep neural networks furnish a rich selection of parametrized hypothesis classes. The simplest such network we will consider is the {\az\B{perceptron}}, specifically the perceptron with a single neuron. We postpone giving the formal definition of a perceptron and other feed-forward networks.

Minimizing the empirical risk (or any cost function) is an example of a learning objective. When the hypothesis class is parametrized (i.e. $\mathcal{H} = ( h_\theta) _{\theta \in T}$ for a set of tuneable parameters $T$), this {\az\B{learning objective}} becomes an optimization problem in the model parameters. 

\begin{rmk}  The hypothesis class associated to a neural network is often chosen with respect to a (neural) {\az\B{activation function}}, or ``non-linearity." This will usually be denoted $\vp$. For this reason, I am not considering linear regression as an example neural network, even though the implicit computation graph is basically that of a single neuron perceptron.
\end{rmk}

{\az\B{Linear regression}} is a natural precursor to the perceptron. This learning model has a particularly simple hypothesis space. Recalling that each $x \in \mathcal{X}$ lies in $\R^N$, for such (input) training data, the linear regression hypothesis space is parametrized by a single weight vector $w \in \R^N$ and a scalar bias $b \in \R$. 

\begin{rmk}
Because each learning model has its own set of parameters, we refer to these generically as $\theta$ when the model is implicit. 
\end{rmk}

\begin{defn} [informal] A {\az\B{learning model}} for the probability measure $\mu_{\text{pop}}$ from which training data $\calD = (\calX, \calY) $ is sampled consists of 
\begin{itemize}
\item a hypothesis space,
\item a loss function (if the cost function is phrased in terms of empirical risk), which sets a precise learning objective
\item a training algorithm, which navigates the hypothesis space
\end{itemize}
\end{defn}

\begin{rmk} Above, the cost function is the empirical risk with respect to a loss function $\ell$. According to [Goodfellow et al. 2016], cross-entropy / maximum likelihood is used in practice because this method removes the need to tailor the cost function to the specific problem?
\end{rmk}

\begin{defn} Given training data $\mathcal{D}$, the {\az\B{linear regression hypothesis space}} is
\eq{
\mathcal{H}_{\text{lin.reg.}} = \{ h \,:\, h(x) = wx + b,\, w \in \R^N, \, b \in \R \}
}
\end{defn}



\begin{defn} The {\az\B{linear regression cost function}} is the one induced by the {\az\B{squared error}} loss function $\ell_{\text{SE}}$
\eq{
\ell_{\text{SE}}(y, \til{y})  = \| y - \til{y} \|^2 
}
\end{defn}

\begin{shadowbox}
{\iris\B{Implementation task}} \hspace{5mm} [Ng, Lecture 2.3] initially considers the hypotheses
\eq{
h_\theta(x) = \theta_0 + \theta_1 x
}
where $x$ is a scalar input-data variable, and where $\theta_0, \theta_1$ are scalar parameters of the learning model, and thus in this case the input and output dimensions $N$ and $M$ are both one. Our training data $\mathcal{D}$ consists of $P$ pairs of scalars, denoting input mapped to output. The cost function is the one arising from squared error loss function.\\

Ng then makes the additional assumption that $\theta_0 = 0$, so that the slope of the line is the only tuneable parameter. Let us write $\theta_1 = \theta$ for simplicity. \\

There are two functions of interest, in this example. The first is the hypothesis function $h_\theta(x) = \theta x$, and the second is the cost function, which is a function of $\theta$ only.\\

Importantly, we are given three co-linear data points, representing three pairs of training data. This is enough information to plot the cost function

\end{shadowbox} 

\vspace{10mm}

For now, we record that the single-neuron perceptron is a function parametrized by a single weight vector $w \in \R^N$, along with a scalar bias $b \in \R$, in addition to a choice of a non-linearity $\vp$



We express the function within the perceptron hypothesis class as $h_{w,b}$, defined to act on $x \in \textsf{X}$ as follows:
\eq{
h_{w,b}(x) \tri \vp(  w \cdot x + b) 
}
where $\vp$ is a fixed non-linearity. This can be thought of as applying a non-linearity to a function in the ``linear regression" class of hypotheses. 

Let us actually start by examining this even simpler instance: 

 \newpage

\section{forward and backprop} 

From [Goodfellow et al. 2016, Chapter 6]: We start by considering a computation graph describing how to compute a single scalar $u^{(n)}$ (they give loss on a single training example as a possibility for $u^{(n)}$. There are 
\eq{
n_i \equiv n(i) 
}
input nodes (the $i$ subscript indicates ``input," not a changing index). The $n_i$ input nodes are labelled 
\eq{
u_{1}^{(1)}, \dots, u_{n(i)}^{(1)} 
}
The gradient we want to compute is 
\eq{
\nabla_{(1)} u^{(n)} \tri \left( \frac{ \pa u^{(n)} }{ \pa u_j^{(1)} }  : \, j \in \{1, \dots, n_i \} \right)
}

Let us relabel slightly to be consistent with their notation: they write the input nodes as 
\eq{
u^{(1)}, \dots, u^{(n(i))} ,
}
and we simply label the rest of the nodes as $u^{(n(i) + 1)}, \dots, u^{(n)}$, with the last node corresponding to the single output node. 

\begin{rmk} They assume the nodes of the computation graph are ordered so that we can compute their output one after the other, and ultimately it seems that this is allowing the algorithm to be written more briefly. If we kept the indexing tracking the layers, a naive first implementation would have a double for-loop. But if I recall correctly, this is something Ng said could be avoided.
\end{rmk}

Below, each node $u^{(i)}$ is associated with an operation $f^{(i)}$ acting on the nodes $\mathbb{A}^{(i)}$ which are parents of node $u^{(i)}$. Thus $\mathbb{A}^{(i)}$ is a collection of nodes; let us write
\eq{
\text{Pa}(u^{(i)}) = \{ j : u^{(j)} \in \mathbb{A}^{(i)} \} 
} 
for the set of indices corresponding to parent nodes of $u^{(i)}$. The value stored at $u^{(i)}$ is computed via
\eq{
u^{(i)} = f^{(i)} ( \mathbb{A}^{(i)} )
}

\vspace{10mm}


\begin{shadowbox}
\B{Algorithm 6.1:} \emph{(forward propagation)}
\begin{itemize}
\item[] for $j = 1, \dots, n_i$ \,:
\item[] \hspace{5mm} $u^{(j)} \leftarrow x_j$  
\hspace{5mm}{\az{\# store single input vector $x = (\,x_1, \dots , x_{n_i}$\,)}}
\item[] end for
\item[] for $i= n_i +1, \dots, n$ \,:
\item[] \hspace{5mm} $\mathbb{A}^{(i)} \leftarrow \{ u^{(i)} | j \in \text{Pa}(u^{(i)}) \}$
\hspace{5mm}{\az{\# store values of parent nodes }}
\item[] \hspace{5mm} $u^{(i)} \leftarrow f^{(i)} ( \mathbb{A}^{(i)} ) $
\hspace{5mm}{\az{\# compute value at $i$\oith \, node }}
\item[] end for
\item[] return $u^{(n)}$
\end{itemize}

%i = 1, ... , n_i:
 %    u^(i) <-- x_i 
%end for

%for i = n_i +1, ... , n:
  %   A^i <-- { u^(j) | j in Pa(u^(i)) 

\end{shadowbox}

\vspace{10mm}

\begin{rmk}
\begin{itemize}
\item ``Any procedure computing the gradient of output layer with respect to variables in input layer will need to choose between storing computations of or recomputing various subexpressions"
\item ``The amount of computation required for performing backprop scales linearly in the number of edges of the computation graph, where the computation for each edge corresponds to computing a partial derivative of one node with respect to a parent"
\item `` The backprop algorithm is designed to reduce the number of common subexpressions without regard to memory.
\end{itemize}
\end{rmk}

Backprop leverages the chain rule: for example, if $u^{(n)}$ lives in the $\ell$\oith\, layer, suppose that $u^{(j)}$ corresponds to some node in layer $\ell-2$. If we have computed the partial derivatives $\pa_{(i)} u^{(n)}$ for each node $u^{(i)}$ in layer $\ell -1$, we may then compute $\pa_{(j)} u^{(n)}$ as follows:
\eq{
\frac{ \pa u^{(n)} }{ \pa u^{(j)}} = \sum_{i : j \in \text{Pa}( u^{(i)} ) } \frac{ \pa u^{(n)} }{ \pa u^{(i)} } \frac{ \pa u^{(i)} }{ \pa u^{(j)} } 
}
or, writing $\pa_{(i)}$ for $\frac{\pa}{\pa u^{(i)} }$, 
\eq{
\pa_{(j)} u^{(n)} = \sum_{i : j \in \text{Pa}( u^{(i)} ) } \pa_{(i)} u^{(n)} \pa_{(j)} u^{(i)} 
}
This is accomplished by building up a data structure of derivatives of $u^{(n)}$ with respect to a growing sequence of nodes and computing whatever derivatives are necessary along the inter-layer edges. 

\vspace{10mm}




\begin{shadowbox}
\B{Algorithm 6.2:} \emph{(simplified backprop)}
\begin{itemize}
\item Run the forward propagation algorithm (6.1) to obtain the activations of the network. 
\item Initialize $\nabla$-table, denoted $\mathcal{G}$, a data structure which stores derivatives that have been computed. The entry 
\eq{
\mathcal{G}[ u^{(i)} ] 
}
stores the computed value of $\pa_{(i)} u^{(n)}$. 
\item $\mathcal{G}[ u^{(n)} ] \leftarrow 1$
\item for $j = n-1$ down to $1$ \,:
\item[] \hspace{5mm} $\mathcal{G}[u^{(j)}] \leftarrow \sum_{i : j \in \text{Pa}(u^{(i)}) } \mathcal{G}[ u^{(i)} ] \pa_{(j)} u^{(i)}$  
\hspace{5mm}{\az{\# apply chain rule to compute using existing values in $\mathcal{G}$ and a single computation along an edge between two layers}}
\item return $\mathcal{G}$
\end{itemize}

%i = 1, ... , n_i:
 %    u^(i) <-- x_i 
%end for

%for i = n_i +1, ... , n:
  %   A^i <-- { u^(j) | j in Pa(u^(i)) 

\end{shadowbox}

\vspace{10mm}

 This is a naive algorithm as well as a simplified one: it is naive because the true implementation has a ``dynamic programming" strategy, which fills a table of possible subexpressions as computed, to avoid recomputing; backprop takes advantage of being able to sort the computations in a natural way according to the chain rule. I think this also corresponds to a pruning of the data structure that takes place in later algorithms. Finally, this is simplified because we have only computed the gradient of the output layer relative to a loss function evaluation on a single training example. In practice, mini-batching is used. 
 
 \newpage
 Andrew NG: SVM allow for infinite number of features, this sounds like spin glass. 
 
 Unsupervised learning used by google news -- clustering
 
 
 
 \vspace{20mm}
 
 
  of \emph{training data}; here $X = (X^1, \dots, X^N)$ is a set of data, with each $X^i$ a vector in $\R^d$. For the sake of simplicity, we assume the proper label $y^i$ is one-dimensional, even binary. 

\begin{figure}[h]
\centering
\includegraphics[scale=1]{./images/net.eps}
%\caption{} 
\label{fig:net}
\end{figure}

Above, we have a sketch of the neural net computation graph. A \emph{feedforward neural net} has the above multipartite structure. The left-most nodes represent the input layer, layer zero, thus there are $d$ nodes in this layer. We suppose that each edge $e$ in the computation graph is associated to a weight $w_e$. Between the zeroth and first layers is the weight matrix $W^0$, and the value of the net at the first layer, on input $X \equiv X(0)$ is 
\begin{align}
X(1) = \vp ( W^0 X(0) ) \,,
\end{align}
where $\vp$ is a function called the \emph{nonlinearity}, and where it is understood to act entry-wise to produce the vector on the left. The values at each layer are computed inductively until the last. We'll say that the \emph{architecture} of the net is the computation graph, nonlinearity pair. Once one knows the architecture and the weights, the function computed by the neural net is determined. 

\subsubsection{training} To train a net, we need a way of evaluating its performance on training data. This is done through a loss function, such as mean-squared error, defined below:
\begin{align}
L(w) = \frac{1}{N} \left( \sum_{i=1}^N  \left( \wt{y}^i - y^i \right)^2 \right)^{1/2} \,,
\end{align}
where we emphasize that the loss function is a function of the \emph{weights} $w$. The above sum takes place over the training data, and the weights are implicit in the $\wt{y}^i$; these are outputs of the neural net on $X^i$ for the weights $w$. 

The process of learning amounts to adjusting the weights according to the loss function. It is here that a possible analogy with spin glasses emerges: the space of possible weights is a high-dimensional space. A loss function is a function on this high-dimensional space. Naturally, one asks: how similar is the geometry of this non-random function to the geometry of the landscape of a spin glass model?

Of course, there are many choices to be made in order to produce the landscape in the machine learning setting: we must choose an architecture, we must choose a class of loss function (as in the example above), and finally, the function itself is determined by the training data. 

Nonetheless, the spin glass metaphor has been put forward because navigation of the loss function landscape to a low enough local minimum is the task of learning, and learning dynamics suggest similarities with spin glass landscapes. For instance, when one runs two different instances SGD with the same starting configuration of weights, the learning dynamics land in two distinct but relatively good local minima, and these minima are essentially orthogonal in the weight space. This suggests an abundance of orthogonal minima, which is a feature of spin glass landscapes. 

According to the CS community \CITE, in contrast with the pure $p$-spin spherical model, which is 1-RSB and hence has diverging energy barriers between local minima, it's expected that there are \emph{not} diverging energy barriers between the local minima in loss function landscapes. Here there are perhaps canyons connecting the local minima. 

We now give another example of a learning scheme (which is apparently distinct from the above feed-forward neural net). \\

\begin{shadowbox}
\begin{eg}[Student-Teacher] We imagine that we have two feed-forward neural nets of the same type: there are only two layers, an input layer and an output layer. One is called the \emph{teacher}, the other is the \emph{student}. We assume that the output is one-dimensional, so that the weights of each net can be encoded as a vector. The input layer will have dimension $d$. 

Suppose that we generate a collection of unlabeled training data $X^1, \dots, X^N$, each an i.i.d. gaussian vector in $\R^d$. We next generate the weight vector for the teacher net, $w = (w_1, \dots, w_d)$, by taking an i.i.d. sample from some unknown law $\prob_w$, at least the law is not known to the student. For some nonlinearity $\vp$, we then use the teacher weights to generate labels $y^i$ for each input $X^i$ via $y^i := \vp(w \cdot X^i)$. 

The teacher communicates to the student the now labeled data $(X,y)$ as well as the nonlinearity $\vp$ used, and the task of this example is to estimate $\prob_w$. 
\end{eg}
\end{shadowbox}

\four

References here are apparently Gardner '89, Gardner-Derrida '89, Giorgi '89 and Sompolinksy et al '90. Gardner-Derrida showed that the best estimate a student can make (according to what metric??) for the law of the weights is given by 
\begin{align}
\prob(w | (X,y) ) = \frac{1}{Z(X,y)} \prod_{i=1}^N \prob(y^i | \vp, X^i )  \,,
\end{align}
which has the form of a Gibbs measure. I'm unclear about how to obtain the probabilities on the right-hand side though! Moreover, is the above clearly a spin glass? If the inputs are i.i.d. gaussian vectors, do these constitute the disorder of the model in some natural way? 

Write $\al = N/d$. Gardner-Derrida compute the limiting free energy of this model, where one assumes that $N,d \to \infty$ keeping $\al$ fixed. The conclusion seems to be that this model is RS, and the model gives rise to the following phase diagram:

\begin{figure}[h]
\centering
\includegraphics[scale=1]{./images/alg-inf-thresh.eps}
%\caption{} 
\label{fig:net}
\end{figure}

On the vertical axis, the generalization error is being plotted as a function of $\al$. The black curve represents the ``true" optimal generalization error, i.e. there is $\al_{inf}$, the first intersection of a vertical line with the horizontal axis, above which the generalization error is zero. Here learning is information theoretically possible. On the other hand, the green curve represents the generalization error obtained from approximate message passing (AMP), and the region enclosed by the two curves is considered computationally hard. The green curve creates another critical parameter, $\al_{alg}$ above which it is algorithmically feasible to have zero generalization error, and below which it is not. One then assumes that AMP is a good representative of the performance of any algorithm. The information theoretic threshold is like a static phase transition, while the algorithmic threshold is like a dynamical phase transition, I guess. \\

\begin{shadowbox}
\begin{eg}[Perceptron] Consider $S^{N-1}$, the sphere in $\R^N$ of radius $\sqrt{N}$, and let $X_1, \dots, X_M$ be points chosen uniformly at random on the sphere. Write 
\begin{align}
J := \bigcap_{i=1}^M \left\{ y \in S^{N-1} : y \cdot X_i \geq 0 \right\} \,,
\end{align}
this is in some sense the ``solution space," if we consider a solution to the points $X_1, \dots, X_M$ as a unit vector having non-negative inner product with all of the $X_i$. The $X_i$ I believe are called \emph{patterns}, and in some sense a solution vector \emph{stores} all the patterns given. 
\end{eg}
\end{shadowbox}

\four 

Writing $\al = M /N$ and taking the $N \to \infty$ limit of this model with the ratio held constant, one finds that there is a critical storage capacity $\al^*$ such that for any $\al > \al^*$, 
\begin{align}
\prob (J \text{ non-empty } ) \leq \exp \left(-NK(\al) \right) \,,
\end{align}
and for $\al < \al^*$, we can make sense of the limit (.. is this even the right normalization?)
\begin{align}
\frac{1}{N} \log \vol(J) \to \vp(\al) \,,
\end{align}
i.e. the solution space has an asymptotic volume? By Scherbina '03, it's known that $\al^* = 2$. This is completely open on the hypercube, where there is a connection with this model and $k$-SAT. \\

\begin{shadowbox}
\begin{rmk} Instead of ``balanced" half-spaces, one can consider constraints of the form $y \cdot X_i \geq \kappa$ for $\kappa >0$, or $y \cdot X_i \geq - \kappa$ for $\kappa > 0$. The former is apparently an easier model, while the latter is harder. The critical storage capacity now depends on $\kappa$, and for certain parameters the model is expected to be full-RSB. {\red{What does this mean?}} 
\end{rmk}
\end{shadowbox}

\begin{comment}

\chapter{open problems}

%%%%
\newpage
\n\hrulefill
\vspace{2mm}
\section{\yellow{Open questions}}
\begin{align*}
\text{June 5}
\end{align*}
\vspace{2mm}
%%%%


\begin{enumerate}
\item Parisi measures (SK and mixed $p$-spin) 
\subitem
\begin{shadowbox}
(i) Given a model on the hypercube, show that for all $\beta \geq 0$, $\mu_*$ has a jump at $\sup \supp (\mu_*)$. 
\end{shadowbox}
\subitem
\begin{shadowbox}
(ii) Show that for the SK model on the hypercube, the model is FRSB for $\beta > \beta_c$, or even just at sufficiently low temperature. 
\end{shadowbox}
\subitem
\begin{shadowbox}
(iii) Extend strict convexity of $\mathcal{P}$ to other functionals arising from other PDEs in spin glass theory. 
\end{shadowbox}
\subitem
\begin{shadowbox}
(iv) On the sphere, understand the properties of $\mu_*$ as in the spirit of (i). Analogously to questions we ask about first-passage limit shapes, what measures are realizable as Parisi measures. In particular, given an atomic measure, does there exist a model-temperature pair whose Parisi measure is this given measure? 
\end{shadowbox}
\subitem
\begin{shadowbox}
(v) Explore Parisi's PDE characterization of $f(q) = \mu_*[0,q]$. 
\end{shadowbox}
\item Fluctuations (same models)
\subitem
\begin{shadowbox}
(i) Can one deduce a limiting law for 
\begin{align*}
a_N ( N^{-1} \log Z_N - \mathcal{P}(\beta) )
\end{align*}
 for some sequence $a_N$? This question is asked at low temperature; at high temperature the fluctuations are gaussian.
\end{shadowbox}
\subitem
\begin{shadowbox}
(ii) Compute the order of the variance of $\log Z_N$. 
\end{shadowbox}
\subitem 
\begin{shadowbox}
(iii) What is the rate of convergence of $\E N^{-1} \log Z_N$ to $\mathcal{P}(\beta)$? Note that for some spin glasses, this question is different from the one directly above. 
\end{shadowbox}
\subitem
\begin{shadowbox}
(iv) The same three above questions, but for the ground state. 
\end{shadowbox}
\subitem 
\begin{shadowbox}
(v) We know that 
\begin{align*}
\E \la \psi(R_{12} ) \ra \to \int \psi(x) d\mu_*(x) \,,
\end{align*}
so what can we say when we remove the mean? i.e., do we have convergence almost surely, can we make statements about concentration?
\end{shadowbox}
\item Large deviations for overlap matrix and free energy in these models: nothing is known. 
\item Misc. in these models
\subitem
\begin{shadowbox}
(i) How different are $S^{N-1}$ and $\{\pm 1\}^N$ in the following sense: for instance, writing $\sig'$ as the argmin for the spherical model, and $\sig''$ as the argmin for the hypercubic model, does
\begin{align*}
\frac{1}{N} \la \sig' , \sig'' \ra \,
\end{align*}
converge? Does it converge to zero? Similarly, what can be said about the limit of
\begin{align*}
\frac{1}{N} ( H_N(\sig') - H_N(\sig'') ) \,?
\end{align*}
\end{shadowbox}
\subitem
\begin{shadowbox} 
(ii) Given $\sig = (\sig_1, \dots, \sig_N) \sim G_N$, i.e. a single spin configuration sampled from the Gibbs measure, consider the spin correlation matrix $(\la \sig_i \sig_j \ra )_{ij}$, where brackets denote the ensemble average with respect to $G_N$. What can we say about the eigenvalues of this matrix? What can be said about the maximum entry?
\end{shadowbox}
\subitem 
\begin{shadowbox}
(ii) We can ask the same questions for a different matrix. Instead of sampling once from the Gibbs measure, sample an infinite number of replicas $\sig^1, \sig^2, \dots$, and consider the matrix of overlaps, whose $ij$th entry is
\begin{align*}
N^{-1} \la \sig^i , \sig^j \ra \,,
\end{align*}
where brackets denote inner product. 
\end{shadowbox}
\subitem
\begin{shadowbox}
(iii) In the SK model at the critical temperature $\beta =1$, show that the limit of
\begin{align*}
N^{2/3} \E \la R_{12}^2 \ra 
\end{align*}
exists and lies in $(0,\infty)$. This seems do-able; apparently one can answer the analogous question using free energy fluctuations in subcritical case. Now critical fluctuations are known, so perhaps the same technique can be applied. See Aizenman-Lebowtiz-Ruelle. 
\end{shadowbox}
\item Perceptron
\subitem
\begin{shadowbox}
(i) Find out what happens to the negative perceptron; for instance, for $\kappa >0$, can one deduce a limit for
\begin{align*}
\vol\left( \bigcap_{i=1}^M \{ \sig : \la \sig , x^i \ra \geq - \kappa \} \right) \,,
\end{align*}
at some fixed ratio $\al = M/ N$. 
\end{shadowbox}
\subitem 
\begin{shadowbox}
(ii) For the positive perceptron on the hypercube, prove something about the fluctuations of the volume. 
\end{shadowbox}
\item Landscape questions 
\subitem
\begin{shadowbox}
(i) Compute the quenched complexity of $\crit_{N,k}$ in the pure $p$-spin models for $k$ finite.
\end{shadowbox}
\subitem
\begin{shadowbox}
(ii) Compute the quenched complexity in general (i.e. for $\crit_N$). 
\end{shadowbox}
\subitem
\begin{shadowbox}
(iii) Do something regarding the TAP complexity for the hypercube models. The TAP complexity is associated to a \emph{free energy} landscape, thus this landscape depends on $\beta$. 
\end{shadowbox}
\subitem
\begin{shadowbox}
(iv) The gradient descent question: can one understand
\begin{align*}
\lim_N \lim_{t \to \infty} H_N(X_t) ; \quad dX_t = -\nabla H_N(X_t) dt \,,
\end{align*}
with uniform starting position. One can also ask this question in expectation. Moreover, if one considers instead Langevin dynamics given by
\begin{align*}
d X_t = - \nabla H_N(X_t) dt + \eps dB_t \,,
\end{align*}
one can also examine the two-time correlations of this process with itself, or the one-time correlation between the Langevin dynamics and the coupled brownian motion: $\la X_t, X_s \ra$ and $\la X_t, B_t \ra$, where $t$ and $s$ are both taken to $\infty$ in a good way.
\end{shadowbox}
\subitem 
\begin{shadowbox}
(v) One can study the dynamical phase transition through the TAP free energy (at high temperature). This relates to the previous problem. 
\end{shadowbox}
\subitem
\begin{shadowbox}
(vi) Try to compute the complexity for $q$-partite models via Kac-Rice. Perhaps it's useful to let $q$ tend to $\infty$ with the dimension. 
\end{shadowbox}
\subitem
\begin{shadowbox}
(vii) Say something about nodal sets of pure $p$-spin. (Christian's thesis to be) 
\end{shadowbox}
\subitem 
\begin{shadowbox}
(viii) Quenched complexity for critical points of diverging index, starting from finer asymptotics for the mean coming from random matrix theory.
\end{shadowbox}
\subitem 
\begin{shadowbox}
(ix) The AT line: consider SK model on the hypercube at temperature $\beta$ and external field $h$. When $h =0$, it's known that the model is RS in the high temperature regime, conjectured FRSB in low temperature regime. When the external field is non-zero, it's known that the model is RS in a compact range of $\beta \in (0,1)$: can one extend this to the full interval? And can one say anything in low-temperature?
\end{shadowbox}
\subitem 
\begin{shadowbox}
(x) Check 16.7 of Talagrand for a reformulation of (v). 
\end{shadowbox}
\item EA model
\subitem 
\begin{shadowbox}
(i) Anything
\end{shadowbox}
\subitem 
\begin{shadowbox}
(ii) Tuca, Pax and Christian are working on showing that the number of local minima of EA tend to the number of local minima of SK as $d \to \infty$
\end{shadowbox}
\item Misc. (general)
\subitem 
\begin{shadowbox}
(i) Find an algorithm to compute $Z(J)$ for given disorder. This is NP hard. But try to find an algorithm for which
\begin{align}
\prob( Z_A(J) = Z(J) ) = 1- o(1) \,,
\end{align}
and do the same for the ground state.
\end{shadowbox}
\subitem 
\begin{shadowbox}
(ii) Study SG models on random graphs (some questions are easier here, for instance the second question in the EA section is an exercise for random d-regular graphs.
\end{shadowbox}
\subitem 
\begin{shadowbox}
(iii) The random Lorentz gas: for $\al > 0$, consider a PPP uniform in the box with $\al$ presumably controlling the density? Thicken each point in the process to a disc and run billiard dynamics. Is there an $\al_c$ such that for $\al < \al_c$ the billiard dynamics percolate (i.e. make it from one side of the box to another)? 
\end{shadowbox}
\end{enumerate}
\newpage

\chapter{barriers}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\large \cite{Ros_2019}} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{setup} \hrulefill\\

Let
\begin{align*}
\bbS_N \tri \leftcb \sig \in \R^N : \| \sig \|^2 = N \rightcb.
\end{align*}
For $\sig \in \bbS_N$, define
\begin{align*}
H(\sig)
	 \equiv 
	 	\HNp(\sig) 
			\tri 
				 \frac{1}{ N^{(p-1)/2}} \sum_{ i_1 \, ,\,  \dots \, ,  \, i_p} J_{i_1, \, \dots \, , i_p } \sig_{i_1} \cdot \, \dots\, \cdot\sig_{i_p} \,,
\end{align*}
where the coefficients $J_{i_1, \dots, i_p}$ are i.i.d. standard Gaussians. This is how the model is defined in math papers. One should be aware that the hamiltonian in the Ros et al. paper has an extra factor of $1/ \sqrt{2}$.

\four

\subsubsection{Complexity} Rescale the configuration space to the unit sphere 
\begin{align*}
\bbS \tri \leftcb \sig \in \R^N : \| \sig \|^2 = 1 \rightcb
\end{align*}
and also rescale the values taken by the Hamiltonian: for $s \in \bbS$, define
\begin{align*}
h(s) \tri N^{-1/2} H ( \sqrt{N} s ) \,,
\end{align*}
so that for any $s, t \in \bbS$, we have $\E h(s) h(t) =  \la s, t \ra^p$. For $u \in \R$, let $\mathcal{C}_N(u)$ denote the following set of critical points:
\eq{
\mathcal{C}_N(u) \tri \{ s \in \bbS : \bm{g} (s) = 0,\, h(s) \leq \sqrt{N} u \} ,
}
where $\bm{g}$ is the spherical gradient. Define
\eq{
\crit_N( u) \tri \# \mathcal{C}_N(u).
}
This random variable gives some indication of how rough the landscape is. It was shown in  \cite{ABC} that, for $u$ within a certain range of energies,
\eq{
\lim_{N \to \infty} \frac{1}{N} \log \E \crit_N(u) = \Sigma(u) \equiv \Sigma_p(u) > 0 \,,
}
where $\Sigma(u)$ is called the complexity function. One can also restrict attention to critical points of a given index $k = 0,1, \dots, N-1$, defining
\eq{
\mathcal{C}_{N,k}(u) \tri \{ s \in \mathcal{C}_N(u) : \ind (\bm{\ch}(s)) = k \}
}
and 
\eq{
\crit_{N, k} (u) \tri \# \mathcal{C}_{N,k}(u) \,,
}
and it was also shown that 
\eq{
\lim_{N \to \infty} \frac{1}{N} \log \E \crit_{N,k}(u) = \Sigma_k(u) \equiv \Sigma_{p,k}(u) >0  \,,
}
within an appropriate range of energies. 

\four

\subsubsection{Bottom of the landscape} The low-energy configurations of the sphere are physically relevant. The \emph{bottom of the landscape} is the pre-image of the interval $\sqrt{N} \cdot (\sfE_0, \sfE_\infty)$, where $\sfE_0 \equiv \sfE_0(p) $ denotes the ground state energy of the model, and where $\sfE_\infty \equiv \sfE_\infty(p)$ is called the \emph{threshold energy}, given explicitly by
\eq{
\sfE_\infty = - 2\sqrt{  (p-1) / p } ,
}


\subsubsection{Nested structure}

In studying finite-index critical points 


\subsubsection{Goal of the paper: ``geometric information"} 

Counting critical points does not tell us how they are arranged in space. Average vs typical. 

\subsubsection{How the paper counts critical points} 
``Take a fixed minimum $\upsigma_*$ drawn at random from the population of minima with energy $u_0 \in ( \gs, \thr )$."



For $q \in (-1,1)$ and $u \in ( \gs, \thr )$, define $\crit_N (u, q | u_0)$ as the number of critical points with energy $v$ at a fixed distance $1-q$ from the selected $\upsigma_*$. Taken at face value, this random variable is zero almost surely. Before stating the main results, we suggest a possible alternative to this random variable. 

\subsubsection{How we count critical points} 

Consider the rescaled field $h$, and let $(0, \dots, 0,1) \tri \np \in \bbS$ denote the north pole of the unit sphere. Let $\un{h} \equiv \un{h}_{\,u_0}$ denote the field $h$ conditioned on the event
\begin{align*}
\{ \bm{g}(\np) = 0 , \, h(\np) = \sqrt{2N} u_0 \} ,
\end{align*}
and let 
\eq{
\mathcal{C}_N(u , q | u_0 ) = \leftcb s \in \bbS : \un{h} (s) \in \sqrt{N} (u - \delta_1, u), \, \un{\bm{g}}(s) = 0, | \la \np, s \ra - q | < \delta_2 \rightcb
}
where $\sqrt{N}\cdot (a,b)$ is shorthand for the interval $( \sqrt{N} a , \sqrt{N} b )$, and where $\un{\bm{g}}$ is the spherical gradient of $\un{h}$. Let us write 
\eq{
\crit_N(u ,q | u_0 ) \tri \# \mathcal{C}_N(u,q | u_0 ).
}
Because it is sometimes useful to write in terms of indicator functions, let $E_u \tri (u -\delta_1, u)$, and let $I_q \tri (q - \delta_2, q + \delta_2)$. \\

\four

\emph{(Let us be clear: we are not constraining the index of the Hessian at $\np$. Because local minima dominate the complexity at the bottom of the landscape, this doesn't seem too egregious.)}

\four
\four

\subsubsection{main result}

\four
\begin{thm}[{\cite[display (3)]{Ros_2019}}] 
\begin{align*}
\Sigma ( u,q | u_0 ) = \frac{1}{2} \log \left( \frac{p}{2} \left( \til{z} - v \right)^2 \right) + \frac{ p \left( v^2 + v \til{z} \right) }{2(p-1) } + \frac{Q}{2}
\end{align*}
where
\begin{align*}
Q = \log \left( \frac{ 1- q^2} { 1- q^{2p-2} } \right) - 2 \left( \eps_0^2 U_0(q) + \eps_0 \eps U(q) + \eps^2 U_1(q) \right)
\end{align*}
with $\til{z} = \sqrt{ \eps^2 - \thr^2 }$ and
\begin{align*}
U_0(q) &=  
	\frac
		{ 
			q ^ { \, 2p } ( \, - q ^ {\, 2p } + p ( \, q ^ {\,2} - q ^ {\,4} \, ) + q ^ 4 \,)
		}
		{
			q ^ { \, 4p } - ( \, ( \, p - 1 \, )^{ \,2} ( \, 1+q \, )^{\, 4} - 2( \, p-2 \, ) pq^{\,2} \, ) q^{\,2p} + q^{\,4} 
		}\\
U(q)	&=
		\frac{
				2 q ^ { \, 3p  } ( \,  p ( \, q^{\,2} - 1 \, ) + 1 \, ) - 2 q^{\,p+4}
		}
		{
				q^{ \, 4p } - ( \, ( \, p-1 \, )^{\, 2} ( \, 1+q \, )^{\,4} - 2( \, p-2 \, ) pq^ {\, 2} \, ) q^{ \, 2p} + q^{\,4} 
		}\\
U_1(q)	&=
	\frac{
	q^{\,4} - q^{\,2p} ( \, p ( \, ( \,  p-1 \, ) q^{\,4} + ( \, 3-2p \, )q^{\,2} + p -2 \, ) + 1 \,)
	}
	{
				q^{ \, 4p } - ( \, ( \, p-1 \, )^{\, 2} ( \, 1+q \, )^{\,4} - 2( \, p-2 \, ) pq^ {\, 2} \, ) q^{ \, 2p} + q^{\,4} 
	}
\end{align*}
\end{thm}

\four

\subsubsection{consequences}

\begin{itemize}

\item In the given range of energies, the critical points contributing to the complexity are either local minima or index-1 saddles. \\


\item ``This is quite surprising as the presence of the constraint was expected to lead to non-trivial correlations between critical points" \\


\end{itemize}


\newpage
\subsection{Tools} \hrulefill\\




\subsubsection{deterministic formula} 

Define the approximation to the identity $\delta_\eps : \R^N \to \R$  
\begin{align*}
\delta_\eps \equiv \frac{1}{|B(0,\eps)|} \1 \{ B(0,\eps)\},
\end{align*}
as a sequence ( in $\eps$) of averages over progressively smaller balls as $\eps \to 0$. 

\begin{lem} Let $K \subset \R^N$ be compact, and let $g : K \to \R^N$ be a smooth function. Let 
\begin{align}
Z(g) \tri \{ x \in K : g(x) = 0\}.
\end{align} 
Suppose that for each $x \in Z(g)$, $\det \nabla g(x) \neq 0$. If additionally, $Z(g) \cap \pa K = \emptyset$, then $\# Z(g)$ is finite, and
\begin{align}
\# Z(g) = \lim_{\eps \to 0} \int_K \delta_\eps (g(x) ) | \det \nabla g (x) | dx \,,
\end{align}
with the above equality holding for sufficiently small $\eps$. 
\end{lem}

\begin{proof} The non-degeneracy of $\nabla g$ ensures that critical points are isolated, and finitely many in $K$ because it is compact. It remains to show the formula for $\# Z(g)$ holds. Enumerate the elements of this set:
\begin{align*}
Z(g) = \{ x_1, \dots, x_m \} 
\end{align*}

Given $x_i \in Z(g)$, consider the map $g$ restricted to a small neighborhood of $x_i$. As $\det\nabla g (x_i) \neq 0$, it is also non-zero in a small neighborhood of $x_i$, and thus given $B(0,\eps) \subset \R^N$, for $\eps$ small enough, one has that 
\begin{align*}
g^{-1} ( B(0,\eps) ) = \bigcup_{i=1}^m U_i,
\end{align*}
where the $U_i$ are open, disjoint such that $x_i \in U_i$ and $g |_{U_i} : U_i \to B(0,\eps)$ is a homeomorphism. By writing $1$ in a silly way, and changing variables in the integral, we then have for each $x_i$, 
\begin{align}
1 &= \int_{B(0,\eps)} \delta_\eps (y) dy \\
&= \int_{U_i} \delta_\eps ( g(x) ) | \det \nabla g(x) | dx 
\end{align}
Summing over the $x_i$ completes the proof. \end{proof}

\four

\subsubsection{Kac-Rice in the Gaussian setting} The summary of how we get the next result: Fubini, and allow the $\eps \to 0$ limit to induce a kind of conditioning. 

\begin{thm}[Kac-Rice, Gaussian setting] Let $K \subset \R^N$ be compact and $B \subset \R$ be nice. Consider smooth Gaussian random fields $g: K \to \R^N$ and $f: K \to \R$. Write
\begin{align*}
Z ( g;f, B) =  \{ x \in K : g(x) =u, f(x) \in B \} 
\end{align*}
Then, 
\begin{align*}
\E \# Z(g ; f, B) = \int_K \E \left( | \det \nabla g(x)| \1 \{ f(x) \in B \}  | g(x) = 0 \right) \vp_x(0) dx \,,
\end{align*}
As before, $\vp_x(0)$ is the density of $g(x)$ at $0$. 
\end{thm}

\begin{proof} Let us give a sketch of the proof. Pointwise, we have 
\begin{align*}
\# Z (g ; f , B) = \lim_{\eps \to 0 } \int_K \delta_\eps(g(x)) | \det \nabla g(x) | \1 \{ f(x) \in B \} ,
\end{align*}
and we take expectations of both sides of this, freely interchanging limit and integral and expectation:
\begin{align*}
\E \# Z (g ; f , B) = \lim_{\eps \to 0 } \E \int_K \delta_\eps(g(x)) | \det \nabla g(x) | \1 \{ f(x) \in B \} dx,
\end{align*}
Now we are more explicit about what we are taking expectation in: there are three random objects: $f(x)$, $g(x)$ and $\nabla g(x)$, let us denote the joint density of these three objects by $p_x(u,v,W)$, where $u$ is the variable corresponding to the first object $f(x)$, $v$ corresponds to the vector $g(x)$ and $W$ corresponds to the matrix $\nabla g(x)$. Writing the expectation out as an integration against this density, one has
\begin{align*}
\E \# Z(g ; f, B) &= \int_K \int_{\textrm{sym}(N)}^{(W)} \int_\R^{(u)} \1 \{ u \in B \} | \det W | \\
&\quad\quad\quad \times \left(  \lim_{\eps \to 0}  \int_{\R^N}^{(v)} \delta_\eps (v) p_x(u,v,W) dv \right) du \,dW \,dx
\end{align*}
The limit within this inner-most integral becomes $p_x(u,0,W)$
\begin{align*}
\E \# Z(g ; f, B) &= \int_K \int_{\textrm{sym}(N)}^{(W)} \int_\R^{(u)} \1 \{ u \in B \} | \det W | p_x(u,0,W)  du \,dW \,dx \\
&= \int_K  \left( \E | \det \nabla g(x) | \1 \{  f(x) \in B \} \Big| g(x) = 0 \right) \vp_x(0) dx\,,
\end{align*}
where we have used that $p_x(u,W| v = 0) \vp_x(0) \equiv p_x(u,0,W)$. 
\end{proof}

\newpage
\subsection{ replicating Kac-Rice formula} \hrulefill\\

\subsubsection{applying Kac-Rice in the case $n = 1$}  Let $\un{\E}$ denote expectation taken with respect to the conditioned field $\un{h}$. Recall that we defined the sets $E_u$ and $I_q$ constraining energy and overlap respectively:
\eq{
\crit_N(u,q| u_0) \equiv \# \{ s \in \bbS : \un{\bm{g}}[s] = 0, \, \un{h}[s] \in E_u, \, \la s , \np \ra \in I_q \}
}
The Kac-Rice formula implies 
\eq{
\un{\E}  \crit_N(u,q | u_0) = \int_{\bbS}  \un{\E} \leftp | \det \un{\bm{\ch}}[s] | \1\{ \un{h}[s] \in E_u\} \1 \{ \la s , \np \ra \in I_q\} \big| \un{g}[s] = 0 \rightp \un{\vp}_s (0)
}
where $\un{\vp}_s(0)$ is the density function of the random variable $\un{\bm{g}}[s]$ evaluated at the vector of all zeroes (of length $N-1$). 

The integrand has three relevant pieces: 
\begin{itemize}
\item[(i)] The density accounts for the probability that a given $s \in \bbS$ is a critical point. 
\item[(ii)] The determinant term assigns an energy-dependent multiplicity. 
\item[(iii)] The indicator functions constraint the state space to a thin neighborhood of a latitude of the sphere. 
\end{itemize}

In contrast to the paper, we choose to constrain the energy $\un{h}[s]$ through an indicator function instead of conditioning on the precise value of this field. This simplifies the application of Kac-Rice (we can apply the specialized version above). 

The replicated version of this formula has the same three components. Most of the technical work of the paper seems to go into computing exact-enough expressions for these pieces in order to extract desired asymptotics. 

\four\four

\subsubsection{the replica trick} The replica trick is based on the following identity:
\eq{
\log A = \lim_{ x\to 0 } \frac{ A^x -1}{x} \,,
}
which follows from L'Hopital's rule. 

\four\four

\subsubsection{setting up the replica trick} 
Apply the replica trick to the random variable $\crit_N(u,q | u_0)$:
\eq{
\un{\E} \log \crit_N(u,q | u_0) = \lim_{n \to 0} \frac{ \un{\E} \crit_N(u,q | u_0 )^n -1 }{ n }
}
Thus, 
\eq{
\Sigma( u, q | u_0 ) &= \lim_{N \to \infty} \frac{1}{N} \un{\E} \log \crit_N(u,q|u_0) 
&= \lim_{N \to \infty} \lim_{n \to 0} \frac{ \un{\E} \crit (u, q | u_0 )^n - 1 }{Nn} .
}
We take these equalities as given, despite the lack of justification. We turn to computing the $n\oith$ moment of $\crit_N(u,q | u_0)$. We can express the $n\oith$ moment generically in terms of an $n$-tuple $\vec{s} = (s^1, \dots, s^n)$ of points on the $(N-1)$-dimensional unit sphere $\bbS$:
\eq{
\crit_N(u,q | u_0) ^n = \# \{ \vec{s} \in \bbS^n : \text{ for all $a$}, \, \un{\bm{g}}(s^a) = 0 ,\, \un{h}(s^a) \in E_u, \la s^a , \np \ra \in I_q \} ,
}
and we refer to the $s^a$ as \emph{replicas}. Extend $\un{h}$ to a function $\un{h}_{(n)} : \bbS^n \to \R$ in a trivial way, so that the restriction of $\un{h}_{(n)}$ to any coordinate sphere is just a copy of the original conditioned field $\un{h}$. The Kac-Rice formula applies to this Gaussian field also. Before writing this out, we introduce some notation. For each replica $s^a$, write $\un{\bm{g}}^a \equiv \un{\bm{g}}[s^a ]$, and $\un{\bm{\ch}}^a \equiv \un{\bm{\ch}}[s^a]$ for the spherical gradient of $\un{h}$ and the spherical Hessian respectively. 

Let 
\eq{
\vec{\un{\bm{g}}} = (\un{\bm{g}}^1, \dots, \un{\bm{g}}^n)
}
collect these spherical gradients into a single vector. Likewise, let 
\eq{ 
\vec{\un{h}} = (\un{h}^1, \dots, \un{h}^n)}
with $\un{h}^a \equiv \un{h} [\sig^a]$. Define 
\eq{
\Delta^{(n)} \tri \prod_{a = 1}^n \leftp | \det \un{\bm{\ch}}^a | \1 \{ \la s^a , \np \ra \in I_q \} \1\{ \un{h}^a \in E_u \} \rightp,
}
so that
\eq{
\un{\E}  \crit_N(u,q | u_0)^n = \int_{\bbS^n}  \un{\E} \left( \Delta^{(n)} \big| \vec{\un{\bm{g}}}(\vec{s}) = 0 \right) \vec{\un{\vp}}_{\vec{s}} (\vec{0})
}
is a direct consequence of Kac-Rice applied to the replicated landscape $\un{h}^{(n)}$. 

\four\four

\subsubsection{strategy} 
Vague: 
\begin{enumerate}
\item Get a nicer formula for $\E \crit_N^n$, treating $n$ as a positive integer. 
\item Use the saddle point method on this formula to evaluate the $N \to \infty$ limit (which involves another interchange of limits which we don't justify). 
\item This gives us a function $f(n)$ of $n$ only, and we try to extract something meaningful in the $n \to 0$ limit. 
\end{enumerate}

Thus, to apply the replica trick, we must understand the leading order in $N$ of $\un{\E} \crit_N(u,q | u_0)^n$. The product of determinants is central to this computation, so a first step is

The first step is to compute the joint law of the matrix elements of each Hessian. As discussed above, let us put all of the critical points on equal footing by not constraining the index at the north pole $\np$. To do the replica computation, we don't even look at the Hessian at $\np \equiv \sigma^0$. But the conditioning we start with, based on $\np$, is felt by each of the Hessians at each replica. \\

In order to compute, we should introduce a convenient basis within each tangent space $T_a \bbS \equiv T_{\sig^a} \bbS$. We will denote these well chosen bases by 
\eq{
\mathcal{B} [\sig^\alpha ] = \{ e_1^\al , \dots, e_{N-1}^\al \} .
}
The well-chosen bases will leverage symmetry of the model. The rotation invariance of the original field tells us that the covariance structure of the Hessian entries is a function of the scalar products
\eq{
e_i^\al \cdot e_j^\beta \text{ and } e_i^\al \cdot \sig^\beta\,,
}
and a convenient choice of each $\mathcal{B} [ \sig^\alpha ]$ allows us to write the joint distribution of these matrix elements as a function only of the mutual overlaps
\eq{
q_{\al\beta} \tri \sig^\al \cdot \sig^\beta,
}
and really we should write $q_{\al \beta}$, as the overlap of any replica with $\np$ is assumed to be close to $q$, the paper itself using equality. In other words, we will be able to write $M_n$ as 
\eq{
M_n = \int \prod_{a < b =1}^n d q _{a b} \exp \leftp N S_n ( \eps, \hat{Q} | \eps_0 )  + o(Nn) \rightp \,,
}
where $\hat{Q}$ is an $(n +1) \times (n+1)$ overlap matrix with components
\eq{
Q_{\al \beta} = \delta_{\al \beta} + (1- \delta_{\al \beta} ) [q_{\al \beta} + (\delta_{\al 0 } + \delta_{\beta 0} ) (q- q_{\al \beta}) ],
}
which is literally just an encoding of all of the ``mutual angles" just described. In the difference quotient involved in the replica trick, there are two limits being taken. As $N \to \infty$ in the outer limit, we were concerned with extracting the leading order in $N$ of the expression within this limit. But the $n \to 0$ limit suggests writing
\eq{
S_n(\eps, \hat{Q} | \eps_0 ) = n \Sigma( \eps, \hat{Q} | \eps_0 ) + O(n^2) \,.
}
There is a variational problem in the parameters $q_{ab}$ to be solved, in order to compute the desired asymptotics. To reduce the space of matrices over which this variational problem is phrased, we use a ``replica symmetric" ansatz $q_{ab} \equiv q_1$. We call this replica symmetric because in some sense, we are assuming that any sampling from the given lattitude of the sphere which is governed by the landscape has the property that with high probability, any two configurations sampled have overlap close to $q_1$. \\

As said in the paper, this ansatz corresponds to assuming that the landscape, conditioned on the north pole being a minimum (and here we close our eyes to the fact that this constrains the index of the critical point there), is 1-RSB in a neighborhood of the north pole. \\

\subsection{B. Covariances of fields and choice of basis vectors}

Let us first discuss how to effectively compute the laws of Riemannian (more specifically, the spherical) gradient and Hessians. The starting point for this is the field $h$, considered as a function on $\R^N$ (if we like, extended via homogeneity from its definition of $\bbS \subset \R^N$. \\

Let $\bm{\nabla} h^\al \tri \bm {\nabla} h[ \sig^\al]$ denote the Euclidean gradient of $h$, viewed as a function on $\R^N$ in this way. Likewise, let $\bm{\nabla}^2 h^\al \tri \bm{\nabla}^2 h[ \sig^\al]$ denote the  full Euclidean Hessian of $h$, so that it is an $N \times N$ matrix. \\

The spherical gradient is straightforward to compute: for each $\al$, one has
\eq{
\bm{g}^\al = \pi_\al ( \bm{\nabla} h^\al ) \,,
}
i.e. the spherical gradient at $\sig^\al$ is the projection of $\bm{\nabla}h^\al$ onto the tangent space $T_\al \bbS$, and this is what we take as our definition of the map $\pi_\al$.\\

{\red
The spherical Hessians $\bm{\ch}^\al$ can be computed from the gradient. The matrix elements of $\bm{\ch}_{\beta \gamma}^\al$ can be computed as follows:
\eq{
\bm{\ch}_{\beta \gamma}^\al = e_{\beta}^\al \cdot \leftp \bm{\nabla}^2 h^\al - ( \bm{\nabla} h^\al \cdot \sig^\al ) \matI \rightp \cdot e_\gamma^\al
}
}
Can I come back to the heuristic for this? I think it's not too complicated. Let us call the first computation a lemma. It gives us an efficient way of describing the matrix elements of any of the spherical Hessians with respect to any orthonormal basis. 

\begin{lem} Let $q_{\al \beta} = \sig^\al \cdot \sig^\beta$. For $v_1, v_2 \in \bbS$ arbitrary, one has the following covariance structure between the field and the gradient:
\eq{
\lla ( \bm{\nab} h^\al \cdot v_1 ) h^\beta \rra = p( q_{\al \beta} )^{p-1} (v_1 \cdot \sig^\beta ) \,,
}
and the following covariance structure between the field and the Hessian:
\eq{
\lla ( v_1 \bm{\nab}^2 h^\al \cdot v_2 ) h^\beta \rra = p(p-1) (q_{\al \beta} )^{p-2}  (v_1 \cdot \sigma^\beta) (v_2 \cdot \sigma^\beta) 
}
The covariances between gradient components are given by 
\eq{
\lla ( \bm{\nab} h^\al \cdot v_1 ) ( \bm{\nab} h^\beta \cdot v_2 ) \rra = p ( q_{\al \beta} )^{p-1} ( v_1 \cdot v_2) + p(p-1) q_{\al \beta}^{p-2} (v_2 \cdot \sig^\al) (v_1 \cdot \sig^\beta) 
}
the covariances between the Hessian entries are
\eq{
\lla (v_1 \cdot \bm{\nab}^2 h^\al \cdot v_2 ) ( v_3 \cdot \bm{\nab}^2 h^\al \cdot v_4 ) \rra &= \frac{p!}{(p-4)!} q_{\al \beta}^{p-4} (v_1 \cdot \sig^\beta) ( v_2 \cdot \sig^\beta) ( v_3\cdot \sig^\al) (v_4 \cdot \sig^\al) \\
&+ \frac{p!}{(p-3)!}  q_{\al \beta}^{p-4} (v_1 \cdot v_4) (v_2 \cdot \sig^\beta) (v_3 \cdot \sig^\al) \\
&+  \frac{p!}{(p-3)!} q_{\al \beta}^{p-3} (v_2 \cdot v_4) (v_2 \cdot \sig^\beta) ( v_3 \cdot \sig^\al ) \\
&+  \frac{p!}{(p-3)!} q_{\al \beta}^{p-3}  (v_1 \cdot v_3) (v_2 \cdot \sig^\beta) ( v_4 \cdot \sig^\al) \\
&+  \frac{p!}{(p-3)!} q_{\al \beta}^{p-3}  (v_2 \cdot v_3) (v_1 \cdot \sig^\beta) (v_4 \cdot \sig^\al) \\
&+  \frac{p!}{(p-2)!}  q_{\al \beta}^{p-2} ( (v_1 \cdot v_3) (v_2 \cdot v_4) + (v_1 \cdot v_4) (v_2 \cdot v_3) ) 
}
and lastly, the correlations between the Hessian and the gradient:
\eq{
\lla (v_1 \cdot \bm{\nab}^2 h^\al \cdot v_2) ( \bm{\nab}h^\beta \cdot v_3) \rra &= \frac{p!}{(p-3)!} q_{\al \beta}^{p-3} (v_1 \cdot \sig^\beta) (v_2 \cdot \sig^\beta) (v_3 \cdot \sig^\al) \\
&+ p(p-1) q_{\al \beta}^{p-2} (v_1\cdot v_3) (v_2 \cdot \sig^\beta) \\
&+ p(p-1) q_{\al \beta}^{p-2} (v_2 \cdot v_3) (v_1 \cdot \sig^\beta) 
}
\end{lem}

This can be cleaned up somewhat, but is a consequence of \cite[(5.5.4)]{AT}, just below\\

\begin{lem}%%%%%
\label{lem:cov_deriv}% lem:deriv
Let $f : T \to \R$ be a smooth, centered Gaussian field over the reasonable domain $T \subset \R^d$. Let $C(s,t)$ be the covariance associated to this field. Then $\pa_k f$ is a smooth gaussian $k$-tensor with covariance structure given by
\begin{align*}%%%
\E \left( \frac{ \pa_k f(s) }{ \pa s_{i_1} \dots \pa s_{i_k} } \frac{ \pa_k f(t) }{ \pa t_{i_1} \dots \pa t_{i_k} } \right) = \frac{ \pa _ {2k} C(s,t) } { \pa s_{i_1} \dots \pa s_{i_k}  \pa t_{i_1} \dots \pa t_{i_k} } \,.
\end{align*}%%%
\end{lem}%%%%%
 
 and 
 
 \begin{lem} %%%%%
\label{lem:condition}% lem:condition
Let $X = (X_1, X_2)$ be a gaussian vector where $X_1$ has length $n$ and where $X_2$ has length $d - n$. Let $m = (m_1, m_2)$ be the corresponding vector of means, and write the associated covariance matrix $\matC$ in block form, as
\begin{align*}%%%
\begin{bmatrix}
\matC_{11} & \matC_{12} \\ \matC_{21} & \matC_{22} \,.
\end{bmatrix}
\end{align*}%%%
The conditional law of $X_1$ given $X_2$ has mean $m_{1|2}$, with
\begin{align*}%%%
m_{1|2} = m_i + \matC_{12} \matC_{22}^{-1} (X_2 - m_2 ) \,,
\end{align*}%%%
and with covariance $C_{1|2}$ given by
\begin{align*}%%%
\matC_{1|2} = \matC_{11} - \matC_{12} \matC_{22}^{-1} \matC_{21} \,.
\end{align*}%%%
\end{lem}%%%%%

Let us see how specific we must be in order to proceed. As said above, we need to pick a good basis at each tangent space $T_a\bbS$. Let $S \subset \R^N$ be the span of the north pole and all the replicas:
\eq{
S = \span( \sig^0, \dots, \sig^n) \,,
}
We choose the bases $\mathcal{B}[\sig^\al]$ in each tangent plane so that the last $n$ basis vectors
\eq{
e_{N-n-1}^\al, \dots, e_{N-1}^\al 
}
along with the outward normal $\sig^\al$ together span $S$. Within each tangent space, we can then use the same orthonormal basis for $S^\perp$. The components of the gradients and Hessians along the first $M = N-1 - n$ directions in each tangent plane are uncorrelated with the energy fields of all replicas. They satisfy:
\eq{
\lla ( \bm{\nab} h^\al \cdot e_i )( \nab h^\beta \cdot e_j) \rra = p q_{\alpha \beta} ^{p-1} \delta_{ij}
}
and
\eq{
\lla ( e _ i \cdot \bm { \nab } ^ 2 h ^ \al e _ j ) 
	( e _ k  \cdot \nab ^ 2 h ^ \beta e_\ell ) 
		\rra 
			= p(p-1) q_{\alpha \beta}^{p-2} [ \delta_{ik} \delta_{j \ell} + \delta_{i \ell} \delta_{jk} ] 
}
(I think this last remark just completes the rosetta stone picture). The basis is not written down completely explicitly in my opinion, but the vectors are chosen and enumerated so that $e_{N-1}^a$ being the unique basis vector with non-zero overlap with $\sig_0$. 

\subsection{C. Statistics of the conditioned Hessians (I)}

We now discuss the statistics of the $n$ Hessian matrices $\bm{H}^a$ conditioned to the gradients $\bm{g}^\al = 0$ and to the appropriate energies. 

\section{Notes on a selection of the above problems}

%%%%
\newpage
\n\hrulefill
\vspace{2mm}
\subsection{comparison of ground state (energies) for soft spin and hard spin SK models (4.i)}
\begin{align*}
\text{--}
\end{align*}
\vspace{2mm}
%%%%

My only observation right now is that the spherical model is a RMT question, while the hard spin model has a ground state recoverable by the Parisi formula. This question raises another question, or a reduction of the above: is there a fine enough and perhaps $N$-dependent mesh one can put on the sphere so that the comparison of energies goes to zero, or that the inner product of the ground states goes to one. 


%%%%
\newpage
\n\hrulefill
\vspace{2mm}
\subsection{multipartite landscape complexity (starting with bipartite)}
\begin{align*}
\text{--}
\end{align*}
\vspace{2mm}
%%%%

In accordance with the rescaling in course notes, we want the field to live on the unit sphere, configurations $\sig \in S^{2N-1}(1)$. The function will be given by
\begin{align}
f(\sig) = \frac{1}{N} \sum_{i,j = 1}^N \sig_{1i} \sig_{2j} J_{ij} \,, 
\end{align}
with the $J_{ij}$ standard normal. We now want to do the analogous computation from the pure $p$-spin model. 

\newpage
\section{A zoo of learning models}

%%%%
\vspace{2mm}
\subsection{\red{linear and logistic regression}}
\begin{align*}
\text{--}
\end{align*}
\vspace{2mm}
%%%%



\subsubsection{linear regression} 

Suppose we have a set of data $D = (X,y)$, where $X$ is a collection of input vectors $(X^1, \dots, X^N)$, each living in $\R^d$, and where $y$ is a collection of corresponding labels, which I'll assume are one-dimensional. 

Initially, we suppose that the hypothesis space is the set of linear functions $h : \R^d \to \R$, each of which can be represented by taking the inner product with some vector in $\R^d$. This hypothesis space can be expanded to polynomials of higher degree while still falling under the purview of linear regression. I'm not committed to making explicit the hypothesis space in each example, but here it's easy enough to say. 

The error function we want to minimize in this case is the least-squares error. Given a hypothesis function $h$, the associated error $E(h)$ is defined by
\begin{align}
E(h) = \frac{1}{N}  \left( \sum_{i =1}^N (h(X^i) - y^i)^2 \right)^{1/2}\,,
\end{align}
and the best prediction we can make according to this measure of training error is given by
\begin{align}
h_* := \argmin_h E(h) \,. 
\end{align}
Note that in this particular hypothesis space, $h_*$ can be found using calculus. Then, $h_*$ can be used 


\subsubsection{logistic regression}

%%%%
\newpage
\n\hrulefill
\vspace{2mm}
\subsection{perceptrons}
\begin{align*}
\text{..}
\end{align*}
\vspace{2mm}
%%%%

\newpage
\subsubsection{the perceptron} 

\subsubsection{feed-forward neural net / restricted Boltzmann machine}

\subsubsection{student-teacher}

\subsubsection{Hopfield model}

\subsubsection{convolutional neural nets}

\subsubsection{variational auto-encoders}

\subsubsection{generative adversarial networks}


\newpage
.
\newpage
\section{ML journal entries}

%%%%
\n\hrulefill
\vspace{2mm}
\subsection{Bias-variance tradeoff}
\begin{align*}
\text{..}
\end{align*}
\vspace{2mm}
%%%%

We're following the exposition in \cite{mehta2018book}. Consider a set $D = (x,y)$ of classified data. Here $x$ is a collection of input vectors $(x_1, \dots, x_N)$ lying in some vector space $X$, and true data is generated from a noisy model:
\begin{align}
y_i = f(x_i) + \eps_i \,,
\end{align}
where $f$ is the unknown function we want to learn, and where $(\eps_i)$ is a collection of i.i.d. centered gaussians with variance $\sig_\eps^2$.  We have a space of hypothesis functions $(g_\theta)$ for $f$ indexed by a parameter vector $\theta$. We suppose that we're using the least-squares procedure for choosing the ``best" $\theta$: we define the following cost function, which depends on $\theta$ and the data set $D$.
\begin{align}
C(y, g_\theta(x)) := \sum_i (y_i - g_\theta(x_i))^2 \,;
\end{align}
the best choice of $\theta$ according to this cost function is 
\begin{align}
\wh{\theta} = \argmin_\theta C(y, g_\theta(x)) \,,
\end{align}
and we write $g_\theta$ corresponding to $\theta = \wh{\theta}$ as $\wh{g}$. At this point, it becomes relevant that $X$ is actually equipped with a probability measure (something that hasn't yet been emphasized in \cite{mehta2018book} or the corresponding first python notebook), from which the data points $x_1, \dots, x_N$ are drawn. As $\wh{\theta}$ depends both on the drawn data $x$ and the realization of the noise $(\eps_i)$, we can thus think of $\wh{\theta}$ as a random variable, and in turn, $\wh{g}$ as a random function. 

Let $\E_x$ denote expectation with respect to the law on $X$ (drawn from independently $N$ times), and let $\E_\eps$ denote expectation with respect to the noise variables. Let $\E$ be expectation with respect to either. We now do a computation:
\begin{align}
\E [ C(y, \wh{g}(x) ) ] &= \E \left( \sum_i (y_i - \wh{g}(x_i))^2 \right) \,, \\
&= \E \left(  \sum_i (y_i - f(x_i) + f(x_i) - \wh{g}(x_i))^2 \right) \,, \\
&= \sum_i \E (y_i - f(x_i))^2 + \E ( f(x_i) - \wh{g}(x_i))^2 -2 \E (y_i - f(x_i))(f(x_i) - \wh{g}(x_i)) \,.
\end{align}
The last term above vanishes because conditional on $x_i$, $(y_i - f(x_i))$ depends only on the noise, and is mean zero, while $f(x_i) - \wh{g}(x_i))$ depends only on $x$. We can thus integrate this term in the noise first to get zero. Thus,
\begin{align}
\E [ C(y, \wh{g}(x) ) ] &= \sum_i \sig_\eps^2 + \E ( f(x_i) - \wh{g}(x_i))^2 \,.
\end{align}
We choose to decompose the last term in the sum even further. The text suggests that in the second term above, that the expectation should only be taken in $x$, but this doesn't seem to be right: not only have we drawn the data, but we've formed $\wh{g}$, which \emph{depends} both on the drawn input data and the noise. Let's perform the $\E_x$ integration first, thinking of conditioning on the noise. 
\begin{align}
\E_x  ( f(x_i) - \wh{g}(x_i))^2 &= \E_x  ( f(x_i) - \E_x \wh{g}(x_i) + \E_x \wh{g}(x_i) - \wh{g}(x_i))^2 \,, \\
&= \E_x ( f(x_i) - \E \wh{g}(x_i))^2 + \E_x ( \E_x \wh{g}(x_i) - \wh{g}(x_i))^2 + 2 \E_x ( f(x_i) - \E_x \wh{g}(x_i) )( \E_x \wh{g}(x_i) - \wh{g}(x_i) ) \,.
\end{align}
I think the last term is supposed to vanish, but I don't see that it does: I get
\begin{align}
\E_x  ( f(x_i) - \wh{g}(x_i))^2  =  \E_x ( f(x_i) - \E \wh{g}(x_i))^2 + \E_x ( \E_x \wh{g}(x_i) - \wh{g}(x_i))^2 + 2 \Cov_{x,i} (f, \wh{g}) \,,
\end{align}
where $\Cov_{x,i}(f,\wh{g})$ is $\E_x [ f(x_i) \wh{g}(x_i) ] - \E_x f(x_i) \E_x \wh{g}(x_i)$. Let us now integrate everything over the noise, writing $\E_\eps \Cov_{x,i}$ as $\Cov_i$. We find:
\begin{align}
\E [ C(y, \wh{g}(x) ) ] = \sum_i \left( \sig_\eps^2 \E ( f(x_i) - \E_x \wh{g}(x_i))^2 + \E ( \E_x \wh{g}(x_i) - \wh{g}(x_i))^2 + 2 \Cov_{i} (f, \wh{g})  \right)
\end{align} 
Write
\begin{align}
\text{bias}^2 &= \sum_i \E ( f(x_i) - \E_x \wh{g}(x_i))^2 \,, \\
\text{variance} &= \sum_i  \E ( \E_x \wh{g}(x_i) - \wh{g}(x_i))^2 \,,\\ 
\text{covariance} &= \sum_i 2 \Cov_i (f, \wh{g}) 
\end{align}
I think we are supposed to believe that the covariance term is close to zero. Thus,
\begin{align}
E_{out} \equiv \E [ C(y, \wh{g}(x) ) ] \approx \text{bias}^2 + \text{variance} + \text{noise} \,,
\end{align}
where $\text{noise} = \sum_i \sig_\eps^2$. 



%%%%
\newpage
\n\hrulefill
\vspace{2mm}
\subsection{gradient descent}
\begin{align*}
\text{..}
\end{align*}
\vspace{2mm}
%%%%
I'm following \cite{mehta2018book}, probably also worth looking at Ruder's overview \cite{ruder2019overview} on the one hand, and for a beginning mathematical treatment, Hirsch-Smale-Devaney Section 9.3 is a start (but honestly gives very little).  From the iPython notebook:

``From the onset, we emphasize that doing gradient descent on the surfaces is different from performing gradient descent on a loss function in Machine Learning (ML). The reason is that in ML not only do we want to find good minima, we want to find good minima that generalize well to new data."

\newpage
\section{entropy}

% \emph{This can be expanded in so many directions. In addition to talking more about what's written below, one can write about large deviations, Markov chains, Gaussian processes, concentration}

%%%%
\n\hrulefill
\vspace{2mm}
\subsection{basics}
\begin{align*}
\text{..}
\end{align*}
\vspace{2mm}
%%%%




\subsubsection{Shannon entropy and KL divergence} We follow \cite{mezard2009information} Chapter 1. The first definition that we start with is the Shannon entropy of a discrete random variable.\\

\begin{shadowbox}
\begin{defn} Given a discrete random variable $X$ taking values in a space $\mathcal{X}$, the \emph{entropy} of $X$ is defined by
\begin{align}
H_X := - \sum_{x \in \mathcal{X}} p(x) \log_2 p(x) \,,
\end{align}
where $p$ is the probability mass function of $X$. 
\end{defn}
\end{shadowbox}

\four

There is also a notion of relative entropy of one probability measure with respect to another. This notion seems important both in physics and statistics.\\

\begin{shadowbox}
\begin{defn}
Given two discrete random variables taking values in $\mathcal{X}$ with mass functions $p,q$, the \emph{relative entropy} or \emph{Kullback-Leibler divergence} is 
\begin{align}
D ( q \| p ) := \sum_{x \in \mathcal{X}} q(x) \log \left( \frac{q(x)}{p(x)} \right) \,.
\end{align}
\end{defn}
\end{shadowbox}

\four

This can be thought of as a distance, despite its lack of symmetry. I wonder if there is an inequality relating KL divergence to the TV distance between measures. When $p$ is uniform, the KL divergence of $q$ relative to $p$ is, up to a minus sign and positive constant, the entropy of $q$. We record some useful properties as a lemma:\\

\begin{shadowbox}
\begin{lem} Let $X$ and $Y$ be discrete random variables. The following hold:
\begin{enumerate}
\item $H_X \geq 0$, and $H_X = 0$ if and only if the random variable $X$ takes a single value with probability one.
\item Among all probability measures on $\mathcal{X}$, with $| \mathcal{X} | = M$, the law with maximal entropy is uniform.
\item If $X$ and $Y$ are independent, then
\begin{align*}
H_{X,Y} = H_X + H_Y 
\end{align*}
\item Given any pair of random variables $X,Y$, we have $H_{X,Y} \leq H_X + H_Y$. 
\item Write $\mathcal{X} = \mathcal{X}_1 \sqcup \mathcal{X}_2$. Let $X$ be a random variable giving $\mathcal{X}_1$ mass $q_1$, and likewise for $\mathcal{X}_2$ and $q_2$. For $x \in \mathcal{X}_1$, let $r_1(x) = p(x) / q_1$ be the conditional probability of selecting $x$, and for $x \in \mathcal{X}_2$, let $r_2(x) = p(x) / q_2$ be the conditional probability of selecting this $x$. The entropy admits a kind of chain rule:
\begin{align*}
H_X = H(q) + \wt{H}(q,r) \,,
\end{align*}
where $H(q)$ is the entropy of the two-value random variable with masses $q_1, q_2$, and where 
\begin{align*}
\wt{H}(q,r) = - q_1 \sum_{x \in \mathcal{X}_1} r_1(x) \log_2 r_1(x) - q_2 \sum_{x \in \mathcal{X}_2} r_2(x) \log_2 r_2(x)\,,.
\end{align*}
\end{enumerate}
\end{lem}
\end{shadowbox}

\four

\begin{shadowbox}
\begin{rmk} The above properties (with maybe a bit extra) can be used to define entropy axiomatically. 
\end{rmk}
\end{shadowbox}

\four 

We'll now introduce the entropy rate of a sequence of random variables. Good examples of sequences of random variables are (1) i.i.d. sequences (2) Markov chains. Recall that \\

\begin{shadowbox}
\begin{defn} A sequence $(X_n)$ of random variables is a \emph{Markov chain} if 
\begin{align*}
P_N(x_1, \dots, x_N) = p_1(x_1) \prod_{n=1}^{N-1} w( x_n \to x_{n+1} ) \,,
\end{align*}
where $p_1$ is the measure determining the initial state, and where the collection $( w(x \to y ))_{x,y \in \mathcal{X}}$ are the transition probabilities. 
\end{defn}
\end{shadowbox}

\four

\begin{shadowbox}
\begin{defn} Given a sequence $(X_n)$ of random variables, the \emph{entropy rate} of the sequence is defined by the following limit, when it exists:
\begin{align*}
h_X = \lim_{N \to \infty} H_{X_N} / N \,.
\end{align*}
\end{defn}
\end{shadowbox}

\four

For i.i.d. sequences, the entropy rate is just the entropy of the single random variable that the sequence is modeled after. For Markov chains, supposing the existence of a stationary measure
\begin{align}
p^*(x) = \lim_{n \to \infty} p_n(x) \,,
\end{align}
where $p_n(x)$ is the marginal of the $n$th variable in the chain, the entropy rate of the chain $X$ has the following expression:
\begin{align}
h_X = - \sum_{x,y \in \mathcal{X}} p^*(x) w(x \to y) \log w(x \to y) \,. 
\end{align}

I guess this can be derived in most cases. Moving back to pairs of random variables, we now define conditional entropy and related concepts.\\

\begin{shadowbox} 
\begin{defn}
Given random variables $X,Y$, define the \emph{conditional entropy} $H_{Y|X}$ as 
\begin{align*}
H_{Y|X} := - \sum_{x \in \mathcal{X}} p(x) \sum_{y \in \mathcal{Y}} p(y|x) \log p(y |x) \,.
\end{align*}
\end{defn}
\end{shadowbox}

\four

This is somewhat interesting: the entropy rate of a Markov chain looks like the conditional entropy of the transition kernel given the stationary measure. 

We can also define the joint entropy of the pair $(X,Y)$ in a natural way:
\begin{align}
H_{X,Y} = - \sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x,y) \log p(x,y) \,,
\end{align}
an entropy which takes into account the correlations between $X$ and $Y$. We have the following identity, which is called the \emph{chain rule}:
\begin{align}
H_{X,Y} = H_X + H_{Y|X} \,. 
\end{align}
This is basically a one-line computation. It becomes relevant to also define the \emph{mutual information} $I_{X,Y}$:
\begin{align}
I_{X,Y} := \sum_{x,y} p(x,y) \log \frac{ p(x,y) }{p(x) p(y) } \,. 
\end{align}
This is just the KL divergence of the pair $p(x,y), p(x)p(y)$. We know it is zero exactly when $X$ and $Y$ are independent. Note that this is symmetric in $X$ and $Y$ (so the KL divergence can be used to easily produce something symmetric, we just needed to plug in the right things). Moreover,
\begin{align}
I_{X,Y} = H_Y - H_{Y|X} = H_X = H_{X|Y} \,. 
\end{align}
Thus, if $H_{Y|X}$ is strictly smaller than $H_Y$, this means that knowing $X$ has given us information about $Y$, and the difference $H_Y - H_{Y|X}$ quantifies this. 

\subsubsection{thermodynamics} In stat. mech., we have a partition function $Z(\beta)$ which is defined from the Gibbs / Boltzmann law $\mu_\beta$ associated to some energy function $H(x)$. The \emph{free energy} is then defined via
\begin{align}
F(\beta) = - \frac{1}{\beta} \log Z(\beta)\,,
\end{align}
and from this, the \emph{internal energy} and the \emph{canonical entropy} are defined in terms of derivatives of the free energy:
\begin{align}
U(\beta) &= \pa_\beta ( \beta F(\beta)) \,, \\
S(\beta) &= \beta^2 \pa_\beta F(\beta) \,. 
\end{align}
These are thermodynamic definitions, in that they make sense given any $F$, not just one defined in terms of the partition function. When we specialize to the above stat. mech. definition of $F$, we find that $U$ and $S$ are natural objects: $U = \la H(x) \ra$, and $S$ is exactly the Shannon entropy of $\mu_\beta$. Moreover, we get the thermodynamic formula $F = U - \beta^{-1} S$. 




\newpage
\section{Belief propagation: factor graphs, the cavity method and TAP}

%%%
\newpage
\subsection{factor graphs} Following Mezard-Montanari: consider a set of $N$ variables $x_1, \dots, x_N$ taking values in a finite alphabet $\mathcal{X}$. Suppose that their joint probability distribution has the following form:
\begin{align}
P(x) = \frac{1}{Z} \prod_{a = 1}^M \vp_a( x_{\pa a} ) \,,
\label{eq:factor_1}
\end{align}
where for $a \in [M]$, $\pa a \subset [N]$ with size $k_a$. We explicitly write $\{i_1^a, \dots, i_{k_a}^a\}$ for $\pa a$. The functions $\vp_a : \mathcal{X}^k \to \R$ are called \emph{compatibility functions}. 

A \emph{factor graph} is a way to represent a probability distribution of the form \eqref{eq:factor_1}. Such a probability distribution is called an \emph{undirected graphical model}. The factor graph has two types of nodes: $N$ \emph{variable nodes} and $M$ \emph{function nodes}. We assume $| \pa a| \geq 1$ for any factor node $a$. 

Undirected graphical models enjoy a \emph{global Markov property}.

\begin{prop} Let $A,B,S \subset[ N]$ be three disjoint subsets of variable nodes, and let $x_A, x_B, x_S$ be the corresponding sets of variables. If $S$ separates $A$ and $B$ in the factor graph, i.e. any path from $A$ to $B$ uses a vertex of $S$, then 
\begin{align}
\prob( x_A, x_B | x_S ) = \prob(x_A | x_S ) \prob(x_B | x_S) 
\end{align}
\end{prop}

%%%
\newpage



\subsection{aspects of REM}

\subsection{aspects of GREM}

\subsection{error correcting codes}

\section{Gaussian processes} 

\subsection{Girsanov}

\subsection{Feynman-Kac}

\subsection{Kac-Rice} 

\subsection{Slepian} 

\subsection{Gaussian Hilbert spaces}

\subsection{Graphons}


\newpage


\section{..}

%%%
\newpage
\subsection{dynamical phase transition} Let's start by looking at \cite{derrida1989dynamical}. Here, instead of through the restricted partition function, Derrida defines a dynamical phase transition in the following way: suppose we have two starting configurations in $\{\pm1\}^N$ for dynamics. We then consider dynamics (heat bath for instance) whose stationary measure is the disordered Gibbs measure. We run the dynamics with the two configurations as initial. 

There are supposedly three outcomes. For some $T_1$, when $T > T_1$, the distance between configurations vanishes in the long-time limit. There is another $T_2 < T_1$ such that for $T_1 > T > T_2$, the distance between configurations has a non-zero limit not depending on the starting configurations. Below $T_2$, the distance has a non-zero limit which does depend on the starting configurations. My understanding is that $T_2$ should coincide with the static spin glass transition, so that $T_1$ is associated to the dynamical phase transition.

This reminds me of thinking I did about a problem Boris brought up. He wanted to run two starting configurations through a neural network and track their distance as they were propagated through the network. Here, being pushed through to the next layer seems the analogue of a single step of the dynamics. Again, I come to a question I've thought about before: \emph{what is the analogue of temperature for this model, or for neural nets in general??}  This seems like a distinct neural net perspective: instead of tuning the weights using SGD, which is thought of as analogous to Langevin dynamics on a spin glass energy landscape, we're treating a neural net with (probably) quenched weights as an intrinsically dynamical object. In the Langevin $\leftrightarrow$ SGD perspective, I think temperature has a natural interpretation in the latter as step size? But in the above, less intuitive perspective, what is the analogue of temperature?

%%%
\newpage
\subsection{Parisi functional computations} I think it's worthwhile to revisit this. We state the version for the SK model. Let $Z_N$ be the corresponding partition function. 
\begin{align}
\frac{1}{N} \log Z_N(\beta) \to \inf_{\mu \in M[0,1] } \left\{ \log 2 + \Phi_\mu(0,0) - \frac{\beta^2}{2} \int_0^1 \mu[0,s] s ds \right\} 
\end{align}
Here $\Phi_\mu$ solves the measure-dependent PDE
\begin{align}
\begin{cases}
\vp_t = - \left( \vp_{xx} + \mu[0,t] (\vp_x)^2 \right) \,, \\
\vp(1,x) = \log \cosh (\beta x) 
\end{cases}
\end{align} 
We first do computations for the SK model with very simple measures. We should try to see if we can get Guerra's upper bound for general atomic measures. Then we should compute for the \emph{bipartite} SK model.\\

\subsubsection{SK, $\mu = \delta_0$:} Note that the above PDE reduces to
\begin{align}
\vp_t = - ( \vp_{xx} + (\vp_x)^2) \,,
\end{align}
which can be solved via Hopf-Cole: define $u(t,x) := \exp( \vp(t,x))$. Then for any solution $\vp$ of the above, we find $u$ satisfies $u_t = - u_{xx}$. The final condition $\vp(1,x) = \log \cosh( \beta x)$ becomes $u(1,x) = \cosh(\beta x)$. The solution is then 
\begin{align}
u(t,x) = \E_x \cosh(\beta B_{1-t})  \equiv \E \cosh (\beta (x + \sqrt{1-t}X))\,,
\end{align}
where $B_{s}$ is a Brownian motion and $X$ is a standard normal. Thus,
\begin{align}
\vp(t,x) = \log \E \cosh(\beta (x + \sqrt{1-t}X ) ) \,,
\end{align}
and we can quickly compute $\vp(0,0)= \beta^2 / 2$. The integral term in the Parisi functional becomes $\beta^2 / 4$, and using Guerra's upper bound, we find
\begin{align}
N^{-1} \log Z_N(\beta) \leq \log 2 + \beta^2 /4 \,,
\end{align}
which is the REM bound. This is the replica symmetric bound we should get for $\mu = \delta_0$, and this shows that the constants and $\beta$'s above are properly calibrated for the SK model. \\

\begin{rmk} We can perform an annealed computation on the bipartite SK model to see what the replica symmetric bound is. 
\end{rmk}

\subsubsection{Endpoints of Guerra interpolation} We follow Guerra's notation now. So $\mu$ is now $x$, $t$ is now $q$ and $x$ is now $y$. 

For $x \equiv \delta_0$, we have $q_0 = 0, q_1 = 0$ and $q_2 = 1$. We have $m_2 = 1$, and $m_1$ does not matter. We now define the interpolated hamiltonian:
\begin{align}
H_t = \frac{ \sqrt{t}}{\sqrt{N}} \sum_{ij} J_{ij} \sig_i \sig_j + \sqrt{1-t} \sum_{a = 1}^2 \sqrt{q_a -q_{a-1}} \sum_i J_i^a \sig_i 
\end{align}
Note that $q_1 -q_0 = 0$ and $q_2 - q_1 = 1$, thus
\begin{align}
H_t = \frac{ \sqrt{t}}{\sqrt{N}} \sum_{ij} J_{ij} \sig_i \sig_j + \sqrt{1-t} \sum_i J_i^2 \sig_i  \,,
\end{align}
where $J_i^2$ is a collection of iid standard normals (the two is a superscript, not a square). We'll let $\E_2$ and $\E_1$ be averages over the $J_i^2$ and $J_i^1$ disorder, the latter being vacuous. $\E_0$ shall be an average over the $J_{ij}$, and $\E$ denotes an average over all disorder. The partition function $Z_t$ is $\sum_\sig \exp(\beta H_t)$. Write $Z_2$ for $Z_t$. Based on Guerra's notation, we define $Z_1^{m_2} = \E_2 Z_2^{m_2}$, thus $Z_1 = \E_2 Z_2$. No matter what $m_1$ is, the $\E_1$-average is vacuous, so $Z_0 = Z_1$. The function of $t$ which will interpolate between the free energy and the desired bound is defined (in this simple case) by
\begin{align}
\al_N(t) := \frac{1}{N} \E_0 \log \E_2 Z_t
\end{align}
One easily sees that $\al_N(0) = N^{-1} \E \log Z_N$, the free energy of the original hamiltonian at scale $N$. On the other hand,
\begin{align}
\al_N(1) = N^{-1} \log \E \left( \sum_\sig \exp \left( \beta \sum_i J_i \sig_i \right) \right)\,,
\end{align}
where the $J_i$ are iid standard normal. Let's see if we can show this is the REM bound. First, observe that 
\begin{align}
\sum_\sig \exp \left( \beta \sum_i J_i \sig_i \right) = \prod_i 2 \cosh(\beta J_i) 
\end{align}
For each $J_i$, note that $\E 2 \cosh (\beta J_i) = 2 \exp(\beta^2 /2)$. This gives 
\begin{align}
\al_N(1) = \log 2 + \beta^2 /2 
\end{align}
{\red{\emph{What happened to the $-\beta^2 /4$ term??}}}

\subsubsection{Derivative of $\al_N(t)$} Trying to compute this derivative without some forethought is actually somewhat difficult. It's better to be organized: we'll integrate by parts but we need to introduce functions which help us tilt Gibbs averages appropriately. To illustrate this, note that 
\begin{align}
\pa_t \al_N(t) = \frac{1}{N} \E_0 \left( \frac{ \E_2 \pa_t Z_t }{ \E_2 Z_t } \right)\,.
\end{align}
Computing a bit further, we find:
\begin{align}
\pa_t Z_t = Z_t \left( \frac{\beta}{2} \frac{1}{\sqrt{tN}} \sum_{ij} \la J_{ij} \sig_i \sig_j \ra_t - \frac{\beta}{2} \frac{1}{\sqrt{1-t}} \sum_i \la J_i \sig_i \ra_t \right)
\end{align}
We would like to apply Gaussian integration by parts on $\E_2 \la J_{ij} \sig_i \sig_j \ra_t$ and $\E_2 \la J_i \sig_i \ra$ for instance, but the $Z_t$ in front of this prevents us from doing so. Taking into account the denominator, we have a kind of martingale term sitting in front of the "good" expression. This is computational motivation for introducing the following for $a = 1,2$:
\begin{align}
f_a := \frac{Z_a^{m_a}}{\E_a \Z_a^{m_a}} \,,
\end{align}
which is only non-trivial for $a =2$. In this case, $f_a = Z_t / \E_2 Z_t$, the problematic term appearing above. In the case $a = 1$, we have $f_1 \equiv 1$. 

We need more notation: let $\omega(\cdot)$ be the measure / state associated to the hamiltonian $H_t$. Write $\wt{\omega}_2 (\cdot) = \omega(\cdot)$. Then, write
\begin{align}
\wt{\omega}_1 (\cdot) &= \E_2 ( f_2 \omega(\cdot)) \\
\wt{\omega}_0 (\cdot) &= \E_1 \E_2( f_1 f_2 \omega(\cdot)) \equiv \E_2 (f_2 \omega(\cdot)) \,,
\end{align}
in fact, in our replica symmetric case, we have $\wt{\omega}_1 \equiv \wt{\omega}_0$. Finally, define the averages $\la \cdot \ra_a$ in general by
\begin{align}
\la \cdot \ra_a = \E ( f_1 \dots f_a \wt{\Omega}_a ) \,,
\end{align} 
where $\wt{\Omega}_a$ denotes the replicated version of the above measures. Let's see what this reduces to in our case (it will be easier to imagine we're taking an average only over one replica):
\begin{align}
\la \cdot \ra_0 &= \E \wt{\omega}_0(\cdot) = \E \E_2 ( f_2 \omega(\cdot)) \,, \\
\la \cdot \ra_1 &= \E f_1 \wt{\omega}_1 = \E f_1 \E_2 ( f_2 \omega(\cdot)) \equiv \E \E_2 (f_2 \omega(\cdot) ) \,, \\
\la \cdot \ra_2 &= \E f_1 f_2 \wt{\omega}_2(\cdot) = \E f_1 f_2 \omega(\cdot)  \,.
\end{align}
Thus, just as $\wt{\omega}_1 \equiv \wt{\omega}_0$, we have $\la \cdot \ra_ 0 \equiv \la \cdot \ra_1$, and in fact $\la \cdot \ra_2$ is the \emph{same}. Each of these averages represents the average of $\la \cdot \ra_t$ tilted by the martingale factor $f_2$. I suspect that in generality, $\la \cdot \ra_0$ and $\la \cdot \ra_K$ interpolate (discretely, not related to the parameter $t$) between a sequence of tiltings of $\omega(\cdot)$, in which an average of the tilting is taken part of the way through. 

Guerra comments that ``the averages $\la \cdot \ra_a$ are able, in a sense, to concentrate the overlap fluctuations around the value $q_a$." Thus $\la \cdot \ra_0$ and $\la \cdot \ra_1$ should concentrate overlap averages around $q_0$ and $q_1$, which are both zero. We should hypothetically have that $\la \cdot \ra_2$ concentrates overlaps around $q_2 = 1$, but I think the reason this doesn't happen is that we can choose $m_1 = 1$, so that $m_2 - m_1 = 0$. This is a gut feeling more than a rigorous statement. I suspect that all of these measures are the same because we're in the replica symmetric setting. 

Returning to our expression for $\pa_t \al_N(t)$, we have
\begin{align}
\pa_t \al_N(t) = \frac{1}{N} \left( \frac{\beta}{2} \frac{1}{\sqrt{tN}} \sum_{ij} \la J_{ij} \sig_i \sig_j \ra_a - \frac{\beta}{2} \frac{1}{\sqrt{1-t}} \sum_i \la J_i \sig_i \ra_a \right) \,, 
\end{align}
for $a = 0,1,2$ as they are all equivalent. We remark that we've already verified Lemma 2 in Guerra's paper in our simple setting:
\begin{align}
\pa_t \al_N(t) = \frac{1}{N} \E (f_1 \dots f_K Z_K^{-1} \pa_t Z_K ) \equiv \frac{1}{N} \E f_2  
\end{align}
Returning to the equation just before the above, we can now use IBP on the terms $\la J_{ij} \sig_i \sig_j \ra_a$ and $\la J_i \sig_i \ra_a$. Setting $a = 0$ for concreteness,
\begin{align}
\la J_{ij} \sig_i \sig_j \ra_0 &= \E( J_{ij} f_1 f_2 \omega(\sig_i \sig_j) ) \,,\\
\la J_i \sig_i \ra_0 &= \E ( J_i f_1 f_2 \omega(\sig_i) )
\end{align}
In the first equation above (for instance), we can treat $f_1, f_2$ and $\omega$ as functions firstly of the disorder variable $J_{ij}$. This is what allows us to do IBP. Then we take an average with respect to the rest of the disorder. The first equation then becomes
\begin{align}
\E( J_{ij} f_1 f_2 \omega(\sig_i \sig_j) ) &= \E \pa_{J_{ij}} f_1 f_2 \om(\sig_i\sig_j) + \E f_1 \pa_{J_{ij}} f_2 \om (\sig_i \sig_j) + \E f_1 f_2 \pa_{J_{ij}} \om(\sig_i \sig_j) \,, \\
&= \E  \pa_{J_{ij}} f_2 \om (\sig_i \sig_j) + \E  f_2 \pa_{J_{ij}} \om(\sig_i \sig_j)
\end{align}

%%%%%%
\newpage
\section{video lectures}



\subsubsection{From reinforcement learning to spin glasses: the many surprises in quantum state preparation} 

\url{https://www.youtube.com/watch?v=RcGJCLQRmAk} \,,\\ 

based on:

\url{http://physics.bu.edu/~pankajm/Papers/Glassycontrol.pdf}

\url{http://physics.bu.edu/~pankajm/Papers/BrokenSymmetry.pdf}

This talk concerns quantum control problems: we have a magnetic field that we can tune / play with over time. We start with a physical system in a certain quantum state, and we have a target state that we want to get our system close to. We give ourselves a certain amount of time $T$ to do this. They decided to attack this using ML. 

This parameter $T$ ends up leading to at least one phase transition determined by the ``control landscape."

Another takeaway: the following ML review aimed at physicists:
\url{https://arxiv.org/pdf/1803.08823.pdf} 


\newpage
\section{Mehta book}

\subsection{I} Let $x$ be an observable quantity in some system. Consider a family of probability distributions $p(\cdot | \theta)$ indexed by a parameter (perhaps multidimensional) $\theta$. After observing data $X$, we then want to find an estimate $\hat{\theta}$ for $\theta$, which is unknown to us. \emph{Estimation} problems concern the accuracy of $\hat{\theta}$, while \emph{prediction} problems concern the ability of the model to predict new observations using $p(\cdot | \hat{\theta})$. 

\subsection{II} Here is the setup (for supervised learning). We have a data set $\mathcal{D} = (X,y)$. Here $X$ is a matrix of ``independent" variables and $y$ is a vector of ``dependent" variables. Each row of $X$ is a single instance of input data. The corresponding "row" in $y$ is a number which represents where or how the data is being classified. 

We then consider a model $f(x | \theta)$ which is a family of functions from the input space to the output space indexed by the parameters $\theta$. The goal is to estimate $\theta$.

Finally, we need a cost function $\mathcal{C}( y, f(X;\theta))$ which determines what metric we're using to estimate $\theta$. This is the setup.

The first step in the analysis (and they are careful to emphasize this) is to \emph{randomly} subdivide the data $\mathcal{D}$ into two classes: training data and test data, $\mathcal{D} = (\mathcal{D}_{train}, \mathcal{D}_{test})$. Apparently this is called \emph{cross-validation}. Typically the training set is taken to be much larger. Then (through an algorithm of some kind), we find $\hat{\theta}$ via
\begin{align}
\hat{\theta} = \argmin_\theta \{ \mathcal{C} ( y_{train} , f(X_{train};\theta) \} 
\end{align}
The cost function for $\hat{\theta}$ and $\mathcal{D}_{train}$ is called the \emph{in-sample error}, denoted $E_{in}$, while the cost function for the same paramter $\hat{\theta}$ using the test data $\mathcal{D}_{test}$ is called the \emph{out-sample error}, denoted $E_{out}$.

\begin{rmk} Typically $E_{out} \geq E_{in}$, but here is the surprise: it seems that the model producing the lowest $E_{out}$ usually doesn't have the lowest $E_{in}$.  \emph{What's the reason for this lack of monotonicity??}
\end{rmk}

\subsection{III} We want to say something meaningful about the relationship between $E_{in}$ and $E_{out}$. 


\newpage
\section{M\'{e}zard-Montanari} 

\end{comment}

%%%%
% BIB 
%%%%
\bibliographystyle{abbrv}
\bibliography{glass}
\nocite{*}


%%% FIN
\end{document}









