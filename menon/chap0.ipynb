{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZuXYfmJPu3l"
   },
   "source": [
    "# **0 ... Prep**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tejUn-iSkHnq"
   },
   "source": [
    "sources: \n",
    "\n",
    "* [Menon 2020](http://www.dam.brown.edu/people/menon/publications/pt-2020.pdf) : italic paragraphs below denote text copied verbatim, but then I got lazy. Almost all text from these notes, am just filling out \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8nqL70nX2d4"
   },
   "source": [
    "## **0.1 ... Markov chains**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfZy_Ow5kHnr"
   },
   "source": [
    "_We assume that the world is a space of random signals. An observer records part of these signals and forms inferences about the world. The process of inference is modeled with a Bayesian paradigm. We assume the observer has a stochastic model that is capable of generating signals that are similar to the true signal. Inference then reduces to tuning the parameters of the model to obtain the best match between the generated and observed signals._\n",
    "\n",
    "_We study stochastic models of increasing complexity in tandem with numeraical algorithms for optimization. The simplest class of stochastic models we consider are Markov chains._\n",
    "\n",
    "Suppose we are given a _finite_ set $\\mathcal{S}$ which we call the **_state space_** of the Markov chain. This will sometimes (such as in text-modeling instances) be called the **_alphabet_**, in which case we may use $\\mathcal{A}$ in place of $\\mathcal{S}$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUVKLVO4kHnr"
   },
   "source": [
    "___\n",
    "\n",
    "**Definition.** $\\quad$ A **_stochastic process_** is a collection of random variables.\n",
    "___\n",
    "\n",
    "We study a stochastic process $(X_t)_{t \\in \\mathbb{N}}$ taking values in the given finite set $\\mathcal{S}$. Thus, the temporal (given by the index $t$ in the stochastic process) and the spatial (given by the state value in $\\mathcal{S}$ of the process) aspects of this process are discrete. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTiOTlvCkHnr"
   },
   "source": [
    "___\n",
    "\n",
    "**Definition.** $\\quad$ The stochastic process $(X_t)_{t \\in \\mathbb{N}}$ is a **_Markov chain_** if for all $t$, \n",
    "\n",
    "$$\n",
    "\\mathbb{P} ( X_{t +1} | X_1, \\dots, X_t ) = \\mathbb{P} (X_{t+1} | X_t )\n",
    "$$\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMN9E60nkHns"
   },
   "source": [
    "_The main modeling assumption in Markov process theory is that the future of the process depends on the current state, but not the past. This is the probabilistic analogue of the idea of Newtonian determinism. This is encoded in the \"forward equation\" below._\n",
    "\n",
    "_Another interpretation of the Markov property is that the future and past are independent, conditional on the present. We typically simplify the structure of Markov chains with the following assumption._\n",
    "\n",
    "___\n",
    "\n",
    "**Definition.** The Markov chain $(X_t)$ is **_stationary_** if \n",
    "\n",
    "$$\n",
    "\\mathbb{P}(X_{t+1} | X_t ) = \\mathbb{P} (X_2 | X_1 ) \n",
    "$$\n",
    "\n",
    "for all $t \\in \\mathbb{N}$. The transition matrix of a stationary Markov chain is \n",
    "\n",
    "$$\n",
    "Q(x,y) = \\mathbb{P}( X_2 = y | X_1 = x ).\n",
    "$$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8aHdzjYkHns"
   },
   "source": [
    "_The most basic statistic associated to the Markov chain is the pmf(=probability mass function), or law of the random variable $X_t$. We denote this by_\n",
    "\n",
    "$$\n",
    "\\pi_t(s) = \\mathbb{P}(X_t = s) , \\quad s \\in \\mathcal{S} \n",
    "$$\n",
    "\n",
    "_We use the Markov property to obtain the recurrence relation_\n",
    "\n",
    "$$\n",
    "\\pi_{t+1}(y) = \\mathbb{P} ( X_{t+1} = y) \\\\\n",
    "= \\sum_{x \\in \\mathcal{S}} \\mathbb{P} (X_{t+1} = y | X_t = x ) \\\\\n",
    "= \\sum_x Q(x,y) \\pi_t(x),\n",
    "$$\n",
    "\n",
    "when we assume the chain is stationary. This calculation yields the **_forward equation_** \n",
    "\n",
    "$$\n",
    "\\pi_{t+1} = \\pi_t Q , \\quad t \\in \\mathbb{N}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynbwJuH8kHnt"
   },
   "source": [
    "Above, we use the convention that $\\pi_t$ is a row vector. The forward equation is linear and explicitly solvable when the chain is stationary. We proceed inductively to find\n",
    "\n",
    "$$\n",
    "\\pi_t = \\pi_1 Q^t , \\quad t \\in \\mathbb{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygm3_KtmkHnt"
   },
   "source": [
    "_One of the central questions in Markov chain theory is to understand the behavior as $t\\to\\infty$._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNKK3_NlkHnt"
   },
   "source": [
    "## **0.2 ... Equilibrium measures, ergodicity, random walks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dR8vBW6_kHnu"
   },
   "source": [
    "___\n",
    "\n",
    "**Definition.** $\\quad$ The pmf $\\pi$ is an **_equilibrium distribution_** or **_stationary distribution_** if \n",
    "\n",
    "$$\n",
    "\\pi = \\pi Q,\n",
    "$$\n",
    "\n",
    "and if $\\pi$ satisfies the normalization conditions\n",
    "\n",
    "$$\n",
    "\\pi(x) \\geq 0 \\quad \\text{ for all } x \\text{ and } \\quad \n",
    "    \\sum_x \\pi(x) =1 \n",
    "$$\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyJUoxEPkHnu"
   },
   "source": [
    "_The first equation_ (in the above definition) _expresses the fact that $\\pi_t = \\pi$ for all $t$ if $\\pi_1 =\\pi$. That is, while the values of $X_t$ change with time_ (it's a stochastic process) _its pmf does not. Thus the equilibrium distribution, or equilibrium measure, of a Markov chain captures the intuitive idea of a_ dynamic equilibrium _. The \"macroscopic\" observable $\\pi_t$ is independent of $t$, where as the \"micoscopic\" random variable $X_t$ fluctuates in time._ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUfhqAVHkHnu"
   },
   "source": [
    "_Let us now turn to the task of computing $\\pi$ given $Q$. Observe that $\\pi$ is a_ left _eigenvector of $Q$ with eigenvalue $1$. Further, $1$ must always be an eigenvalue of $Q$ since it corresponds to the_ right _eigenvector $(1, \\dots, 1)^T$. Thus, if we know how to solve for eigenvectors, we can determine $\\pi$. What is interesting in practice is the converse: for large transition matrices, say when $\\# \\mathcal{S}$ is $10^6 \\times 10^6$, it is more efficient to use Markov chains to approximate $\\pi$, than to use naive linear algebra This observation plays an important role in web crawlers and search engines._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33rdz4SpkHnu"
   },
   "source": [
    "_A central question in Markov chain theory is to understand how fast_ (the chain) _mixes. For example, how many shuffles does it take to obtain a truly_ (?) _random distribution if one begins with a perfectly ordered deck of cards?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onbAp0p-kHnv"
   },
   "source": [
    "_The mathematical formulation of this problem is the following. We assume given a Markov chain with a unique equilibrium $\\pi$, and we'd like to obtain rates of convergence for $\\pi_t$ to approach $\\pi$ Thus, when shuffling a deck of cards, we model it as a Markov chain in the permutation group on $52$ symbols, with different forms of shuffling determining different random walks in the permutation group. The act of shuffling a new pack of cards then translates into a random walk in the permutation group that beigins at the identity permutation. The analysis of how fast the pmf $\\pi_t$ approaches the uniform distribution_ (assumed to be the stationary distribution of the shuffling dynamics) _is then converted into the analysis of the eigenvalues of the transition matrix._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_RNATAzkHnv"
   },
   "source": [
    "_The following term will be used._\n",
    "\n",
    "___\n",
    "\n",
    "**Definition.** A stationary Markov chain is **_ergodic_** if it has a unique equilibrium distribution.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIJNL_hKkHnv"
   },
   "source": [
    "_Many conditions are known that ensure ergodicity. For example, if all terms of $Q$ are strictly positive, it is possible for any state to get to any other state, and this ensures that the Markov chain is ergodic. This assumption is too trestrictive in practice, since we a re mainly intersted in Markov chains with local moves such as random walks in a large state space. However, the Markov chain is ergodic if for every pair $x, y \\in \\mathcal{S}$ there is an integer $n(x,y)$ such that $Q^{n(x,y)} >0 $. Intuitively, this means that every state can visit every other state given enough time. We will assume this property holds._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAcp1jLtkHnw"
   },
   "source": [
    "_Physicsts define the term ergodic_ (describing Markov chain $(X_t)_{t\\in \\mathbb{N}}$)  _to mean the following_\n",
    "\n",
    "$$\n",
    "\\lim_{T \\to \\infty} \\sum_{t =1}^T f(X_t) = \\mathbb{E}_\\pi(f),\n",
    "$$\n",
    "\n",
    "where $\\pi = \\pi Q$ and $f: \\mathcal{S} \\to \\mathbb{R}$ is arbitrary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDFKOjpakHnw"
   },
   "source": [
    "_This formulation has the interpretation that a Markov chain is ergodic when the \"time average\" (the left-hand side) is equal to the \"spatial average with respect to the equilibrium measure\" (the right hand side)._\n",
    "\n",
    "For Markov chains ergodic in the first sense, the second notion of ergodicity follows from an ergodic theorem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Us8B7POAkHnx"
   },
   "source": [
    "_It is important to note the following typical bottlenecks and simplifications_\n",
    "\n",
    "* $\\mathcal{S}$ may be very large. For example (in shuffling dynamics), $\\mathcal{S}$ may be the permutation group. \n",
    "\n",
    "* $Q(x,y)$ is a sparse matrix.\n",
    "\n",
    "* Often, we don't need $\\pi$ explicitly, we instead want to evaluate $\\mathbb{E}_\\pi(f)$, where $f: \\mathcal{S} \\to \\mathbb{R}$ is some observable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DY1ASGoZkHnx"
   },
   "source": [
    "___\n",
    "\n",
    "**Example** ( Simple random walk ) **.** $\\quad$ Let $G = (V,E)$ be a graph with vertex set $V$ and edge set $E$. Given $x,y \\in V$, write $x \\sim y$ if the edge $\\{ x, y \\}$ lies in $E$. Let $\\textrm{deg}(x) \\triangleq \\sum_{x \\sim y } 1$ be the _degree_ of vertex $x$. A **_simple random walk_** on $G$ is a Markov chain with state space $V$ and transition matrix\n",
    "\n",
    "$$\n",
    "Q(x,y)\n",
    "    =\n",
    "        \\begin{cases}\n",
    "        \\frac{1}{\\textrm{deg}(x)}, & \\quad \\text{ if } x \\sim y \\\\\n",
    "        0 & \\quad \\text{ otherwise. }\n",
    "        \\end{cases}\n",
    "$$\n",
    "\n",
    "We say \"a\" simple random walk because we have not given the initial conditions of the walk. \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-Kl3l5LkHnx"
   },
   "source": [
    "___\n",
    "**Example** ( Random walks in the permutation group ) **.** $\\quad$ Let $\\mathfrak{S}_N$ denote the permutation group on $N$ symbols. To be clear, let\n",
    "\n",
    "$$\n",
    "[N] \\triangleq \\{ \\, 1, \\dots, N \\, \\},\n",
    "$$\n",
    "\n",
    "and recall that a _permutation_ on $[N]$ is a bijection from $[N]$ to itself. The group structure on the collection of such functions, denoted $\\mathfrak{S}_N$, is given by function composition. \n",
    "\n",
    "Below we describe two distinct rules for Markovian dynamics on $\\mathfrak{S}_N$. Each set of dynamics can be thought of as arising from forgetting the implicit Cayley graph structure on $\\mathfrak{S}_N$: this is the complete graph on $N!$ vertices. Below, we consider adjacency structures on $\\mathfrak{S}_N$ distinct from this and one another. \n",
    "\n",
    "Through the adjacency matrix from the previous example, these new graph structures (writing $\\sim'$ and $\\sim''$ in this example) on $\\mathfrak{S}_N$ induce simple random walk dynamics, and so each describes a concrete Markov chain on $\\mathfrak{S}_N$, modulo initial conditions.\n",
    "\n",
    "1. We declare permutations $\\sigma$ and $\\tau$ adjacent and write $\\sigma \\sim' \\tau$ if $\\sigma$ and $\\tau$ are related by a transposition of adjacent elements. The $'$-degree of each permutation in this graph is $N-1$.\n",
    "\n",
    "2. We declare permutations $\\sigma$ and $\\tau$ adjacent and write $\\sigma \\sim'' \\tau$ if $\\sigma$ and $\\tau$ are related by a transposition, not necessarily of adjacent elements. The $''$-degree of each permutation in this graph is ${N \\choose 2}$. \n",
    "\n",
    "These are examples of _distinct_ ergodic random walks on the same state space, namely $\\mathfrak{S}_N$, both of which have the uniform measure as their equilibrium measure.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjT_8ZyQkHny"
   },
   "source": [
    "## **0.3 ... Text modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAaXI3YCkHny"
   },
   "source": [
    "_The main idea in Shannon's model (of text / written language) is that text is text is a stationary, ergodic stochastic process $(X_t)$ taking values in an alphabet_\n",
    "\n",
    "$$\n",
    "\\mathcal{A} \\triangleq \\{\\, a, \\,b,\\, c,\\, \\dots\\,,\\, z,\\, \\_ \\,\\}\n",
    "$$\n",
    "\n",
    "_This model is easily augmented to include punctuation and case, but we ignore these concepts in the first approximation._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vIWYwNOkHny"
   },
   "source": [
    "_Note that we are not assuming that this process is a Markov chain. To clarify this point, let us note that stationarity is distinct from the Markov property (neither implies the other)._\n",
    "\n",
    "___\n",
    "\n",
    "**Definition.** $\\quad$ A discrete time stochastic process is **_stationary_** if the probabilities are invariant under arbitrary shifts. More precisely,\n",
    "\n",
    "$$\n",
    "\\mathbb{P} (\\, X_1 = x_1, \\dots, X_n = x_n\\,) \n",
    "    = \n",
    "        \\mathbb{P} ( \\,X_{1+k} = x_1, \\dots, X_{n+k} = x_n \\,)\n",
    "$$\n",
    "\n",
    "for all positive integers $k$ and $n$.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGc9qEYBkHny"
   },
   "source": [
    "_Intuitively, this means that if we were to observe strings of length $n$, their statistics are not changed by shifting them forward in time by an integer $k$. For such processes, the notion of ergodicity again means that \"time averages\" are equal to \"spatial averages\", though now we must take functions of the form $f_n(x_1, x_2, \\dots, x_n)$ and equate the time averages:_\n",
    "\n",
    "$$\n",
    "\\lim_{T \\to \\infty} \\frac{1}{T} \\sum_{t =0 }^{T-1} f_n(X_{t+1}, \\dots, X_{t+n}) \n",
    "$$\n",
    "\n",
    "_with the spatial average_\n",
    "\n",
    "$$\n",
    "\\mathbb{E} ( f_n( X_1, \\dots, X_n) )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQzjUbTbkHnz"
   },
   "source": [
    "_The above assumptions are introduced to conform to the idea that written text contains many heirarchical structures: letters are joined by phonetic rules to form words, words are linked by the rules of grammar into sentences, sentences are organized into paragraphs, and so on._\n",
    "\n",
    "_Strange as it may seem at first sight, these rules can be effectively modeled as a stochastic process. The use of stationary ergodic processes provides us with a definition flexible enough to include heirarchical structures (though these may be complex to write down), and simple enough to computationally test._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35VFmMq1kHnz"
   },
   "source": [
    "_The relation between the above discussion and the previous is that we may use Markov chains of increasing complexity to approximate the above \"true\" language. We will denote the law of the stationary stochastic process modeling written text by_\n",
    "\n",
    "$$\n",
    "\\mathbb{P}_{\\textrm{true}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIXU2XdikHnz"
   },
   "source": [
    "_In practice, this law is approximated by mining a large corpus (e.g. the works of Shakespeare) to determine the probabilities $\\mathbb{P}_{\\textrm{true}}(a_1, \\dots, a_n)$ for an arbitrary string $(a_1, \\dots, a_n) \\in \\mathcal{A}^n$._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrvgzr93kHn0"
   },
   "source": [
    "___\n",
    "\n",
    "**Example** (Digram model) **.** $\\quad$ The state space is the alphabet $\\mathcal{A} = \\{ \\, a,\\, b, \\, c, \\, \\dots \\, , \\,z, \\, \\_ \\}$. \n",
    "\n",
    "$$\n",
    "Q^{(2)}(x,y) = \\mathbb{P}_{\\textrm{true}} (\\,X_2 = y | X_1 = x\\,),\n",
    "$$\n",
    "\n",
    "which is to say, we form the character-to-character transition probabilities of a Markov process using the marginals of the full stationary process being approximated. \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJJPY1u2kHn0"
   },
   "source": [
    "We can get better approximations by allowing the Markov chain above to retain more of its history. This means defining a Markov process on $\\mathcal{A}^n$ for some $ n \\geq 2$. When $n=2$, we obtain the \"trigram\" model. \n",
    "\n",
    "___\n",
    "\n",
    "**Example** ( Trigram model ) **.** $\\quad$ Our state space is $\\mathcal{A}^2$, which we style as\n",
    "\n",
    "$$\n",
    "\\{ aa, \\,ab,\\,, \\dots, az, \\dots, \\_a, \\, \\dots, \\, \\_\\,\\_ \\} \n",
    "$$\n",
    "\n",
    "_Let $x = a_1a_2$ and $y = b_1b_2$. In order to form a text of three letters from $x$ and $y$, we must ensure that $x$ and $y$ agree on their overlap, that is when $a_2 = b_1$, so that $x$ and $y$ combine to give us the string $a_1a_2b_2$. With this restriction on $x$ and $y$, we obtain the associated transition matrix_\n",
    "\n",
    "$$\n",
    "Q^{(3)} (x,y) \n",
    "= \n",
    "\\frac{\n",
    "\\mathbb{P}_{\\textrm{true}} ( a_1 a_2 b_2 )\n",
    "}\n",
    "{\n",
    "\\mathbb{P}_{\\textrm{true}} (a_1 a_2) \n",
    "}\n",
    "$$\n",
    "\n",
    "_and $Q(x,y) =0$ when $a_2 \\neq b_1$._\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGTAyyfzkHn1"
   },
   "source": [
    "We can get better approximations by allowing the Markov chain above to retain more of its history. This means defining a Markov process on $\\mathcal{A}^n$ for some $ n \\geq 2$. When $n=2$, we obtain the \"trigram\" model. \n",
    "\n",
    "___\n",
    "\n",
    "**Example** ( $(n+1$)-gram model ) **.** $\\quad$ Our state space is $\\mathcal{A}^{n}$. A state $x$, say $x = (a_1, \\dots, a_n)$ can be followed by a string $y = (b_1, \\dots, b_n)$ only if $a_2 = b_1, a_3 = b_2, \\dots, a_n = b_{n-1}$. This induces transition matrix\n",
    "\n",
    "$$\n",
    "Q^{(n+1)} (x,y) \n",
    "= \n",
    "\\frac{\n",
    "\\mathbb{P}_{\\textrm{true}} ( a_1 a_2 \\dots a_n b_n )\n",
    "}\n",
    "{\n",
    "\\mathbb{P}_{\\textrm{true}} (a_1 a_2 \\dots a_n) \n",
    "}\n",
    "$$\n",
    "\n",
    "as above.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCX41jPMkHn2"
   },
   "source": [
    "_Shannon proved that as $n \\to \\infty$, the law of text generated by the above Markov approximations converges to the law of the true language. Note however that a good proof may not correspond to a good algorithm._\n",
    "\n",
    "_For example, as $n$ increases, the size of the state space_ ( of the approximating Markov process ) _grows exponentially. Thus the dimensions of the transition matrix are $\\#\\mathcal{A}^{2n}$, and worse yet, the size of the training data ( to determine $Q^{(n+1)}$ by mining ) also expands exponentially._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0nMGTygkHn2"
   },
   "source": [
    "_This follows the rules of everyday language. Once $n$ gets large enugh, say $5$ or $6$ in practice, it is much more efficient to make our fundamental unit words, rather than letters, since the number of true words of length $5$ is much lower than the $26^5$ combinations that are possible. This reflects the true nature of the space $\\_$ character. In effect, we are still using the digram model, though we have switched to a new \"alphabet\" whose fundamental units are words._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYGiNxOXkHn6"
   },
   "source": [
    "## **0.4 ... Gibbs measures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urnN5KMjkHn7"
   },
   "source": [
    "Given a finite state space $\\mathcal{S}$, consider an energy function $H : \\mathcal{S} \\to \\mathbb{R}$, and an inverse temperature $\\beta >0$. The associated Gibbs distribution is the pmf\n",
    "\n",
    "$$\n",
    "p_\\beta(x) = \\frac{ e^{- \\beta H(x) } } {Z_\\beta} , \\quad x \\in \\mathcal{S}\n",
    "$$\n",
    "\n",
    "where the normalization factor\n",
    "\n",
    "$$\n",
    "Z_\\beta \\triangleq \\sum_{y \\in \\mathcal{S}} e^{-\\beta H(y)}\n",
    "$$\n",
    "\n",
    "is called the **_partition function_**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYFdnzlWkHn9"
   },
   "source": [
    "_An important task in statistical mechanics is to compute the partition function, since_ (averages of?) _all physical observables can be obtained from it. These computations typically require great ingenuity._\n",
    "\n",
    "_The purpose of this section is to provide an explanation for the ubiquity of the Gibbs distribution on Bayesian principles. This avoids the reliance on heuristics from physics, in particular the use of ergodic theorems, and clarifies the fundamental role of entropy in constructing models for inference. This derivation is due to Jaynes and we will use it at several points in these notes. But first let us briefly provide some context on the origin of the Gibbs distribution, since this has both cultural and scientific importance._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1--OjrtkHn9"
   },
   "source": [
    "_Statistical mechanics is the study of the macroscopic properties of matter based on microscopic models. A fundamental example of macroscopic properties are the laws of an ideal gas, such as the expression_ \n",
    "\n",
    "$$\n",
    "pV = nRT.\n",
    "$$\n",
    "\n",
    "_The microscopic model in this example is Newtonian mechanics, used in the following way. The gas is assumed to consist of a large number, say $10^{23}$, of hard spheres that undergo elastic collisions when they meet. This model formalizes the question on whether Newtonian mechanics can be scaled up to describe macroscopic matter. The study of this problem caused a great deal of angst in the $19$th century. First, the existence of atoms had not yet been established, so the microscopic assumptions were a matter of doubt. Worse yet, that the macroscopic behavior is irreversible (\"the arrow of time\") is familiar to us from everyday experience. But Newtonian mechanics is invariant under time-reversal, so this problem carries subtle paradoxes._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKa1HVuEkHn9"
   },
   "source": [
    "_A similarly fundamental example, which is particularly relevant to pattern theory is the Ising model. This model was introduced to explain how the magnetization of permanent magnets (such as in a compass), can be obtained from a microscopic model of many small spins which balance energetic terms that favor alignment of neighboring spins, with thermal effects that cause them to fluctuate._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BZzdBgEkHn9"
   },
   "source": [
    "_As noted by Geman & Geman,_ ( reference? ) _by changing the background energy, essentially the same model can be used to study the (unphysical!) problem of image segmentation. This connection raises a natural question: why should physical heuristics work on problems that seem to have little to do with physics?_\n",
    "\n",
    "_The answer lies in viewing information theory as a foundation for inference. This approach, which is due to Jaynes, inverts the historical development of ideas by viewing information theory as the primary model for developing statistical mechanics, rather than viewing statistical mechanics as the foundation for information theory. The simplicity of this line of reasoning is best illustrated by the following Bayesian derivation of the Gibbs distribution._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKMqA3aBkHn-"
   },
   "source": [
    "_Suppose $X$ is a discrete random variable taking $m$ values with probabilities $p_1, \\dots, p_m$. These are assumed to be unknown to us. However, we suppose that we have observations of some function $f(X)$, taking values $f_i$ for each state $x_i$ of $X$, so that we may assume_\n",
    "\n",
    "$$\n",
    "\\mathbb{E} (f (X)) =\\theta \n",
    "$$\n",
    "\n",
    "_is known. The question is now the following: what is the best guess $\\mathbb{E} g(X)$ where $g$ is some other function? This is essentially asking for our best guess of the pmf $(p_1, \\dots, p_m)$ of $X$. Naively, this is an ill-posed problem, in the sense that we may have many answers depending on what our notion of \"best\" means: we have $m$ variables $(p_1, \\dots, p_m)$ subject to two linear constraints, namely $\\sum p_i = 1$, and $\\sum f_i p_i = \\theta$, thus there are many answers to this question, even from a linear perspective._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9mIpBs6kHn-"
   },
   "source": [
    "_Information theory provides a principled answer to this question. Shannon postulates that given a pmf $(p_1, \\dots, p_m)$ of a discrete random variable $X$ taking $m$ values, its entropy is_\n",
    "\n",
    "$$\n",
    "H(p) = - \\sum_{i=1}^m p_i \\log p_i \n",
    "$$\n",
    "\n",
    "_The entropy of a random variable describes its uncertainty. In particular, the notion of an optimal code tells us that entropy describes the optimal search procedure to determine the value of $X$ through a series of yes / no questions. This is one of the basic interpretations of entropy in information theory._\n",
    "\n",
    "_The relevance of this interpretation here is that it tells us that the most_ unbiased _estimate of the pmf of $X$ is obtained by maximizing the entropy of $X$ subject to the constraint\n",
    "\n",
    "$$\n",
    "\\mathbb{E} f(X) = \\sum_{i=1}^n p_i f_i = \\theta,\n",
    "$$\n",
    "\n",
    "_with $\\theta$ given. This use of information theory converts an ill-posed problem to a standard constrained maximization problem:_\n",
    "\n",
    "$$\n",
    "p = \\textrm{argmax} \\left( H(p) : \\sum p_i f_i = \\theta, \\, \\sum p_i = 1 \\right)\n",
    "$$\n",
    "\n",
    "_When we solve this problem, we find that_\n",
    "\n",
    "$$\n",
    "p_i = \\frac{ e^{- \\beta f_i } }{ Z_\\beta }\n",
    "$$\n",
    "\n",
    "_Thus, the above maximization of entropy principle recovers the Gibbs distribution, without requiring any form of physical reasoning. This observation explains the ubiquity of the Gibbs distribution in problems of inference._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVSHR8XIkHn_"
   },
   "source": [
    "_Now let's check the calculation. Let us introduce Lagrange multipliers $\\alpha$ and $\\beta$, and consider the unconstrained maximization problem for the function_\n",
    "\n",
    "$$\n",
    "J(p) = H(p) - \\beta \\left( \\sum_i f_i p_i - \\theta \\right) - \\alpha \\left( \\sum p_i -1 \\right)\n",
    "$$\n",
    "\n",
    "_Letting $\\partial_i$ denote the partial derivative with respect to $p_i$, the maximization criterion requires $\\partial_i J =0$ for each $i = 1, \\dots, m$. We differentiate the above expression to obtain_\n",
    "\n",
    "$$\n",
    "0 = - ( 1 + \\log p_i ) - \\alpha - \\beta f_i , \n",
    "$$\n",
    "\n",
    "_which may be solved to yield_\n",
    "\n",
    "$$\n",
    "p_i = \\frac{1}{Z_\\beta} e^{-\\beta f_i } ,\n",
    "$$\n",
    "\n",
    "_having written $Z_\\beta$ for $e^{1+\\alpha}$. This is just a convenient relabeling of the Lagrange multiplier._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOMfLTm5kHn_"
   },
   "source": [
    "_Our formula for $p$ tells us_\n",
    "\n",
    "$$\n",
    "\\frac{1}{Z_\\beta} \\sum f_i e^{-\\beta f_i} = \\theta.\n",
    "$$\n",
    "\n",
    "_This shows that the inverse temperature $\\beta$ is the Lagrange multiplier corresponding to the given constraint on $\\mathbb{E}( f(X))$. Finally, let's observe that the above equation may also be written as_\n",
    "\n",
    "$$\n",
    "-\\frac{\\partial}{\\partial \\beta} \\log Z_\\beta = \\theta.\n",
    "$$\n",
    "\n",
    "_This equation reflects a common theme in statistical mechanics. Since the observables may be obtained by differentiating the partition function $Z_\\beta$ with respect to the parameters of the model, the main task is to determine the partition function._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZMVnVRnkHoA"
   },
   "source": [
    "## 0.5 ... Decoding scrambled text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEOsXoKEkHoB"
   },
   "source": [
    "\n",
    "\n",
    "_We now combine ideas from the previous sections to develop a Bayesian method to decode a scrambled text._\n",
    "\n",
    "_The basic ideas of Bayesian inference are as follows. First, we assume that the world is stochastic and that inference may be modeled probablistically using Bayes rule for conditional probabilities. Our task is to determine the values of the random variable, $S$, that is hidden from us, based on observations $O$. In the example with scrambled text, the hidden state will be a permutation, while the observation is a given string of scrambled text. The underlying probablistic model is that the \"world\" consists of a true language that has been scrambled, letter by letter, by a random permutation._\n",
    "\n",
    "_The mathematical formalism of Bayes rule is simple. If both $S$ and $O$ are random events or random variables on the same probability space, we recall that conditional probabilities are defined by_\n",
    "\n",
    "$$\n",
    "\\mathbb{P}( S \\cap O ) =  \\mathbb{P} ( S | O ) \\mathbb{P} (O) = \\mathbb{P}( O |S) \\mathbb{P}(S), \n",
    "$$\n",
    "\n",
    "_Thus, we may rewrite this equation in the form_\n",
    "\n",
    "$$\n",
    "\\underbrace{\\mathbb{P}(S | O )}_{\\textrm{posterior}}\n",
    "    =\n",
    "        \\frac{\\mathbb{P}(O |S) \\mathbb{P}(S) }{ \\mathbb{P}(O) } \n",
    "    \\propto\n",
    "        \\underbrace{\\mathbb{P} ( O | S ) \\mathbb{P} (S)}_{\\textrm{prior}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uP56ILARkHoB"
   },
   "source": [
    "_The terms prior and posterior reflect the use of Bayes formula in modeling. The right-hand side of the formula reflects our assumption that we have a probablistic model, called the prior distribution, which generates the hidden variable and the observed signals, conditional on the hidden variable. The left hand side says that given an observation, we can use Bayes rules to update our prior for the hidden variable to obtain a new distribution called the posterior._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oanJrt6lkHoB"
   },
   "source": [
    "_A fundamental idea in Bayesian inference is to find the mode of the posterior. More precisely, if what we have is our probablistic model for interpreting the world, then our best guess for the hidden state is the mode of the posterior,_\n",
    "\n",
    "$$\n",
    "S_*  = \\textrm{argmax}_S \\mathbb{P}(S | O ) = \\textrm{argmax}_S \\mathbb{P} ( O |S ) \\mathbb{P}(S) \n",
    "$$\n",
    "\n",
    "_This simple idea acquires its richness from the diversity of its applications. Thus, let us flesh it out by applying it to decoding scrambled text. In order to get started, we need a good probablistic model for the text, as well as for the process of scrambling._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KClk08vEkHoB"
   },
   "source": [
    "_We model the scrambler as a permutation $\\sigma \\in \\mathfrak{S}_{ \\# \\mathcal{A}}$, a permutation on $\\# \\mathcal{A}$ symbols. The scrambler acts on the text letter by letter. That is, if $a_1a_2\\dots a_n$ is a string of true language, the scrambler converts this to a string $b_1 \\dots b_n$, where $b_i = \\sigma(a_i)$. Since the permutation acts letter by letter, we also have_\n",
    "\n",
    "$$\n",
    "\\mathbb{P}( O = b_1\\dots b_n | S = \\sigma) \n",
    "    = \n",
    "        \\mathbb{P}\n",
    "        \\left( \n",
    "        a_1 = \\sigma^{-1}(b_1), \\, \\dots, \\, a_n  =  \\sigma^{-1}(b_n) \n",
    "        \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHc0molakHoB"
   },
   "source": [
    "_This expression holds for all the probablistic models of text (true language, $n$-gram approximations and word-based Markov chains) mentioned previously. For simplicity, we will explain the process of inference using the digram model; other models may be explored in a similar way._\n",
    "\n",
    "_In the absence of any other information on the scrambling mechanism, the most natural choice for a prior on $\\mathcal{S}$ is to assume that it is chosen uniformly on the permutation group. This ensures that $P(S)$ is independent of $S$. This choice of uniform measure should be intuitive, but note also that it is the maximizer of entropy._\n",
    "\n",
    "_Combining the above ideas, we see that our best guess for the unknown permutation is_\n",
    "\n",
    "$$\n",
    "\\sigma_* = \\textrm{argmax}_{\\sigma\\, \\in\\, \\mathfrak{S}_{\\# \\mathcal{A}}} L(\\sigma)\n",
    "$$\n",
    "\n",
    "_where we have defined the **likelihood function**_\n",
    "\n",
    "$$\n",
    "L(\\sigma) = \\mathbb{P}_{\\textrm{true}} \\left( \\sigma^{-1} (b_1) \\right)\n",
    "    \\prod_{j=1}^{n-1} Q^{(2)} \\left( \\, \\sigma^{-1}(b_j), \\, \\sigma^{-1} (b_{j+1}) \\, \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hly6LhgnkHoC"
   },
   "source": [
    "_At this state, we have obtained a clear mathematical formulation for the problem of decoding a scrambled text. Now we need to see whether this idea works! The catch is the following: in order to find the argmax, we must maximize the likelihood function on a large space. This is computationally intractable without a fundamental randomized algorithm: Markov Chain Monte Carlo (MCMC)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxD8FVPYkHoC"
   },
   "source": [
    "## **0.6 ... Sampling from a Gibbs distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaHZzDBAkHoC"
   },
   "source": [
    "_The_ **_Metropolis algorithm_** _is a randomized algorithm for sampling from a Gibbs distribution. Let us briefly describe the sampling problem. We then connect it to the problem of decoding scrambled text. The mathematics under the hood is discussed in the next section._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGbp03oVkHoC"
   },
   "source": [
    "_Let us assume that we are given a finite state space $\\mathcal{S}$, an energy function $E : \\mathcal{S} \\to \\mathbb{R}$, and an inverse temperature $\\beta$. Our task is to sample from the Gibbs distirbution_\n",
    "\n",
    "$$\n",
    "p_\\beta(x) = \\frac{ \\exp ( - \\beta E(x) ) }{ Z_\\beta} , \\quad x \\in \\mathcal{S}\n",
    "$$\n",
    "\n",
    "_Naively, sampling means that we'd like to generate an i.i.d. sequence $(X_t)_{t \\in \\mathbb{N}}$ distributed according to $p_\\beta$. The average of any observable $f(X)$ may then be approximated by the empirical mean_\n",
    "\n",
    "$$\n",
    "\\mathbb{E} ( f(X)) = \\sum_{ x \\, \\in \\, \\mathcal{S}} f(x) p_\\beta(x) \\approx \\frac{1}{T} \\sum_{t =1}^T f(X_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUWn2YL3kHoC"
   },
   "source": [
    "_When $\\mathcal{S}$ is large, generating i.i.d. sequences is prohibitively expensive. Instead, the Metropolis scheme generates a Markov chain $(X_t)_{t=1}^\\infty$ which as $p_\\beta$ as an equilibrium measure. Provided the underlying Markov chain is ergodic, we may still use the display above to_ (approximately) _compute the expected value of an observable. The number of steps $T$ required to obtain a good estimate depends on how fast the Markov chain decorrelates (i.e. the number of steps $t$ needed so that $X_1$ and $X_t$ are almost independent)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEMAWhPQkHoC"
   },
   "source": [
    "_Let us illustrate these ideas with our model of scrambled text. The state space $\\mathcal{S}$ is the set of permutations on $27$ symbols, denoted $\\mathfrak{S}_{27}$. In accordance with standard notation for permutations, we use $\\sigma$ instead of $x$ to denote a point in $\\mathcal{S}$. Let us define the energy function_\n",
    "\n",
    "$$\n",
    "E(\\sigma) = - \\log L(\\sigma),\n",
    "$$\n",
    "\n",
    "_where $L$ is as above, i.e._\n",
    "\n",
    "$$\n",
    "L(\\sigma) = \\mathbb{P}_{\\textrm{true}} \\left( \\sigma^{-1} (b_1) \\right)\n",
    "    \\prod_{j=1}^{n-1} Q^{(2)} \\left( \\, \\sigma^{-1}(b_j), \\, \\sigma^{-1} (b_{j+1}) \\, \\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqeUe2BVkHoC"
   },
   "source": [
    "_Thus,_\n",
    "\n",
    "$$\n",
    "E(\\sigma) = - \\log \\mathbb{P}_{\\textrm{true}} \\left( \\sigma^{-1} (b_1) \\right) \n",
    "    - \\sum_{j=1}^{n-1} \\log Q^{(2)} \\left( \\, \\sigma^{-1} (b_j), \\, \\sigma^{-1} (b_{j+1}) \\, \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fyPifbPkHoD"
   },
   "source": [
    "Rather than maximizing $L$, we think now of the equivalent goal of minimizing $E$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfUiXgHMkHoD"
   },
   "source": [
    "_The energy function $E$ is a sum of nearest-neighbor terms. This structure arises because of our assumption that text is modeled by a Markov process, not because there is any \"physics\" underlying text ( physical models such as the Ising model have a similar structure; the common thread is the assumption of local spatial dependence for the stochastic process $X_t$ )._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvpOs9vykHoD"
   },
   "source": [
    "## 0.7 ... The Metropolis algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppe7glNzkHoD"
   },
   "source": [
    "_The **Metropolis algorithm** relies on biasing an easily generated Markov chain, such as a symmetric random walk, so that the new equilibrium distribution is $p_\\beta$. Assume given a Markov chain on $\\mathcal{S}$ whose equilibrium distribution is uniform. Let us first describe the scheme as a numerical recipe for decoding scrambled text, so that its description is concrete:_\n",
    "\n",
    "1. _Estimate $\\mathbb{P}_{\\textrm{true}}(a)$ for $a \\in \\mathcal{A}$ and $Q^{(2)}(a_1, a_2)$ for $a_1, a_2 \\in \\mathcal{A}$ by mining a sufficiently large text such as_ War and Peace _._\n",
    "\n",
    "2. _Generate a symmetric random walk in $\\mathfrak{S}_{27}$ using uniform random transpositions and bias it using the following acceptance / rejection rule. Consider a proposed move $\\sigma \\mapsto \\tau$, where $\\sigma$ and $\\tau$ are related through transposition. Compute $\\Delta E = E(\\tau) - E(\\sigma)$._\n",
    "    \n",
    "    _If $\\Delta E < 0$,_ `ACCEPT` _the move._ \n",
    "    \n",
    "    _If $\\Delta E \\geq 0$,_ `ACCEPT` _with probability $\\exp( - \\beta \\Delta E)$ (where $\\beta > 0$ is fixed, say $\\beta =1$ to be concrete)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hp3_emzQkHoD"
   },
   "source": [
    "_The above scheme provides a Markov chain whose equilibrium distribution is $\\exp ( -\\beta E(\\sigma)) / Z_\\beta$, where $E(\\sigma)$ is defined above. The scheme pushes $\\sigma$ \"down\" the energy landscape, while allowing it a small probability of escaping local minima. In practice, given a sufficiently long string (i.e. when $n$ is large enough), the energy landscape is sharply concentrated around its minima at $\\sigma_*$. Thus once the Markov chain hits the minima, it stays there forever. This is why a_ randomized _scheme can be used to solve a_ deterministic _minimization problem._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSQV1G5QkHoD"
   },
   "source": [
    "_Let us now describe the mathematical structure of the Metropolis scheme more carefully. We assume given a state space $\\mathcal{S}$, an energy function $E : \\mathcal{S} \\to \\mathbb{R}$ and an inverse temperature $\\beta >0$. The regime of interest is when $\\mathcal{S}$ is so large that it is impossible to evaluate the partition function_\n",
    "\n",
    "$$\n",
    "Z_\\beta = \\sum_{x \\, \\in \\, \\mathcal{S} } \\exp ( -\\beta E(x) )\n",
    "$$\n",
    "\n",
    "_and thus the Gibbs distribution._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-9etREgkHoD"
   },
   "source": [
    "_The Metropolis scheme relies on the existence of an easily sampled Markov chain. In the first applications of the method, the underlying model was the Ising model and the markov chain was a random walk in the space of spins. Configurations of spins are said to be neighbors when they are related by spin flips._\n",
    "\n",
    "_It is not necessary to assume such a specific structure on $\\mathcal{S}$. Every transition matrix $Q$ that is symmetric, i.e. $Q(x,y) = Q(y,x)$ for all $x,y \\in \\mathcal{S}$ has a uniform equilibrium distribution and may be used as a source of randomness for a Metropolis scheme._\n",
    "\n",
    "_This may be seen as follows. The equilibrium distribution $\\pi$ for $Q$ satisfies the equation $\\pi(y) = \\sum_x \\pi(x) Q(x,y)$. The equilibrium distribution $\\pi$ for $Q$ satisfies the equation_\n",
    "\n",
    "$$\n",
    "\\pi(y) = \\sum_x \\pi(x) Q(x,y) \n",
    "$$\n",
    "\n",
    "_We use this to show $\\pi(x) = 1 / \\# \\mathcal{S}$. Let us start by observing that_\n",
    "\n",
    "$$\n",
    "\\sum_y Q(x,y) = 1,\n",
    "$$\n",
    "\n",
    "_as these are probabilities which must sum to one. We are supposing $Q(x,y) = Q(y,x)$, thus we also have_\n",
    "\n",
    "$$\n",
    "\\sum_y Q(y,x) =1 \\,,\n",
    "$$\n",
    "\n",
    "_which upon re-indexing implies_ \n",
    "\n",
    "$$\n",
    "\\sum_x Q(x,y) = 1\n",
    "$$ \n",
    "\n",
    "_for each $y$. Consider the function $\\tilde{\\pi} : \\mathcal{S} \\to \\mathbb{R}$ which is constant, assigning each state $y \\in \\mathcal{S}$ to $\\tilde{\\pi}(y) \\equiv 1$. Because of this definition, we may insert $\\tilde{\\pi}$ into the display above: $\\tilde{\\pi}(x) \\equiv 1$ accompanies each term in the sum, and we write the $1$ on the right-hand side as $\\tilde{\\pi}(y)$._\n",
    "\n",
    "$$\n",
    "\\sum_x \\tilde{\\pi}(x) Q(x,y) = \\tilde{\\pi}(y).\n",
    "$$\n",
    "\n",
    "It follows that $\\tilde{\\pi}$ is a right-eigenvalue of $Q$, it is thus an equilibrium measure. This implies that $\\tilde{\\pi}$, when properly normalized by $(\\# \\mathcal{S})^{-1}$ is an equilibrium probability measure, which addresses our claim above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaJsAprNkHoD"
   },
   "source": [
    "Given a Markov chain generated by $Q$, we construct a new Markov chain with equilibrium measure $p_\\beta$ by biasing the chain as follows. Define the transition matrix\n",
    "\n",
    "$$\n",
    "B(x,y) = \n",
    "\\begin{cases}\n",
    "Q(x,y),  & \\textrm{ if } p_\\beta(x) < p_\\beta(y) \\\\\n",
    "\\frac{p_\\beta(y)}{p_\\beta(x)} Q(x,y), & \\textrm{ if } p_\\beta(x) \\geq p_\\beta(y) \\\\\n",
    "1 - \\sum_{x'} B(x,x') & \\textrm{ when } x = y\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaJV_yJkkHoD"
   },
   "source": [
    "### **0.7.1 ... Detailed balance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgZPTvQUkHoE"
   },
   "source": [
    "The above biasing procedure looks \"asymmetric,\" but this is misleading. The matrix $B$ is naturally suited to $p_\\beta$ because of the following identity, which is called **_detailed balance_**:\n",
    "\n",
    "$$\n",
    "p_\\beta(x) B(x,y) = p_\\beta(y) B(y,x) , \\quad x, y \\in \\mathcal{S}.\n",
    "$$\n",
    "\n",
    "The above identity is easily established. Without loss of generality, assume $p_\\beta(x) \\geq p_\\beta(y)$. Then,\n",
    "\n",
    "$$\n",
    "B(x,y) = \\frac{ p_\\beta(y) }{ p_\\beta(x) } Q(x,y), \\quad \\text{and} \\quad B(y,x) = Q(x,y)\n",
    "$$\n",
    "\n",
    "implying the prior display. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Njmu5xpWkHoE"
   },
   "source": [
    "Detailed balance implies that the equilibrium of $B$ is $p_\\beta$. Indeed, summing over $x$ in the right hand side of the detailed balance identity, one has\n",
    "\n",
    "$$\n",
    "p_\\beta(y) \\sum_x B(y,x) = \\sum_x p_\\beta(x) B(x,y) ,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtjJKGJpkHoE"
   },
   "source": [
    "which reduces to $p_\\beta(y) = \\sum_x p_\\beta(x) B(x,y)$, or as a matrix multiplication,\n",
    "\n",
    "$$\n",
    "p_\\beta = p_\\beta B \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OObmxI47kHoE"
   },
   "source": [
    "The success of MCMC relies on the interplay between theoretical guarantees and practical implementations that originate in the condition of detailed balance. We consider some of these below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eohXx9PYkHoE"
   },
   "source": [
    "### **0.7.2 ... Local moves**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wS8X8s2kHoE"
   },
   "source": [
    "We wanted a Markov chain to sample from $p_\\beta$, and we obtain it by biasing $Q(x,y)$ by $p_\\beta(y) / p_\\beta(x)$. What makes the computation of $p_\\beta$ intractible is the sum over states; as mentioned the partition function is difficult to compute, and is necessary to do any kind of naive sampling from $p_\\beta$. However, the ratio\n",
    "\n",
    "$$\n",
    "\\frac{p_\\beta(x)} {p_\\beta(y)} = \\frac{ e^{-\\beta E(x)} }{Z_\\beta} \\cdot \\frac{Z_\\beta}{e^{-\\beta E(y)} } = \\exp \\left( -\\beta\\left( E(x) - E(y) \\right)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbCrLKenkHoE"
   },
   "source": [
    "depends only on the difference in energy between two states and this is easily computed. In particular, for the Ising model, as well as the digram model of text, the change in energy between neighboring states $x$ and $y$ requires an update of only a small number of terms. Thus, it is inexpensive to compute.\n",
    "\n",
    "This property reflects a typical strenght and design principle for Metropolis schemes: they must rely on updates $\\Delta E$ that are cheap to compute. One the other hand, this property also reflects a weakness of Metropolis schemes. If $\\beta \\Delta E$ is large, say at a local minimum of $E$, then many moves are rejected before the state escapes from the local minimum. In such cases, the Metropolis scheme must be replaced by modifications of it such as **_simulated annnealing_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8TL9NmwkHoE"
   },
   "source": [
    "### **0.7.3 ... $B$ generates a reversible Markov chain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9I5oxnvkHoE"
   },
   "source": [
    "While it is natural to think of increasing $t$ as representing the arrow of time, the Markov property requires only that the future and past be independent, conditional on the present. Thus, given a transition matrix for a stationary Markov chain, we may always construct a time reversed Markov chain as follows. Since\n",
    "\n",
    "$$\n",
    "Q(x,y) = \\frac{ \\mathbb{P} ( X_1 = x, X_2 = y) }{ \\mathbb{P} (X_1 =x ) } ,\n",
    "$$\n",
    "\n",
    "by conditioning on $X_2$ instead of $X_1$, we obtain the reversed chain with transition matrix\n",
    "\n",
    "$$\n",
    "R(y,x) \\triangleq \\frac{ \\mathbb{P}(X_2 = y, X_1 = x)}{ \\mathbb{P}(X_2 = y) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQpOBQJvkHoF"
   },
   "source": [
    "While it is natural to think of increasing $t$ as representing the arrow of time, the Markov property requires only that the future and past be independent, conditional on the present. Thus, given a transition matrix for a stationary Markov chain, we may always construct a time reversed Markov chain as follows. <font color='blue'>( This is at least a sense in which stochastic dynamics can share a \"time-reversibility\" with classical Newtonian mechanics )</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since\n",
    "\n",
    "$$\n",
    "Q(x,y) = \\frac{ \\mathbb{P} (X_1 = x, X_2 = y) }{ \\mathbb{P}(X_1 = x) },\n",
    "$$\n",
    "\n",
    "By conditioning on $X_2$ instead of $X_1$ we obtain the reversed chain with transition matrix\n",
    "\n",
    "$$\n",
    "R(y,x) \\equiv \\frac{ \\mathbb{P}(X_2 = y, X_1 = x) }{ \\mathbb{P} (X_2 = y ) } = \\frac{ \\mathbb{P}(X_1 = x) }{\\mathbb{P}(X_2 = y) } Q(x,y)\n",
    "$$\n",
    "\n",
    "___\n",
    "\n",
    "**Rmk** $\\quad$ I think this part does a good job suggesting a relationship between the detailed balance condition for a Markov chain and the Bayes rule. It suggests a relationship between inference and dynamics on a graph, and this makes things like belief propagation and message passing algorithms feel like more natural objects.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above equation may be rewritten as the identity\n",
    "\n",
    "$$\n",
    "R(y,x) \\pi(y) = Q(x,y) \\pi(x),\n",
    "$$\n",
    "\n",
    "where $\\pi$ is the equilibrium for $Q$, and thus $R$ (the chain is stationary).\n",
    "\n",
    "In general, the time-reversed Markov chain generated by $R$ is not the same as the forward chain generated by $Q$. However, these are identical for the Metropolis scheme. Indeed, replacing $Q(x,y)$ with $B(x,y)$ and $\\pi(x)$ with $p_\\beta(x)$ in the above display, we find that the reveresed chain satisfies\n",
    "\n",
    "$$\n",
    "R(y,x) = \\frac{\\mathbb{P}(X_1 = x, X_2 = y) }{\\mathbb{P}(X_2 = y) } = \\frac{ \\mathbb{P} (X_1 = x, X_2 = y) }{ p_\\beta(y) }\n",
    "$$\n",
    "\n",
    "Thus, we may use the definition of $B$ to show\n",
    "\n",
    "$$\n",
    "R(y,x) = B(y,x) , \\quad x,y \\in \\mathcal{S},\n",
    "$$\n",
    "\n",
    "so that the chains are equivalent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculation reflects the fact that detailed balance is a condition of local equilibrium between every pair of states in the Markov chain. This principle plays an important role in the description of chemical reaction networks. In this context, each state of the chain is a chemical compound, and the transition rates $B(x,y)$ and $B(y,x)$ reflect the rates of forward and backward equations. When the system is in equilibrium, each set of forward and backward reactions must be balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.8 ... $B$ is a self-adjoint operator on $L^2_{p_\\beta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of positive weights $m = (m_1, \\dots, m_N)$ may be used to define an inner product on $\\mathbb{R}^N$ as follows:\n",
    "\n",
    "$$\n",
    "\\langle v, w \\rangle_m \\triangleq \\sum_{i=1}^N m_i v_i w_i \n",
    "$$\n",
    "\n",
    "The _adjoint_, $A^*$, of a linear operator $A: \\mathbb{R}^N \\to \\mathbb{R}^N$ with respect to this inner product is defined by \n",
    "\n",
    "$$\n",
    "\\langle A^* v, w \\rangle_m = \\langle v, Aw \\rangle_m \\quad v, w \\in \\mathbb{R}^N\n",
    "$$\n",
    "\n",
    "Finally, we say that an operator is self-adjoint when $A \\equiv A^*$. Now let us apply this idea to $\\mathbb{R}^{\\# \\mathcal{S}}$ choosing $p_\\beta(x)$ as the weight. This weight determines the vector space $L^2_{p_\\beta}$, and the adjoint of a linear operator $A$ on $L^2_{p_\\beta}$ is given by \n",
    "\n",
    "$$\n",
    "A^*(x,y) = \\frac{ p_\\beta(x) }{p_\\beta(y) } A(x,y), \\quad x, y \\in \\mathcal{S}\n",
    "$$\n",
    "\n",
    "In particular, detailed balance implies that $B = B^*$, thus $B$ is self-adjoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-adjoint operators are of fundamental importance in mathematics and physics. They always have real eigenvalues and admit the following _spectral decomposition_:\n",
    "\n",
    "$$\n",
    "A(x,y) = \\pi(y) \\sum_{i=1}^{\\# \\mathcal{S}} \\alpha_i \\psi_i (x) \\psi_i(y) ,\n",
    "$$\n",
    "\n",
    "where the $\\alpha_i$ and $\\varphi_i$ are the eigenvalues and eigenvectors of $A$. It then follows that all powers of $A$ may be expressed as\n",
    "\n",
    "$$\n",
    "(A^n)(x,y) = \\pi(y) \\sum_{i=1}^{\\# \\mathcal{S} } \\alpha_i^n \\psi_i(x) \\psi_i(y), \\quad x,y \\in \\mathcal{S}\n",
    "$$\n",
    "\n",
    "We may also write the spectral decomposition in terms of operators, i.e.\n",
    "\n",
    "$$\n",
    "A = \\sum_i \\alpha_i \\psi_i \\psi_i^*,\n",
    "$$\n",
    "\n",
    "where given a column vector $\\psi$ with entries $\\psi(x)$, $x\\in \\mathcal{S}$, its adjoint is a column vector $\\psi^*$ with entries $\\pi(y) \\psi(y)$, $y \\in \\mathcal{S}$. Then the rank-one operators $P_i \\triangleq \\psi_i \\psi_i^*$ are orthogonal projections in $L_{p_\\beta}^2$ onto the space spanned by the vector $\\psi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.9 ... Convergence of Markov chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now return to Markov chains, contrassting the general theory of convergence for Markov chains, with the theory for Markov chains with a transition matrix that is self-adjoint. \n",
    "\n",
    "Suppose we are given a transition matrix $P$ on a state space of size $N$ which determines an ergodic Markov chain with probability distribution $\\pi$. We would like to use the forward equation $\\pi_{n+1} = \\pi_n P$ to prove that $\\lim_{n \\to \\infty} \\pi_n = \\pi$. We write $v_n$ for the difference $v_n \\equiv \\pi_n -\\pi$, and use $\\pi = \\pi P$ to obtain the equation\n",
    "\n",
    "$$\n",
    "v_{n+1} = v_n P = v_1 P^n,\n",
    "$$\n",
    "\n",
    "by induction on $n$. Thus, if $P = U \\Lambda U^{-1}$, where $\\Lambda$ is the diagonal matrix of eigenvalues and $U$ is a matrix of eigenvectors, we find that \n",
    "\n",
    "$$\n",
    "v_{n+1} = v_1 U \\Lambda^n U^{-1}.\n",
    "$$\n",
    "\n",
    "The asymptotics as $n \\to \\infty$ depend only on $\\Lambda$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with the general theory is that while it tells us that the chain converges to equilibrium, what we care about in practice is the _rate_ of convergence, i.e. how fast all diagonal terms except the first converge to zero. In general, we can't say very much. However, for a reversible Markov chain, such as the Metropolis scheme, we can say a lot more, using the fact that $B$ is self-adjoint in $L^2_{p_\\beta}$ and that it admits a spectral decomposition. In this case, we may order the eigenvalues $1 = \\lambda_1 > \\lambda_2 \\geq \\dots$, and we see that the rate of convergence of the Markov chain is $O( |\\lambda_2 - \\lambda_1|^n)$ as $n \\to \\infty$. Various techniques exist to estimate this spectral gap based on the geometry of the underlying state space ( see [5] for examples on shuffling )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1 ... From text to machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first review central ideas in information theory. These include the interpretation of entropy as the average depth of the optimal search to find a random variable, the entropy rate of a stationary process, the convergence of processes using relative entropy, and the mutual information. These ideas are then illustrated by applications to Markov chain models of text. These include the convergence of the $n$-gram approximations to true language as $n \\to \\infty$, and the use of mutual information to design a parsing scheme to detect word boundaries. The idea of a hidden Markov model (HMM) is introduced in the last section through an application to machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 ... Entropy and entropy rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy of a random variable $X$ taking values in a finite alphabet $\\mathcal{A}$ is \n",
    "\n",
    "$$\n",
    "H(X) \\triangleq - \\sum_{x \\, \\in \\, \\mathcal{A} } p(x) \\log p(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy $H(X_1, \\dots, X_n)$ of a finite sequence of random variables is also defined by the above formula, since a finite sequence taking values in $\\mathcal{A}$ may be lumped into a single random variable $Y \\triangleq (X_1, \\dots, X_n)$ taking values in $\\mathcal{A}^n$.\n",
    "\n",
    "___\n",
    "\n",
    "**Rmk** $\\quad$ That this definition extends so readily from one to $n$ random variables is a consequence of the fact that the entropy never sees the support of the random object, only relative likelihoods. \n",
    "\n",
    "___\n",
    "\n",
    "In order to have a meaningful limit as $n \\to \\infty$, it is necessary to normalize the entropy by the size of the sequence. \n",
    "\n",
    "___\n",
    "\n",
    "**Theorem** ( Entropy rate ) **.** $\\quad$ Suppose $(X_k)_{k=1}^\\infty$ is a stationary stochastic process taking values in a finite alphabet. Then the following limits exist and are equal.\n",
    "\n",
    "$$\n",
    "\\lim_{n \\to \\infty} \\frac{ H(X_1, \\dots, X_n) }{ n } = \\lim_{n \\to \\infty} H(X_n | X_{n-1} , \\dots, X_1 ),\n",
    "$$\n",
    "\n",
    "where we denote the shared limit $F$, and call it the **_entropy rate_**\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "_Proof:_ $\\quad$ Let $F_n = H(X_n | X_{n-1}, \\dots, X_1)$. The two lines below follow from the fact that|  conditioning reduces entropy (there is a footnote) and that the sequence is stationary, \n",
    "\n",
    "$$ \n",
    "F_{n+1} = H(X_{n+1} | X_n, \\dots, X_1 ) \\\\\n",
    "\\leq H (X_{n+1} | X_n, \\dots, X_2 ) \\\\\n",
    "= H(X_n | X_{n-1}, \\dots, X_1 ) = F_n,\n",
    "$$\n",
    "\n",
    "Thus $F_n$ is a decreasing sequence of numbers, bounded below. The equivalence of the two limits in the definition of the entropy rate is as follows. We use the entropy chain rule to write\n",
    "\n",
    "$$\n",
    "H(X_1, \\dots, X_n) = H(X_1) + H(X_2 | X_1) + H(X_3 | X_2, X_1) + \\dots +  H(X_n | X_{n-1}, \\dots, X_1) \\\\\n",
    "= F_1 + \\dots F_n,\n",
    "$$\n",
    "\n",
    "and given that the sequence $F_n$ is tending to $F$, it follows that\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} (F_1 + \\dots F_n ) \n",
    "$$\n",
    "\n",
    "is very close to $F$, when $n$ is large, completing the proof. $\\quad\\quad\\quad \\square$\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "**Example.** $\\quad$ Since a stationary Markov chain is also a stationary stochastic process, we may use the above formula to compute its entropy rate. To this end, suppose $(X_k)$ is a Markov chain with transition matrix $Q(x,y)$ and equilibrium pmf $\\pi(x)$. We specialize the entropy rate formula as follows.\n",
    "\n",
    "$$\n",
    "F = \\lim_n H(X_n | X_{n-1}, \\dots, X_1 )  \\underbrace{=}_{ \\textrm{Markov property}} \\lim_{n \\to \\infty} H(X_n | X_{n-1} ) \\\\\n",
    "\\underbrace{=}_{\\textrm{stationarity}} H(X_2 | X_1) \\\\\n",
    "= \\sum_{x \\, \\in\\, \\mathcal{A} } \\mathbb{P}(X_1 = x) H(X_2 | X_1 =x) \\\\\n",
    "\\equiv - \\sum_{x \\, \\in \\, \\mathcal{A} } \\pi(x) \\sum_{x \\, \\in \\, \\mathcal{A}} Q(x,y) \\log Q(x,y)\n",
    "$$\n",
    "\n",
    "Here we have just used the definition of entropy.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 ... Entropy and data compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Entropy as coding length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the fundamental interpretations of entropy is that it corresponds to the length of an _optimal code_. We first review the idea for binary prefix codes. Suppose $X$ is a random variable taking values in a finite alphabet $\\mathcal{A} = \\{1, \\dots, m\\}$ with probabilities $(p_1, \\dots, p_m)$. A binary code is a map $C: \\mathcal{A} \\to \\{0,1\\}^*$, where $\\{0,1\\}^*$ denotes the space of finite binary strings. We say that $C$ is a **_prefix code_** if no codeword $C(x)$ is a prefix of another codeword $C(y)$ for all pairs $x, y \\in \\mathcal{A}$. A prefix code is in one-to-one correspondence with a binary tree whose leaves correspond to the codewords $\\{ C(x) : x \\in \\mathcal{A} \\}$. \n",
    "\n",
    "This equivalence may be seen by drawing a binary tree and labeling the vertices in lexicographical order beginning with the root. The labels of the leaves are the codewords. In what follows, we only consider prefix codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a code $C$, the length of the codeword for $x$, denoted $\\ell(x)$, is the number of terms in the binary string $C(x)$. Since every binary prefix code is in correspondence with a binary tree, $\\ell(x)$ may also be interpreted as the length of the path joining the root to the leaf $C(x)$.  \n",
    "\n",
    "The fundamental bounds relating entropy to data compression are as follows. First, every binary prefix code for $X$ is bounded below by the entropy:\n",
    "\n",
    "$$\n",
    "H(x) \\leq E(\\ell(X)) \\triangleq \\sum_x p(x) \\ell(x)\n",
    "$$\n",
    "\n",
    "A code is optimal if it achieves the minimum of $\\mathbb{E}( \\ell(X))$ over the space of codes. When $C$ is optimal, the above bound admits a matching upper bound, \n",
    "\n",
    "$$\n",
    "\\mathbb{E} ( \\ell_*(X) ) < H(X) +1,\n",
    "$$\n",
    "\n",
    "where we have denoted the optimal code by $C_*$ and its length by $\\ell_*$. An optimal code may be computed using Huffman's algorithm as explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dangling $1$ in the display directly above can be improved by considering a block code, replacing the idea of entropy with entropy rate, and considering the code-length per symbol. One easily finds that the entropy rate of a stationary stochastic process is the length-per-symbol of an optimal code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 ... Huffman's algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a pmf $(p_1, \\dots, p_m)$, an optimal code may be computed in $O(m\\log m)$ steps using Huffman's algorithm. This is a greedy algorithm that recursively builds a binary tree whose codewords are leaves. The algorithm has $m$ steps and is indexed in a decreasing fashion, so that the index $k$ starts at $k_{\\textrm{initial}} = m$ at the initialization of algorithm, and $k_{\\textrm{final}} = 1$. The input to the algorithm is the pmf $(p_1, \\dots, p_m)$, and a forest, called $\\tau$, which is initialized to consist of $m$ trees, each with a single vertex. The $(k-1)^{\\, \\textrm{th}}$ iterate is then obtained as follows:\n",
    "\n",
    "1. Rerank the pmf $q^{(k)}$ in decreasing order. More precisely, choose a permutation $\\sigma \\in \\mathfrak{S}_k$ such that $q_{\\sigma(1)} \\geq q_{\\sigma(2)} \\geq \\dots \\geq q_{\\sigma(k)}$. Let $s^{(k)}$ denote the reordered pmf, so that $s_i = q_{\\sigma(i)}$. \n",
    "\n",
    "2. Merge the two smallest probabilities $s_{k-1}$ and $s_k$ to obtain the _next_ (though the index _decreases_) pmf vector\n",
    "\n",
    "$$\n",
    "q^{(k-1)} = (\\,s_1,\\, s_2, \\dots, \\, s_{k-1} + s_k \\, )\n",
    "$$\n",
    "\n",
    "3. Use the permutation $\\sigma$ to relabel the trees within the forest, defining a new forest $\\tilde{\\tau} = (\\, \\tau_{\\sigma(1)} , \\, \\dots, \\, \\tau_{\\sigma(k)} \\,)$ where each $\\tau_j$ is a connected component of the forest. \n",
    "\n",
    "4. Merge the trees $\\tau_{\\sigma(k-1)}$ and $\\tau_{\\sigma(k)}$, associated to the last two probabilities by adding a new root and two edges labeled with $0$ and $1$ respectively that connect the root to $\\tau_{\\sigma(k-1)}$ and $\\tau_{\\sigma(k)}$. Define $\\tau^{(k-1)}$ to be the resulting modification of the forest $\\tilde{\\tau}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 ... Typical sequences and the AEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy rate also quantifies the effective size of our probability space. Fix an integer $N$ and let $\\Omega_N$ denote the space of $\\mathcal{A}$-sequences of length $N$:\n",
    "\n",
    "$$\n",
    "\\Omega_N \\triangleq \\{ \\, a \\in \\mathcal{A}^N \\, | \\, a = (a_1, \\dots, a_N) \\, \\}\n",
    "$$\n",
    "\n",
    "Given a stationary stochastic processs $(X_k)_{k=1}^\\infty$, let $\\mathbb{P}_N$ denote the joint pmf of $(X_1, \\dots, X_N)$. We clearly have\n",
    "\n",
    "$$\n",
    "\\# \\Omega_N = 2^{ N \\cdot \\log \\# \\mathcal{A} }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the fundamental interpretations of entropy is that it \"thins down\" the space of all sequences to a set of \"typical sequences,\" which is of size $2^{N \\cdot F}$, where $F$ denotes the entropy rate of $(X_k)_{k\\geq1}$. As discussed earlier, this shows that entropy quantifies data compression by focusing our attention on the \"set of sequences that matter.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notes go on to explain that the origin of this thinning lies in the weak law of large numbers, or some ergodic theorem more generally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 ... The entropy of English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy rate allows us to quantify the balance between freedom and redundancy in a stochastic process. There are six strings of text listed (I will not copy these verbatim), each over the alphabet\n",
    "\n",
    "$$\n",
    "\\mathcal{A} = \\{ \\,a,\\, \\dots\\, , \\,z,\\,\\_ \\, \\}, \n",
    "$$\n",
    "\n",
    "generated by simulating the following stochastic processes:\n",
    "\n",
    "1. An i.i.d. sequence (product uniform measure on $\\mathcal{A}$).\n",
    "\n",
    "2. An i.i.d. sequence (a product measure on $\\mathcal{A}$), the probabilities of the letters chosen according to their true frequencies in text.\n",
    "\n",
    "___\n",
    "\n",
    "**Rmk** $\\quad$ <font color='blue'>To connect with next, i.i.d. is synonymous with $1$-gram, I think </font>\n",
    "\n",
    "___\n",
    "\n",
    "3. A $2$-gram stationary Markov chain on $\\mathcal{A}$, with transition rates $Q^{(2)}$  determined by the true language\n",
    "\n",
    "4. A $3$-gram stationary Markov chain on $\\mathcal{A}$, with transition rates $^{(3)}$ determined by the true language.\n",
    "\n",
    "5. We adopt a new alphabet, each \"character\" of the new alphabet is an English word,\n",
    "\n",
    "    $$\n",
    "    \\mathcal{W} = \\{ \\,\\textrm{English words} \\,\\},\n",
    "    $$\n",
    "    \n",
    "    The text is an i.i.d. sequence with values in $\\mathcal{W}$, and the probabilities of each word taken from frequences mined from some corpus. The spaces are deterministically added to signify gaps between words. \n",
    "\n",
    "6. A Markov chain on $\\mathcal{W}$ which is the analogue of the $2$-gram approximation above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These samples illustrate several ideas. Examples $(1)$ through $(4)$ show that the stochastic processes mimic true language with greater fidelity as the underlying transition probabilities use more information on joint distributions of the underlying true language. \n",
    "\n",
    "Examples $(5)$ and $(6)$ illustrate a deeper principle: true language carries a heirarchy of structures, along with rules for binding these structures. Examples $(5)$ and $(6)$ illustrate this principle with the simplest heirarchy, namely the difference between letters and words. Even within our naive formalism for language, thiss example shows that a true language can be modeled on different probability spaces $-$ sequences of letters or sequences of words. In order to determine whether $(5)$ is a better approximation than $(6)$, we must formalize the idea of mapping one space to the other. Mapping a word to a sequence of letters is _spelling_; mapping a string of letters to a string of words is a type of _parsing_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 ... Markov chain models cannot be grammatical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The efficacy of Shannon's model, both in generating the above sequences and decoding scrambled text, should also give us pause. Even though the output of $(6)$ is more natural than any of the previous approximations, it clearly doesn't \"feel right\" because of the unusual construction of the sentence, as well as the semantic flaws. How do we model this?\n",
    "\n",
    "The underlying rules that determines a language constitute a _grammar_. Native speakers of a language effortlessly perceive a violation of grammar as we do when reading a sentence such as the one in the sixth example. Such violations may be modeled within a Bayesian framework, since the notion \"something doesn't feel right\" merely means that an observation is very unlikely within a probablistic model. But this also makes explicit the fact that in order to build a probabilistic model of language that is more realistic than Shannon's approximations, we need a deeper mathematical model of grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modern study of grammar was revolutionized a few years after Shannon's work by Chomsky [ ? ]. In contrast with earlier work on linguistics that focused on classifying the grammars of existing languages, Chomsky proposed a set of generative rules that may be used to construct a language. \n",
    "\n",
    "\n",
    "____\n",
    "\n",
    "A striking assertion in Chomsky's work, which provides an extreme counterpoint to Shannon's models above, is that _no_ finite alphabet Markov chain approximation can constitute a true grammar, in Chomsky's sense.\n",
    "\n",
    "___\n",
    "\n",
    "This assertion seems to contradict the efficacy of the Markov chain model in a practical setting such as decoding scrambled text, as well as the theorems that follow proving that these rules converge in a precise sense to true language. In order to resolve this (apparent) contradiction, it is necessary to consider Chomsky's definitions of grammars and heirarchy more carefully (he may write about this in next iteration of notes). For now, we will view Shannon's model as a starting point, and simply use it as a proof of concept that language can be studied through probablistic methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 ... Convergence of Markov chain models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may now apply the ideas of previous sections to form a precise notion of the entropy rate of English. \n",
    "\n",
    "1. We model English as a stationary stochastic process taking values in the usual alphabet $\\mathcal{A}$, with $\\# \\mathcal{A} = 27$. Thus, its entropy rate exists. \n",
    "\n",
    "2. We construct the $n$-gram Markov chain approximations with transition matrix $Q^{(n)}$ and stationary distribution $\\mathbb{P}_{\\textrm{true}}$ independent of $n$. The entropy rate of these Markov chains may be computed using [ the expression for entropy rate? ]\n",
    "\n",
    "3. As $n \\to \\infty$ we show below that the approximations converge to true language in the sense of relative entropy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notation differs slightly from Section 3.1. We now use $F_n$ to denote the entropy rate of the $n$-gram Markov chain (with _letter_ alphabet $\\mathcal{A}$ instead of the word alphabet $\\mathcal{W}$). We will later consider the word-based approximation. \n",
    "\n",
    "In order to compute the entropy rate, we let consider strings $x = a_1a_2 \\dots a_{n-1}$ and $y = a_2 a_3 \\dots a_n$ and recall that the equilibrium distribution and transition probability are given by\n",
    "\n",
    "$$\n",
    "\\pi^{(n)} (x) = \\mathbb{P}_{\\textrm{true}} (a_1 a_2 \\dots a_{n-1}) \\, \\quad Q^{(n)} (x,y) = \\frac{ \\mathbb{P}_{\\textrm{true}}( a_1 \\dots a_n ) }{ \\mathbb{P}_{\\textrm{true}} (a_1 \\dots a_{n-1} ) }\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $H_n$ denote the entropy of the sequence $(X_1, X_2, \\dots, X_{n-1})$. Since $\\mathbb{P}_{\\textrm{true}}$ and the $n$-gram approximation agree (in law) on sequences of length $n-1$, we find that \n",
    "\n",
    "$$\n",
    "H_n = - \\sum_{a_1, \\dots, a_{n-1}} \\mathbb{P}_{\\textrm{true}} (a_1 \\dots a_{n-1} ) \\log \\mathbb{P}_{\\textrm{true}}(a_1 \\dots a_{n-1}) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our first result about entropy rate, one has\n",
    "\n",
    "$$\n",
    "F_n = H_n - H_{n-1}, \\quad \\text{or} \\quad H_n = F_1 + \\dots + F_n \n",
    "$$\n",
    "\n",
    "An argument similar to section 3.1 shows that $F_n$ is a decreasing sequence and that $F = \\lim_{n \\to \\infty} F_n$ is the entropy rate of the true language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculation shows that the n-gram approximations converge to true language, and thus so does the entropy rate. However, it does not provide us with an efficient way to compute the entropy rate $F$. The catch is that the alphabet size for the $n$-gram approximation has size $27^{n-1}$. It becomes increasingly hard to estimate the parameters $Q^{(n)}$ as $n$ increases. \n",
    "\n",
    "Shannon invented a guessing game that re-encodes English text in a way that allows a relatively easy way to estimate the entropy of English. The idea that the $n$-gram approximations converge to the true language is formalized as follows. Both $\\mathbb{P}_{\\textrm{true}}$ and $Q^{(n)}$ define probability distributions on the space of sequences of length $N$, $\\Omega_N$, for every $N$. We fix the length $N$, as well as the scale of approximation $n$ and use $\\mathbb{P}_{\\textrm{true}, \\,N}$ and $\\mathbb{P}_{n-\\textrm{gram},\\, N}$ denote these probability distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compute the relative entropy $D( \\mathbb{P}_{\\textrm{true},\\,N} \\| \\mathbb{P}_{n-\\textrm{gram},\\, N} )$ as follows.\n",
    "\n",
    "First, we note that $D( \\mathbb{P}_{\\textrm{true},\\,N} \\| \\mathbb{P}_{n-\\textrm{gram},\\, N} ) = 0$ when $n \\geq N$, since both probability distributions agree on sequences of length $n$. Therefore, it is sufficient to restrict attention to $N > n$. In this case, we obtain \n",
    "\n",
    "$$\n",
    "D( \\mathbb{P}_{\\textrm{true},\\,N} \\| \\mathbb{P}_{n-\\textrm{gram},\\, N} )\n",
    "    =\n",
    "        \\sum_{x_N \\, =\\, a_1 \\dots a_N} \n",
    "            \\mathbb{P}_{\\textrm{true}}(x_N) \n",
    "            \\log \\frac{\n",
    "                \\mathbb{P}_{\\textrm{true}}(x_N)\n",
    "                }\n",
    "                {\n",
    "                \\mathbb{P}_{n-\\textrm{gram},\\, N }(x_N)\n",
    "                } \\\\\n",
    "                =\n",
    "                    H( \\mathbb{P}_{n-\\textrm{gram},\\, N}) - H(\\mathbb{P}_{\\textrm{true},\\,N})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, we find that\n",
    "\n",
    "$$\n",
    "H( \\mathbb{P}_{ \\textrm{true},\\, N } = F_1 + \\dots + F_N,\n",
    "$$\n",
    "\n",
    "and similarly, we use the entropy chain rule to find that \n",
    "\n",
    "$$\n",
    "H( \\mathbb{P}_{ n-\\textrm{gram} , \\, N} )  = F_1 + \\dots + F_n + (N-n) F_n,\n",
    "$$\n",
    "\n",
    "and the two displays directly above imply\n",
    "\n",
    "$$\n",
    "\\frac{1}{N} D( \\mathbb{P}_{\\textrm{true},\\,N} \\| \\mathbb{P}_{n-\\textrm{gram},\\, N} ) = \\frac{1}{N} \\sum_{k\\,=\\,n+1}^N (F_n - F_k)\\\\ \n",
    "\\leq \\frac{N-n}{N} (F_n - F),\n",
    "$$\n",
    "\n",
    "using the monotonicity of the sequence, and with $F = \\lim_{n \\to \\infty} F_n$. Thus, \n",
    "\n",
    "$$\n",
    "\\limsup_{N \\to \\infty} \\frac{1}{N} D( \\mathbb{P}_{\\textrm{true},\\,N} \\| \\mathbb{P}_{n-\\textrm{gram},\\, N} ) \\leq (F_n - F)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 .. Words and word boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we consider the word based Markov chains in more detail. We first define the lexicon, $\\mathcal{L}$ of a language to be the set of words that it contains. Since we will need to distinguish between sequences of letters and sequences of words, let $\\Omega_{\\mathcal{L}}$ and $\\Omega_{\\mathcal{A}}$ denote the space of sequences taking values in $\\mathcal{L}$ and $\\mathcal{A}$ respectively. \n",
    "\n",
    "So far we have modeled true language as a probability distribution $\\mathbb{P}_{\\textrm{true}}$ on the space $\\Omega_{\\mathcal{A}}$. However, since language (as we are thinking of it here) _is_ a sequence of words, we could also have chosen true language to be a probability distribution $\\mathbb{Q}_{\\textrm{true}}$ on the space $\\Omega_{\\mathcal{L}}$. What's going on here is that we need to distinguish between language as an abstract construct and its representation in different forms (text, speech, strings of words, etc.). Thus, in order to ensure that our definitions are consistent, we must find an invertible map between $\\Omega_{\\mathcal{A}}$ and $\\Omega_{\\mathcal{L}}$ such that the probabilities of corresponding events are equal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This formal description has a familiar interpretation. The act of spelling associates a unique ssequence of letters to each word. Let us call this map `spell`$: \\mathcal{L} \\to \\mathcal{A}$, so that `spell`$(w)$ is a sequence of letters $a_1\\dots a_p$ that spell the word $w$. This extends to a sequence of words in $\\Omega_{\\mathcal{L}}$ as follows: a word-sequence $w_1\\dots w_p$ is mapped to\n",
    "\n",
    "$$\n",
    "\\textsf{spell}(w_1) \\, \\_ \\, \\textsf{spell}(w_2) \\, \\_ \\, \\dots\\, \\_ \\,\\textsf{spell}(w_p)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, given a sequence in $\\Omega_{\\mathcal{A}}$ that is a sample of true language (i.e., lies in the support of $\\mathbb{P}_{\\textrm{true}}$), the associated sequence of words may be defined to be the letters between spaces. (Note that one needs to be more careful mapping letters to words; a sequence of letters that isn't part of the lexicon does not have a preimage. We avoid this issue by choosing only the sequences that live in the support of $\\mathbb{P}_{\\textrm{true}}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of a special character for the space is redundant. The fact that texts without spaces cannot be read easily reflects only our training, not a fundamental arrangement for the use of the space character. For example, it is possible to read slowly and break a sentence into words, even when the space has been removed. Similarly, several written scripts drop information (e.g. vowels in Semitic languages), which experienced readers judge from context. \n",
    "\n",
    "This issue becomes especially important when considering speech. By the same logic as above, speech is a manifestation of true language. The fundamental unit of speech is a **_phoneme_**, and the act of dictation is the conversion of a string in $\\Omega_{\\mathcal{A}}$ to a string of phonemes. Unlike written text, words in speech are not separated by spaces (that would sound like staccato speech, whereas we prefer words to flow). Yet humans have no difficulty separating a sequence of phonemes into a string of words.\n",
    "\n",
    "This discussion suggests that the space character is redundant and that it should be possible to parse a sequence of letters without spaces into a sequence of words. We will call this map `parse`$:\\Omega_\\mathcal{A} \\to \\Omega_{\\mathcal{L}}$. In what follows we describe a model that is similar to the digram model for text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 ... Parsing word boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We adopt the following notation in this section. Given alphabet $\\mathcal{A}$ and lexicon $\\mathcal{L}$, let $\\mathcal{A}^*$ and $\\mathcal{L}^*$ respectively denote the spaces of infinite strings of symbols from each. Observe that a Markov chain on $\\mathcal{L}$ generates a sequence in $\\mathcal{L}^*$, whereas the $n$-gram model generates a sequence in $\\mathcal{A}^*$. These spaces are related through the `spell` and `parse` maps. \n",
    "\n",
    "As discussed, parsing a string in $\\mathcal{A}^*$ is easy when $\\mathcal{A}$ includes the space character. A more interesting question is how to parse a string without a special space character. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We approach this problem by constructing an energy function that uses the mutual information. Given two random variables $X$, $Y$ with joint law $\\mathbb{P}_{X,Y}$, their **_mutual information_** $I(X,Y)$ is defined as the relative entropy\n",
    "\n",
    "$$\n",
    "I(X,Y) = D( \\mathbb{P}_{X,Y} \\| \\mathbb{P}_X \\otimes \\mathbb{P}_Y ) \n",
    "$$\n",
    "\n",
    "Loosely speaking $I(X,Y)$ quantifies how strongly coupled $X$ and $Y$ are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an energy for word boundaries as follows. Consider two strings $\\theta = a_1 \\dots a_n$ and $\\tau = a_{m+1} \\dots a_n$ and let $\\sigma = \\theta \\tau$ denote their concatenation. We assume that $X$ and $Y$ are consecutive strings drawn from the true language, such that $X = \\theta$, $Y = \\tau$. Then the joint distribution and marginals are\n",
    "\n",
    "$$\n",
    "\\mathbb{P}_{X,Y}\n",
    "(X= \\theta, Y \\tau) = \\mathbb{P}_{\\textrm{true}}(a_1 \\dots a_n) \\\\\n",
    "\\mathbb{P}_X (X = \\theta) = \\mathbb{P}_{\\textrm{true}} (a_1 \\dots a_m) , \n",
    "\\quad \\mathbb{P}_Y (Y = \\tau) = \\mathbb{P}_{\\textrm{true}}(a_{m+1} \\dots a_n) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mutual information of these random variables is\n",
    "\n",
    "$$\n",
    "I(X,;Y) \n",
    "    = \n",
    "        \\sum_{\\theta, \\, \\tau} \n",
    "        \\mathbb{P}_{\\textrm{true}} (\\theta \\tau) \n",
    "        \\left( \n",
    "        \\log \n",
    "        \\frac{ \n",
    "        \\mathbb{P}_{\\textrm{true}}(\\theta \\tau) \n",
    "        }\n",
    "        {\n",
    "        \\mathbb{P}_{\\textrm{true}} (\\theta) \\mathbb{P}_{\\textrm{true}} (\\tau) \n",
    "        }\n",
    "        \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mutual information vanishess when strings $X$ and $Y$ are independent. While we do not expect two distinct words to be independent, hueristically it is clear that two consecutive strings from distinct words should be less closely related than two strings that are part of the same word. The simplest version of this idea is the analogue of the digram approximation. We consider two consecutive pairs of letters, i.e. $n = m = 2$, and we define the **_binding energy_** for a sequence of four letters $a_1 a_2 a_3 a_4$ to be\n",
    "\n",
    "$$\n",
    "E( a_1 a_2 a_3 a_4 ) \n",
    "    =\n",
    "        \\log \n",
    "        \\left(\n",
    "        \\frac{ \n",
    "        \\mathbb{P}_{\\textrm{true}}(a_1 a_2 a_3 a_4) \n",
    "        }\n",
    "        {\n",
    "        \\mathbb{P}_{\\textrm{true}} (a_1 a_2) \\mathbb{P}_{\\textrm{true}} (a_3 a_4) \n",
    "        }\n",
    "        \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following numerical experiment may now be carried out: assume the law of true English is known (say for all strings $a_1 a_2 a_3 a_4$ of length $4$). Now take a corpus of English text and \"clean\" it by removing all punctuation and case, including the space character. This gives a string of letters in the Latin alphabet. In order to determine the word boundaries in this text, we define the function\n",
    "\n",
    "$$\n",
    "e(k) = E(a_k a_{k+1} a_{k+2} a_{k+3}),\n",
    "$$\n",
    "\n",
    "which computes the binding energy of consecutive $4$-strings. This naive scheme works surprisingly well [ 12 ]. \n",
    "\n",
    "___\n",
    "\n",
    "**Q** $\\quad$ The above energy only looks forward in time, with respect to the data. Does this energy work better than one which is _centered_ around $k$? \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 ... The HMM model for machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine translation is a fundamental task in natural language processing. We assume given a string of words or phrases in one language. Our task is to translate this string of words into another language. The difficulty of the task depends on the differences between the languages, but is even challenging for two languages with common Latin roots, for instance. One of the difficulties is that word-for-word translation often provides poor results, since the use of different words in different languages relies strongly on context. What follows is a simplified model which nevertheless contains a central idea of successful algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say the two languages are French and English. Given a sequence of words in French, $f = f_1 \\dots f_m$, we seek a sequence of English words $e = e_1 \\dots e_m$ that is a translation of $f$ (it is not essential that the strings have the same number of words since we may map words to phrases, or map a word to a null character, but we will assume a word-to-word translation for simplicity).\n",
    "\n",
    "___\n",
    "\n",
    "_I can imagine a $1$-$1$ mapping (allowing mapping to null word) at least deciding which of the translations of each word will be present in the resulting translated sentence._\n",
    "___\n",
    "\n",
    "\n",
    "\n",
    "In order to apply the Bayesian paradigm to this problem we must first associate a joint probability distribution to the strings $e$ and $f$. We may then apply Bayes rules as usual to obtain\n",
    "\n",
    "$$\n",
    "\\mathbb{P}( e | f) \\propto \\underbrace{\\mathbb{P}( f | e ) \\mathbb{P}(e) }_{\\textrm{constructed using the prior}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $e$ is the hidden state - the unknown string in English, and $f$ is the observation - the given French string. Our best guess of the translation is then\n",
    "\n",
    "$$\n",
    "e_* = \\textrm{argmax}_{e} \\log \\mathbb{P}(f | e ) \\mathbb{P}(e) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct the prior using Markov chains. To this end, we assume that\n",
    "\n",
    "* $e$ is a Markov chain\n",
    "\n",
    "* We assume we have an English to French statistical dictionary that provides probabilities $\\mathbb{P}(f|e)$, i.e. the probability that an English word is associated to a French word $f$. It should be familiar from experience with a dictionary that a single word may have many meanings; in a similar manner, an English word may have many possible translations. Thus, _all_ dictionaries are implicitly statistical, since a reader must typically use context to determine the meaning or translation of a word. \n",
    "\n",
    "Observe that the Markov chain uses only the structure of the language of the observer. We use the transition kernel of this Markov chain to compute the probability of a string of English words $e = e_1 \\dots e_m$,\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(e) = \\mathbb{P}(e_1) Q(e_1, e_2) Q(e_2, e_3) \\dots Q(e_{m-1}, e_m)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the statistical dictionary to define the conditional probability \n",
    "\n",
    "$$\n",
    "\\mathbb{P}(f \\| e ) = \\mathbb{P} (f_1 | e_1) \\mathbb{P}(f_2 | e_2) ... \\mathbb{P}(f_m | e_m) \n",
    "$$\n",
    "\n",
    "With these two elements in place, the task of machine translation is reduced to the task of computing the argmax (over $e$) of $\\mathbb{P}(e |f)$. This maximum may be computed fast using the fact that the logarithm of the likelihood can be written as sums of terms. This algorithm is of independent interest and is treated in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 ... Dynamic programming ( Bellman's algorithm ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effective implementation of the HMM model requires a fast algorithm to determine the mode of the posterior. The log-likelihood of the posterior has the feature that it decomposes into a sum of terms that depend only on their nearest neighbors. This structure appears in several graphical models and it may be exploited as follows.\n",
    "\n",
    "___\n",
    "\n",
    "**Theorem** ( Bellman's algorithm ) **.** $\\quad$ Assume given a function of the form \n",
    "\n",
    "$$\n",
    "F(x_1, \\dots, x_n) = f_1(x_1, x_2) + f_2(x_2, x_3) + \\dots + f_{n-1}(x_{n-1} , x_n) \n",
    "$$\n",
    "\n",
    "where the $x_k$ take values in a finite set $S_k$, such that $|S_k| \\leq s $ for all $k$. Then the minimum of $F$ can be determined in $\\mathcal{O}(ns^2)$ steps. \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "**Remark.** $\\quad$ The size of the search space is $\\mathcal{O}(s^n)$ since $|S_k| = \\mathcal{O}(s)$ for each $k$. This algorithm reduces the size of the search space dramatically using a recursive strategy.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Proof._ $\\quad$ The algorithm relies on a recursive decomposition that provides the min and argmin of partial sums. The last step is a form of back propagation that provides the min and argmin of $F$.\n",
    "\n",
    "* Step 1 (Forward iteration). $\\quad$ We define\n",
    "\n",
    "    $$\n",
    "    h_k(x_k) = \\min_{x_1,\\, \\dots\\, ,\\, x_{k-1}}  [ \\, f_1(x_1, x_2) + \\dots + f_{k-1}( x_{k-1}, x_k )  \\, ] \\\\\n",
    "    = \\min_{x_{k-1}} [ \\, h_{k-1}(x_{k-1}) + f_{k-1} (x_{k-1}, x) \\, ]\n",
    "    $$\n",
    "\n",
    "    _(J: the above relies on the additive structure of the function $F$)_ \n",
    "\n",
    "    For each index $k$, we assume given a value $x_k$ and we must sweep through all values of $x_{k-1}$ to determine $h_k(x_k)$ and $\\Phi(x_k)$. This requires at most $s^2$ steps. Since there are $n$ possible indices, the entire procedure takes at most $ns^2$ steps. \n",
    "    \n",
    "    $$\n",
    "    \\text{ }\n",
    "    $$\n",
    "    \n",
    "* Step 2 (Back substitution). $\\quad$ When we get to the last step, we observe that the minimum over $x_n$ of $h_n(x_n)$ is the minimum of $F$. Let $\\bar{x}_n$ be this argmin. Then\n",
    "\n",
    "    $$\n",
    "    h_n( \\bar{x}_n) \n",
    "        = \n",
    "        \\min_{x_1,\\, x_2,\\,  \\dots\\, ,\\, x_{n-1}, \\,x_n} \n",
    "        [ \\, f_1 (x_1 , x_2) + \\dots + f_n(x_{n-1}, x_n) \\, ]\n",
    "    $$\n",
    "    \n",
    "    We determine the argmin of $F$ by backsubstitution, setting\n",
    "    \n",
    "    $$\n",
    "    \\bar{x}_{n-1} = \\Phi_n(\\bar{x}_n),\\, \\bar{x}_{n-2} = \\Phi_{n-1}(\\bar{x}_{n-1}),\\, \\dots\\,,\n",
    "    \\bar{x}_1 = \\Phi_2( \\bar{x}_2 ) \n",
    "    $$\n",
    "    \n",
    "$\\square$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch 4 ... Gaussian processes and Fourier analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 ... Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayesian paradigm of inference relies on the construction of priors on the space of signals. These lectures began with a model of text because of its simple structure: the signal $(X_t)_{t=1}^\\infty$ takes values in a finite alphabet and the time $t$ is discrete. When we extend our interest to signals such as music or speech, we must consider continuous random variables $(X_t)$ since the underlying auditory signals are pressure waves. Music is typically less complex than speech, since (many forms of) music can be transcribed with a score. Examples of such signals are shown in the Figures below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to apply the Bayesian paradigm to such signals we must construct models that generate music or speech signals. This requires familiarity with stochastic processes in continuous space and time. While a proper mathematical treatment of this subject can seem forbiddingly abstract, the theory has a strong intuitive foundation, since a small number of fundamental examples can be used to construct signals of great complexity. The most important examples are Gaussian processes, compound Poisson processes, and continuous time Markov processes. In each case, the underlying stochastic process is characterized by a relatively simple set of parameters (covariance kernels for Gaussians, jump kernels for compound Poisson processes and generators for Markov processes). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this chapter and the next is to illustrate the Gaussian paradigm. We first review Gaussians on $\\mathbb{R}^n$ and the Fourier transform. These ideas are then extended to random Gaussian functions and the construction of stationary Gaussian processes. The unifying thread is that all Gaussian processes are characterized by covariance matrices (which are called covariance kernels when $n \\to \\infty$). In the next chapter, these ideas culminate in the theory of Brownian motion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A central tool, especially when we consider random functions, is Fourier analysis. The main idea here is to decompose a space of functions into simple building blocks (a Fourier basis) and then to form random combinations of these basis functions. Fourier analysis admits many variants depending on the domain of the underlying function. When considering periodic functions, we use the term Fourier series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "summary:\n",
    "\n",
    "* If $X \\in \\mathbb{R}^n$ is Gaussian with covariance kernel $K$ we may evaluate this integral to find \n",
    "\n",
    "    $$\n",
    "    H(X) = \\frac{1}{2} \\log ( \\det(2\\pi K) )\n",
    "    $$\n",
    "\n",
    "* In $\\S4.3$, using Parseval to work with Gaussian random functions defined on the Fourier side. \n",
    "\n",
    "* $\\S 4.4$, the characteristic function approach to the CLT is sketched. In the implicit calculation, we subtract the mean of the random variable, so Taylor expanding the ch. f. of object in question now is of the form $(1 - \\frac{1}{2}z^2)$, and the $-\\frac{1}{2} z^2$ is the reason for Gaussian. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch 5 ... Brownian Motion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 ... Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The physical phenomenon of Brownian motion was first recorded by the botanist Robert Brown in 1827. He observed the erratic motion of pollen grains under a microscope, but did not provide an explanation for this motion. The origin of this phenomena remained obscure until 1905, when Albert Einstein recognized that the random motion of pollen grains, or more generally colloidal particles, originated in a vast number of microscopic collisions with even smaller particles, invisible under a microscope. This allowed Einstein to provide a microscopic explanation for diffusion and a method to compute Avogadro's number. Einstein's predictions were verified in Perrin's experiments in 1908 and provided evidence for the existence of atoms and molecules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The construction of a mathematical theory of Brownian motion, beginning with the work of Norbert Wiener, is one of the triumphs of 20th century mathematics. Wiener modeled Brownian motion as a Gaussian random function, writing it as a Fourier series of the form\n",
    "\n",
    "$$\n",
    "B(t) = X_0 t + \\sqrt{2} \\sum_{n=1}^\\infty \\frac{X_n}{n} \\sin n \\pi t , \\quad t \\in [0,1],\n",
    "$$\n",
    "\n",
    "where $(X_n)_{n \\geq 0}$ is an i.i.d. sequence of standard normal random variables. The specific choice of these variances ensures that $( \\,B(t):t\\in [0,1]\\,)$ has the following properties:\n",
    "\n",
    "1. $B(0) = 0$.\n",
    "\n",
    "2. The increment $B(t) - B(s)$ is independent of $B(s)$ for all $t > s$. \n",
    "\n",
    "3. The increment $B(t) - B(s)$ is normal with mean zero and variance $t-s$ for all $0 \\leq s < t$\n",
    "\n",
    "4. $B(t)$ is continuous with probability $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This construction reveals certain features of the Brownian motion, but obscures others. The most delicate property to establish is the last. To understand some of the subtleties, let us formally differentiate the expression for Brownian motion above,\n",
    "\n",
    "$$\n",
    "B'(t) \\underbrace{=}_{?} X_0 + \\sqrt{2} \\pi \\sum_{n=1}^\\infty X_n \\cos n \\pi t \n",
    "$$\n",
    "\n",
    "In contrast with the series considered previously, the above diverges almost surely. (J: nonetheless we can make sense of the object as a random signed measure) Thus the erratic nature of (physically observed) Brownian motion is reflected in the fact that the (mathematically constructed) function $B(t)$ is _nowhere differentiable_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A proof of these facts would take us too far afield, but one may get a feeling for what is involved by studying simpler examples, called _lacunary series_. These are Fourier series of the form\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{n=1}^\\infty \\frac{ \\sin( n! \\pi x ) }{n^2} , \\quad  \\text{ or } \\quad g(x) = \\sum_{n=1}^\\infty a^n \\sin (b^n x),\n",
    "$$\n",
    "\n",
    "with $0 < a < 1 < b$. Such series are called lacunary because most Fourier coefficients vanish (for example, in the first example $\\hat{f}(k) = 0$, unless $k$ is an integer of the form $k = n!$). The graphs of these series are typical examples of fractals, a term popularized in the 1970s by Benoit Mandelbrot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to establish the convergence of such series. Since $|\\sin(y) | \\leq 1$, we find that \n",
    "\n",
    "$$\n",
    "\\max_{x \\in [0,1]} | f(x) - f_N(x) | \\leq \\sum_{n= N+1}^\\infty \\frac{1}{n^2} = O(1/N),\n",
    "$$\n",
    "\n",
    "where $f_N(x)$ denotes the finite sum\n",
    "\n",
    "$$\n",
    "f_N(x) = \\sum_{n=1}^N \\frac{1}{n^2} \\sin (n! \\pi x) \n",
    "$$\n",
    "\n",
    "In the late 1800s, Weierstrass proved that the uniform limit of continuous functions on $[0,1]$ is continuous. In particular, $f_N$ converges uniformly to $f$, so that $f$ is continuous. The problem however is that the derivative of $f_N$ grows fast. Indeed, we see that the partial sums\n",
    "\n",
    "$$\n",
    "f_N'(x) =\\sum_{n=1}^N \\frac{n!}{n^2} \\cos(n! \\pi x)\n",
    "$$\n",
    "\n",
    "diverge as $N \\to \\infty$. With some work, one can deduce that the limiting function $f$, while continuous, does not possess a derivative at _any_ point $x \\in (0,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weierstrass's construction of nowhere differentiable functions was rejected by many mathematicians of his day. (footnote: _\"A typical example, is a remark in a letter from Hermite to Stieltjes in May 1893: I turn away with fear and horror from the lamentable plague of continuous functions which do not have derivatives\"_) Even many who accepted it, viewed such functions as mathematical curiosities that were unphysical (J: and yet Mandelbrot's book is called \"The Fractal Geometry of Nature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we recognize Weierstrass and Wiener as a triumph of reason. Nowhere differentiable functions are typical, not atypical, and their construction and analysis has deep theoretical and practical consequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 ... The Levy-Ciesielski construction of Brownian Motion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of Fourier series makes Wiener's original construction of Brownian motion somewhat cumbersome. A simpler construction, in the same spirit, uses the Haar basis for $L^2(0,1)$. Good notation for this uses two indices (as in a \"typewriter sequence\" of functions, a canonical example of convergence in measure but not a.s.). The Schauder functions are defined to be the primals of the Haar basis. \n",
    "\n",
    "Brownian motion is then constructed as a Gaussian linear combination of these, and one can compute that $\\mathbb{E} (B(s) B(t)) = s \\wedge t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 ... The scaling limit of random walks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above construction of Brownian motion has the advantage of being direct. However, it obscures the Markov property of Brownian motion. This property is easy to see when we view Brownian motion as the scaling limit of random walks. Suppose $(Z_k)_{ k\\geq 1}$ are i.i.d. random variables such that $\\mathbb{E}(Z_k) = 0$, $\\mathbb{E}(Z_k^2) =1$. Let $S_n = Z_1 + \\dots + Z_n$. The central limit theorem asserts that\n",
    "\n",
    "$$\n",
    "\\lim_{n \\to \\infty}\n",
    "    \\mathbb{P} \\left(\n",
    "    a \\leq \\frac{S_n}{\\sqrt{n}} \\leq b \n",
    "    \\right) = \\frac{1}{\\sqrt{2\\pi}} \\int_a^b e^{-s^2 /2 } ds , \\quad -\\infty < a < b < \\infty\n",
    "$$\n",
    "\n",
    "We may interpret the central limit theorem as a statement about random walks in the following way. For each positive integer $n$, we define a random function $S^{(n)} : [0,\\infty) \\to \\mathbb{R}$, by first defining its values on the grid of times $t = k /n$ by\n",
    "\n",
    "$$\n",
    "S^{(n)} (t) = \\frac{Z_1 + \\dots + Z_{nt} }{ \\sqrt{n} } , \\quad t = \\frac{k}{n}.\n",
    "$$\n",
    "\n",
    "We then extend $S^{(n)}$ to a function on the interval $t \\in [0,\\infty)$ by piecewise linear interpolation. The central limit theorem implies that $S^{(n)} (t)$ converges to $\\mathcal{N}(0,t)$ as $n \\to \\infty$ for each $t > 0$. (which is the law of $B(t)$ for a Brownian motion)\n",
    "\n",
    "The Markov property of Brownian motion may be obtained from the above approximation. Suppose $0 \\leq s < t$ and suppose for simplicity that $k = sn$ and $\\ell = tn$ are integers. Then,\n",
    "\n",
    "$$\n",
    "S^{(n)} (t) - S^{(n)}(s) = \\frac{ Z_{k+1} + \\dots + Z_\\ell }{ \\sqrt{n}},\n",
    "$$\n",
    "\n",
    "and it is thus immediate that $S^{(n)} (s)$ is independent of the increment $S^{(n)}(t) - S^{(n)}(s)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A more precise discussion of the marginals of BM through membership in a finite collection of intervals at a finite collection of times. The analogy is a skier and \"slalom gates.\"\n",
    "\n",
    "* A deeper version of the CLT, Donsker's theorem, establishes convergence of the random functions $S^{(n)}$ to Brownian motion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 ... The heat equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand what was novel about Einstein's approach to the theory of diffusion, it is necessary to first review the classical theory of diffusion (or heat flow). The heat equation was first introduced in Fourier's work in 1822. He modeled heat flow with a PDE for the temperature distribution $\\rho(x,t)$ and solved this equation using the method of Fourier series. Let us review this approach in its simplest setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heat equation on the line is the PDE\n",
    "\n",
    "$$\n",
    "\\partial_t \\rho = \\frac{1}{2} \\partial_x^2 \\rho, \\quad - \\infty < x < \\infty, \\, t > 0\n",
    "$$\n",
    "\n",
    "Here $\\rho(x,t)$ is a positive function describing the temperature in an infinitely long bar. This equation also models diffusion (this is the process that describes the spread of a dye in still water). In this context, $\\rho$ describes the density of the dye. The initial value problem for the heat equation is to solve the above equation for $\\rho(x,t)$ subject to the condition $\\rho(x,0) = \\rho_0 (x)$, where $\\rho_0(x)$ is a given initial temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we assume that $\\rho(x,t)$ decauys fast enough as $|x| \\to \\infty$. Then consider the Fourier transform\n",
    "\n",
    "$$\n",
    "\\hat{\\rho}(\\xi, t) = \\int_{\\mathbb{R}} e^{-\\textrm{i} \\xi x } \\, \\rho(x,t) \\, \\textrm{d}x , \\quad \\xi \\in \\mathbb{R}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fourier transform converts differentiation in $x$ to multiplication by $\\xi$ as follows (by integrating by parts):\n",
    "\n",
    "$$\n",
    "\\int_{ \\mathbb{R} } e^{-\\textrm{i} x \\xi } \\,\n",
    "\\partial_x^2 \\rho(x,t) \\, \\textrm{d}x  = \\int_{\\mathbb{R}} \\partial_x^2 \\left( e^{ - \\textrm{i} x \\xi } \\right) \\, \\rho(x,t) \\, \\textrm{d} x \\\\\n",
    "= (- \\textrm{i} \\xi )^{2} \\int_{ \\mathbb{R} } e^{-2\\pi \\textrm{i} x \\xi } \\, \\rho(x,t) \\, \\textrm{d} x \\equiv (-\\xi^2) \\hat{\\rho}(\\xi, t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting this expression in the heat equation, we find that\n",
    "\n",
    "$$\n",
    "\\partial_t \\hat{\\rho} (\\xi, t) = - \\frac{ \\xi^2}{2} \\hat{\\rho}(\\xi, t) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we have converted the PDE into an infinite family of ODE ( parametrized by $\\xi$ ), which may be integrated explicitly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We solve the equation above to obtain\n",
    "\n",
    "$$\n",
    "\\hat{\\rho} ( \\xi, t ) = e^{- \\xi^2 t /2 } \\hat{\\rho}_0 ( \\xi ) \\,, \\quad \\xi \\in \\mathbb{R}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now invert the Fourier transform to obtain the solution formula for the heat equation:\n",
    "\n",
    "$$\n",
    "\\rho(x,t) = \\int_{ \\mathbb{R}} g_t(x - y) \\rho_0 (y) dy,\n",
    "$$\n",
    "\n",
    "with \n",
    "\n",
    "$$\n",
    "g_t(x) = \\frac{1}{ \\sqrt{2\\pi} t } e^{-x^2 / t }.\n",
    "$$\n",
    "\n",
    "The function $g_t$ is called the fundamental solution to the heat equation, since it is the solution to the heat equation with a Dirac initial condition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The appearance of $g_t(x)$ in the above solution formula has the following probablistic interpretation. The above display of the solution of $\\rho(x,t)$ has the probablistic interpretation\n",
    "\n",
    "$$\n",
    "\\rho(x,t) = \\mathbb{E} (\\, \\rho_0 (x - B(t) )\\, ),\n",
    "$$\n",
    "\n",
    "where $B(t)$ is Brownian motion, since $\\mathbb{P}(\\,x - B(t) = y \\,) \\equiv g_t(y)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to interpret the flow of heat, or diffusion, as arising from the random motion of individual particles. Think of $\\rho_0$ as describing an initial population of particles. Each of these particles executes an independent Brownian motion and in order to obtain the density at the point $x$ at time $t$, we sum over all the Brownian paths that end at $(x,t)$. \n",
    "\n",
    "In order to demonstrate the power of this viewpoint, we now explain its utility in situations where there is no exact solution to a PDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 ... The probablistic solution of the Dirichlet problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Laplacian (written $\\Delta$) is the differential operator that acts on smooth functions $f : \\mathbb{R}^n \\to \\mathbb{R}$ by\n",
    "\n",
    "$$\n",
    "\\Delta f(x) \\triangleq \\partial_{x_1}^2 f + \\dots + \\partial_{x_n}^2 f ,\n",
    "$$\n",
    "\n",
    "The heat equation in $\\mathbb{R}^n$ is the equation \n",
    "\n",
    "$$\n",
    "\\partial_t \\rho = \\frac{1}{2} \\Delta \\rho \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A closely related PDE is Laplace's equation $\\Delta u = 0$. When solving a PDE, we must also consider its initial values and boundary conditions. We will now consider Laplace's equation on a spatial domaini $D \\subset \\mathbb{R}^n$ with boundary $\\partial D$. The Dirichlet problem is as follows: we assume given boundary data $f: \\partial D \\to \\mathbb{R}$, and we must solve\n",
    "\n",
    "$$\n",
    "\\Delta u = 0, \\quad x \\in D\\, \\\\\n",
    "u(x) = f(x), x\\in \\partial D \n",
    "$$\n",
    "\n",
    "The Dirichlet problem is explicitly solvable on certain simple domains such as the square or circle and their higher dimensional analogues. For example, when $D \\subset \\mathbb{R}^2$ is a square, we may use separation of variables and Fourier series to solve the Dirichlet problem. However, this method is limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will approach the problem in a different way. Let $B_t$ now denote a Brownian motion in $\\mathbb{R}^n$ ( which is simply $n$ independent copies of Brownian motion in $\\mathbb{R}$ ). Given $x \\in D$, let\n",
    "\n",
    "$$\n",
    "X_t^x \\triangleq x + B_t ,\n",
    "$$\n",
    "\n",
    "denote a Brownian path starting at $x$. Given $X_t^x$ define the ( random ) first exit time\n",
    "\n",
    "$$\n",
    "T = \\min_{t > 0} \\{ X_t^x \\text{ does not lie in $D$ } \\}\n",
    "$$\n",
    "\n",
    "Then the solution to the Dirichlet problem has the probablistic representation\n",
    "\n",
    "$$\n",
    "u(x) = \\mathbb{E} ( f(X_T^x ) )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it takes some work to establish this formula in full generality, the main idea underlying this formula is the interplay between martingales and harmonic functions, which can be understood in the simpler context of random walks on a lattice. This formula may then be generalized to Brownian motion.\n",
    "\n",
    "To this end, consider the integer lattice $\\mathbb{Z}^n$. Given a function $u : \\mathbb{Z}^n \\to \\mathbb{R}$, define the discrete Laplacian \n",
    "\n",
    "$$\n",
    "\\Delta u(x) = \\frac{1}{2^n} \\sum_{ | x - y |= 1} (u(y) - u(x )), \\quad x \\in \\mathbb{Z}^n.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a subset $D \\subset \\mathbb{Z}^n$, we define its boundary $\\partial D$ to be the set of points $x \\in D$ such that at least one neighbor of $x$ does not lie in $D$. The discrete Dirichlet problem is as follows. Given a function $f: \\partial D \\to \\mathbb{R}$, we must solve\n",
    "\n",
    "$$\n",
    "\\Delta u =0, \\quad x \\in D,\\\\\n",
    "u(x) = f(x) , \\quad x \\in \\partial D \n",
    "$$\n",
    "\n",
    "This discrete analogue of the Dirichlet problem admits an analogous probablistic solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X_t^x$ denote the simple random walk on $\\mathbb{Z}^n$, starting at $x$ and $t = 0,1,2,\\dots$. Let $T(X_t^x)$ denote the first time $X_t^x$ hits $\\partial D$. The solution to the discrete Dirichlet problem is\n",
    "\n",
    "$$\n",
    "u(x) = \\mathbb{E} ( f (X_T^x) ).\n",
    "$$\n",
    "\n",
    "Given a spatial domain and a Laplacian on it (such as either of the examples above), we say that a function $v : D \\to \\mathbb{R}$ is **_harmonic_** if it satisfies $\\Delta v(x) = 0$ for each $x \\in D$. The precise definition of a martingale requires some care, since it involves a description of measurable events that can be generated by a collection of random variables. We will sweep these technicalities under the rug, and adopt the following working definition.\n",
    "\n",
    "A real-valued discrete time stochastic process $(Y_t)$ is a **_martingale_** with respect to the filtration generated by the stochastic process $(Z_t)$ if\n",
    "\n",
    "$$\n",
    "\\mathbb{E} (Y_{t+1} | Z_t , \\dots, Z_1 ) = Y_t \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interplay between harmonic functionss and martingales is captured in the following.\n",
    "\n",
    "___\n",
    "\n",
    "**Theorem.** $\\quad$ Suppose $B_t$ is a simple random walk on $\\mathbb{Z}^n$ and $v : \\mathbb{Z}^n \\to \\mathbb{R}$ is harmonic. Then $v(B_t)$ is a martingale with respect to the filtration generated by $B_t$.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "_Proof._ $\\quad$ Observe that\n",
    "\n",
    "$$\n",
    "\\mathbb{E}( \\, v(B_{t+1}) | B_t, \\dots, B_1 ) = \\mathbb{E} (v(B_{t+1}) | B_t )\n",
    "$$\n",
    "\n",
    "by the Markov property of simple random walk. Next, since the random walk will jump to each neighbor with probabiity $1 / 2n$, we find that\n",
    "\n",
    "$$\n",
    "\\mathbb{E} ( v(B_{t+1}) | B_t)  = \\frac{1}{2n} \\sum_{ | y - B_t | =1 } v(y) = v(B_t) + \\Delta v(B_t)\n",
    "$$\n",
    "\n",
    "using the definition of the Laplacian. Since $v$ is harmonic, $\\Delta v = 0$, and we see that \n",
    "\n",
    "$$\n",
    "\\mathbb{E}( v(B_{t+1}) | B_t) = v(B_t)\n",
    "$$\n",
    "\n",
    "$\\square$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This theorem doesn't really suffice to solve the Dirichlet problem in a bounded domain $D$, but it does give us a feeling for what's going on. In order to solve the Dirichlet problem we want $u$ such that $\\Delta u = 0$ in $D$ and $u = f$ on $\\partial D$. \n",
    "\n",
    "Let $x$ be an interior point of $D$ and let $X_t^x$ denote a simple random walk starting at $x$. A calculation as in the above theorem tells us that if $u$ solves $\\Delta u = 0$ in $D$ then $u (X_t^x)$ is a martingale _provided $X_t^x$ has not exit the domain $D$_. In order to state this precisely, we define the _stopped_ processes\n",
    "\n",
    "$$\n",
    "Y_t^x \\triangleq X_{T \\wedge t }^x , \\quad T = \\min \\{ \\, s : X_s^x \\in \\partial D \\, \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that $Y_t^x$ is also a Markov process and that we have the following stronger version of the above theorem:\n",
    "\n",
    "$$\n",
    "\\mathbb{E} ( \\, u(Y_{t+1}^x ) | Y_t^x \\, ) =  u(Y_t^x) \n",
    "$$\n",
    "\n",
    "Therefore, taking (unconditional) expectations we see that\n",
    "\n",
    "$$\n",
    "\\mathbb{E} ( \\, u( Y_{t+1}^x) \\, ) = \\mathbb{E} ( \\, u(Y_t^x) \\,) .\n",
    "$$\n",
    "\n",
    "Proceeding inductively, we find that\n",
    "\n",
    "$$\n",
    "\\mathbb{E} ( \\, u(Y_t^x) \\, ) = \\mathbb{E}(\\, u(Y_0^x) \\,) = u(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let $t \\to \\infty$ on the LHS above. If the domain is bounded, then $\\lim_{t \\to \\infty} Y_t^x = X_T^x$ with probability one, so that\n",
    "\n",
    "$$\n",
    "u(x) = \\mathbb{E} ( \\, u(X_T^x) \\, ) = \\mathbb{E} ( \\, f(X_T^x) \\, ).\n",
    "$$\n",
    "\n",
    "(this would be an applicaiton of martingale convergence). In conclusion, we see the theory of Brownian motion providess a deeper understanding of heat flow by focusing our attention on the stochastic process that describes the \"mechanism\" of heat flow. This viewpoint allows us to to recover the classical solution formulas for Laplace's equation and the heat equation for domains in $\\mathbb{R}^n$. Further, they provide us with a new mathematical structure - martingales - that expand the reach of the classical ideas to spaces that are very different from $\\mathbb{R}^n$ (for example, the world-wide-web is a large graph, not a Euclidean space; but we may still define the notion of heat flow on it to model the transmission of information on the web)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 6 ... From Music to Markov processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chap0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
